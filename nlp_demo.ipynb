{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Qualitative Analysis: Ideas for Content and Thematic Analysis Using Python"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Background Information"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Overview of Content and Thematic Analysis\n",
    "\n",
    "#### Summary\n",
    "Content Analysis involves systematically categorizing and quantifying elements of the text to identify trends and frequencies, often with a more structured and objective approach.\n",
    "\n",
    "Thematic Analysis focuses on identifying and interpreting themes and patterns within qualitative data to understand meanings and context.\n",
    "\n",
    "Both methods provide valuable insights but serve different purposes: thematic analysis is more exploratory and interpretive, while content analysis is more systematic and quantifiable."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Content Analysis\n",
    "* **Purpose**: To systematically analyze the content of qualitative data, often to quantify and categorize specific elements.\n",
    "\n",
    "* **Approach**:\n",
    "    * **Quantitative Focus**: Involves coding the text into categories or themes and often quantifies the frequency of these categories.\n",
    "    * **Focus**: Concentrates on identifying and counting the occurrence of specific words, phrases, or themes within the text. It can be either qualitative or quantitative, but often involves quantifying textual elements to identify trends or patterns.\n",
    "    * **Outcome**: Provides a structured overview of the content, highlighting how often specific elements appear and how they relate to each other.\n",
    "    * **Process**:\n",
    "        * Define categories or codes.\n",
    "        * Systematically code the text according to predefined criteria.\n",
    "        * Quantify the occurrence of categories.\n",
    "        * Analyze patterns and trends in the data.\n",
    "\n",
    "* **Application**: Useful for obtaining a systematic and objective overview of the content, identifying frequently occurring themes or elements, and analyzing trends across large volumes of text.\n",
    "\n",
    "#### Thematic Analysis\n",
    "* **Purpose**: To identify, analyze, and report patterns (themes) within qualitative data.\n",
    "\n",
    "* **Approach**:\n",
    "\n",
    "    * **Inductive or Deductive**: Themes can emerge from the data (inductive) or be predefined (deductive).\n",
    "    * **Focus**: Concentrates on understanding the underlying meanings and patterns within the text. It involves coding the data into themes and sub-themes that capture significant patterns or ideas.\n",
    "    * **Outcome**: Provides insights into how different themes relate to each other and to the overall research questions, often aiming to interpret and understand the context and meaning behind the data.\n",
    "    * **Process**:\n",
    "        * Familiarize with the data.\n",
    "        * Generate initial codes.\n",
    "        * Search for themes.\n",
    "        * Review and refine themes.\n",
    "        * Define and name themes.\n",
    "        * Write up the analysis.\n",
    "\n",
    "* **Application**: Useful for exploring complex patterns and meanings within data, and understanding how different themes contribute to the overall narrative.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tools Used in This Presentation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* [VS Code](https://code.visualstudio.com/)\n",
    "    * [Jupyter Overview](https://jupyter.org)\n",
    "        * [Jupyter in VS Code](https://code.visualstudio.com/docs/datascience/jupyter-notebooks)\n",
    "        * [Markdown](https://www.markdownguide.org/basic-syntax)\n",
    "    \n",
    "\n",
    "\n",
    "* [Python](https://www.python.org/)\n",
    "    * Packages\n",
    "        * [Pandas](https://pandas.pydata.org/docs)\n",
    "        * [NLTK](https://www.nltk.org) (Natural Language Tool Kit)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Additional Resources"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Python\n",
    "\n",
    "* [Getting Started with Python in VS Code](https://code.visualstudio.com/docs/python/python-tutorial)\n",
    "* [How to set up Python on Visual Studio Code](https://www.youtube.com/watch?v=9o4gDQvVkLU) (YouTube)\n",
    "* [How to install Python Packages in VS Code](https://www.youtube.com/watch?v=InRmKECJK3s) (YouTube)\n",
    "* [Python Package Index (PyPI)](https://pypi.org/) (additional packages for Python to extend it functionality)\n",
    "* [Learning Python](https://www.reddit.com/r/learnpython/wiki/index/) (Reddit - large list)\n",
    "* [Learning Python](https://www.youtube.com/watch?v=qwAFL1597eM) (YouTube)\n",
    "\n",
    "\n",
    "\n",
    "#### Qualitative Research\n",
    "* [The Practical Guide to Qualitative Content Analysis](https://delvetool.com/blog/guide-qualitative-content-analysis)\n",
    "* [How to Analyze Qualitative Data from UX Research: Thematic Analysis](https://www.nngroup.com/articles/thematic-analysis)\n",
    "* [How to do a content analysis](https://paperpile.com/g/content-analysis/#what-is-content-analysis)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Why Python?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. **Cost-Effective and Easily Available**: Python is open-source and free to use, avoiding the costs associated with purchasing licenses for proprietary software.\n",
    "\n",
    "2. **Customizability**: Python allows for extensive customization and development of tailored solutions that precisely fit your analysis needs, whereas proprietary software may offer limited flexibility.\n",
    "\n",
    "3. **Transparency and Control**: Python code is open and modifiable, allowing users to understand, modify, and improve the analysis methods used, unlike proprietary software where algorithms and methodologies are often hidden.\n",
    "\n",
    "4. **Reproducibility**: Python scripts and notebooks enable easy sharing and replication of analyses, which is crucial for maintaining reproducibility in research.\n",
    "\n",
    "5. **Integration Capabilities**: Python integrates well with other tools and technologies, including databases, APIs, and various data formats, facilitating a seamless workflow.\n",
    "\n",
    "6. **Scalability**: Python solutions can be scaled and adapted to handle large volumes of data and complex analyses, providing more flexibility for growing or changing research needs.\n",
    "\n",
    "7. **Extensive Libraries**: Python has a rich ecosystem of libraries (e.g., pandas, NumPy, scikit-learn, NLTK, spaCy) that support various aspects of qualitative analysis, including data manipulation, natural language processing, and machine learning.\n",
    "\n",
    "8. **Learning and Skill Development**: Using Python can enhance your programming and data analysis skills, which are valuable and transferable across various domains and projects.\n",
    "\n",
    "9. **Community Support**: A large and active community of developers and researchers provides continuous support, documentation, and updates, which can be invaluable for troubleshooting and learning.\n",
    "\n",
    "10. **Low Barrier to Entry**: With the development of [Large Language Models](https://huggingface.co/spaces/mike-ravkine/can-ai-code-results) (like ChatGPT), getting started has never been easier. You can generate working code in seconds using natural language queries. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Presentation Overview"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* **[Data Preprocessing](https://en.wikipedia.org/wiki/Data_preprocessing)**: Clean data and ensure its properly standardized.\n",
    "* **[N-Grams](https://en.wikipedia.org/wiki/N-gram)**: Provides quantitative data on word patterns and frequencies.\n",
    "* **[Parts-of-Speech](https://en.wikipedia.org/wiki/Part_of_speech) Analysis**: Analyzes grammatical elements and their frequency.\n",
    "    * The results from these analyses can be used to identify broader themes and patterns within the comments.\n",
    "* **[Sentiment Analysis](https://en.wikipedia.org/wiki/Sentiment_analysis)**: Quantifies emotional tones across comments.\n",
    "\n",
    "* **Counting occurrences and creating [indicator/dummy variables](https://en.wikipedia.org/wiki/Dummy_variable_(statistics))**: This quantifies the presence of each theme in the comments.\n",
    "* **Calculating a [correlation](https://en.wikipedia.org/wiki/Correlation) matrix**: This step analyzes the relationships between different themes to understand how they co-occur."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import os\n",
    "import pandas as pd\n",
    "import string\n",
    "import chardet\n",
    "import nltk\n",
    "from nltk import ngrams, word_tokenize\n",
    "ps = nltk.PorterStemmer()\n",
    "wn = nltk.WordNetLemmatizer()\n",
    "from autocorrect import Speller\n",
    "from collections import Counter\n",
    "import subprocess # Used to launch Excel\n",
    "import openpyxl #Used by Pandas to open Excel files\n",
    "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download NLTK resources if not already downloaded. This only has to be done once.\n",
    "\n",
    "# nltk.download('stopwords')\n",
    "# nltk.download('punkt')\n",
    "# nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load to dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "\n",
    "def detect_encoding(file_path):\n",
    "    \"\"\"\n",
    "    Detect the encoding of a file.\n",
    "\n",
    "    Args:\n",
    "        file_path (str): The path to the file whose encoding needs to be detected.\n",
    "\n",
    "    Returns:\n",
    "        str: The detected encoding of the file.\n",
    "\n",
    "    Raises:\n",
    "        FileNotFoundError: If the file does not exist.\n",
    "        IOError: If there is an issue reading the file.\n",
    "    \"\"\"\n",
    "    if not os.path.isfile(file_path):\n",
    "        raise FileNotFoundError(f\"The file at {file_path} does not exist.\")\n",
    "    \n",
    "    try:\n",
    "        with open(file_path, 'rb') as f:\n",
    "            result = chardet.detect(f.read(10000))  # Read a sample of the file\n",
    "        return result['encoding']\n",
    "    except IOError as e:\n",
    "        raise IOError(f\"An error occurred while reading the file: {e}\")\n",
    "\n",
    "def load_file_to_dataframe(file_path, sheet_name=None):\n",
    "    \"\"\"\n",
    "    Load a file into a pandas DataFrame based on its file extension.\n",
    "\n",
    "    Args:\n",
    "        file_path (str): The path to the file to be loaded.\n",
    "        sheet_name (str, optional): The name of the sheet to load from Excel files. Defaults to None.\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: A pandas DataFrame containing the data from the file.\n",
    "\n",
    "    Raises:\n",
    "        FileNotFoundError: If the file does not exist.\n",
    "        ValueError: If the file type is unsupported or the DataFrame is empty.\n",
    "        RuntimeError: If an error occurs while loading the file.\n",
    "    \"\"\"\n",
    "    # Check if file exists\n",
    "    if not os.path.isfile(file_path):\n",
    "        raise FileNotFoundError(f\"The file at {file_path} does not exist.\")\n",
    "    \n",
    "    # Extract the file extension\n",
    "    file_extension = file_path.split('.')[-1].lower()\n",
    "\n",
    "    encoding = 'utf-8'  # Default encoding\n",
    "    \n",
    "    try:\n",
    "        # Check the file extension and open the file accordingly\n",
    "        if file_extension == 'csv':\n",
    "            df = pd.read_csv(file_path, encoding=encoding)\n",
    "        elif file_extension in ['xls', 'xlsx']:\n",
    "            df = pd.read_excel(file_path, sheet_name=sheet_name, engine='openpyxl')\n",
    "        elif file_extension == 'json':\n",
    "            df = pd.read_json(file_path, encoding=encoding)\n",
    "        elif file_extension in ['txt', 'tsv']:\n",
    "            delimiter = '\\t' if file_extension == 'tsv' else ','\n",
    "            df = pd.read_csv(file_path, delimiter=delimiter, encoding=encoding)\n",
    "        elif file_extension == 'parquet':\n",
    "            df = pd.read_parquet(file_path)\n",
    "        elif file_extension == 'hdf':\n",
    "            df = pd.read_hdf(file_path)\n",
    "        elif file_extension == 'feather':\n",
    "            df = pd.read_feather(file_path)\n",
    "        else:\n",
    "            raise ValueError(\"Unsupported file type. Supported types are: CSV, Excel (xls, xlsx), JSON, TXT, TSV, Parquet, HDF, Feather\")\n",
    "        \n",
    "    except (UnicodeDecodeError, pd.errors.ParserError) as e:\n",
    "        # Try detecting the file encoding if initial read fails\n",
    "        detected_encoding = detect_encoding(file_path)\n",
    "        try:\n",
    "            if file_extension == 'csv':\n",
    "                df = pd.read_csv(file_path, encoding=detected_encoding)\n",
    "            elif file_extension == 'json':\n",
    "                df = pd.read_json(file_path, encoding=detected_encoding)\n",
    "            elif file_extension in ['txt', 'tsv']:\n",
    "                delimiter = '\\t' if file_extension == 'tsv' else ','\n",
    "                df = pd.read_csv(file_path, delimiter=delimiter, encoding=detected_encoding)\n",
    "        except Exception as e:\n",
    "            raise RuntimeError(f\"An error occurred while loading the file: {e}\")\n",
    "    \n",
    "    # Basic data validation: Check if DataFrame is empty\n",
    "    if df.empty:\n",
    "        raise ValueError(\"The loaded DataFrame is empty.\")\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# def load_file_to_dataframe(file_path):\n",
    "#     # Extract the file extension\n",
    "#     file_extension = file_path.split('.')[-1].lower()\n",
    "\n",
    "#     # Check the file extension and open the file accordingly\n",
    "#     if file_extension == 'csv':\n",
    "#         df = pd.read_csv(file_path)\n",
    "#     elif file_extension in ['xls', 'xlsx']:\n",
    "#         df = pd.read_excel(file_path)\n",
    "#     elif file_extension == 'json':\n",
    "#         df = pd.read_json(file_path)\n",
    "#     elif file_extension in ['txt', 'tsv']:\n",
    "#         delimiter = '\\t' if file_extension == 'tsv' else ','\n",
    "#         df = pd.read_csv(file_path, delimiter=delimiter)\n",
    "#     elif file_extension == 'parquet':\n",
    "#         df = pd.read_parquet(file_path)\n",
    "#     elif file_extension == 'hdf':\n",
    "#         df = pd.read_hdf(file_path)\n",
    "#     elif file_extension == 'feather':\n",
    "#         df = pd.read_feather(file_path)\n",
    "#     else:\n",
    "#         raise ValueError(\"Unsupported file type. Supported types are: CSV, Excel (xls, xlsx), JSON, TXT, TSV, Parquet, HDF, Feather\")\n",
    "\n",
    "#     return df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Open in Excel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def open_csv_in_excel(file_path):\n",
    "    \"\"\"\n",
    "    Opens a CSV file in Microsoft Excel.\n",
    "\n",
    "    This function attempts to find the Microsoft Excel executable (`EXCEL.EXE`) on the system\n",
    "    and uses it to open the specified CSV file. It first checks common installation paths for\n",
    "    Excel and then searches additional directories if necessary.\n",
    "\n",
    "    Args:\n",
    "        file_path (str): The path to the CSV file that needs to be opened in Excel.\n",
    "\n",
    "    Returns:\n",
    "        None\n",
    "\n",
    "    Raises:\n",
    "        FileNotFoundError: If the file_path does not point to a valid file.\n",
    "        RuntimeError: If Excel cannot be found on the system or if there is an issue opening the file.\n",
    "\n",
    "    Notes:\n",
    "        - This function assumes that Microsoft Excel is installed on the system.\n",
    "        - The `find_excel_exe` function is used to search for Excel in common installation directories.\n",
    "        - The function prints an error message if Excel cannot be found or if there is an issue opening the file.\n",
    "    \"\"\"\n",
    "\n",
    "    excel_paths = [r\"C:\\Program Files (x86)\\Microsoft Office\\root\\Office16\\EXCEL.EXE\", r\"C:\\Program Files\\Microsoft Office\\root\\Office16\\EXCEL.EXE\"]\n",
    "    \n",
    "    excel_program_path = None\n",
    "\n",
    "    for excel_path in excel_paths:\n",
    "\n",
    "        # Check if EXCEL.EXE exists in check_excel_path_1\n",
    "        if os.path.exists(excel_path):\n",
    "            excel_program_path = excel_path\n",
    "\n",
    "    # # Check if EXCEL.EXE exists in check_excel_path_2\n",
    "    # if not excel_path and os.path.exists(check_excel_path_2):\n",
    "    #     excel_path = check_excel_path_2\n",
    "\n",
    "    if not excel_program_path:\n",
    "        # Directories to search for EXCEL.EXE\n",
    "        directories_to_search = [\n",
    "            r\"C:\\Program Files\",\n",
    "            r\"C:\\Program Files (x86)\",\n",
    "            r\"C:\\\\\",\n",
    "            # Add more directories to search if needed\n",
    "        ]\n",
    "\n",
    "        excel_program_path = find_excel_exe(directories_to_search)\n",
    "\n",
    "    if excel_program_path:\n",
    "        subprocess.Popen([excel_program_path, file_path])\n",
    "    else:\n",
    "        print(\"Sorry. Cannot open file directly. Excel cannot be found\")\n",
    "\n",
    "def find_excel_exe(directories):\n",
    "    for directory in directories:\n",
    "        for root, dirs, files in os.walk(directory):\n",
    "            for file in files:\n",
    "                if file.lower() == 'excel.exe':\n",
    "                    return os.path.join(root, file)\n",
    "    return None\n",
    "\n",
    "def open_in_excel(filename):\n",
    "    if os.path.exists(filename):\n",
    "        print(f\"Report written to: {filename}\")\n",
    "        open_csv = input(f\"Open file in Excel? Y/N\")\n",
    "        if open_csv.lower() == \"y\":\n",
    "            open_csv_in_excel(filename)\n",
    "    else:\n",
    "        print(\"Error writing report.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Misc Support Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load a 2-column csv file as a pandas dataframe and return a dictionary \n",
    "\n",
    "def load_csv_to_dict(file_path, key_column, value_column):\n",
    "    \"\"\"\n",
    "    Load a CSV file and create a dictionary from specified key and value columns.\n",
    "\n",
    "    Parameters:\n",
    "    file_path (str): The path to the CSV file.\n",
    "    key_column (str): The name of the column to use as keys in the dictionary.\n",
    "    value_column (str): The name of the column to use as values in the dictionary.\n",
    "\n",
    "    Returns:\n",
    "    dict: A dictionary with keys from the key_column and values from the value_column.\n",
    "    \"\"\"\n",
    "    df = pd.read_csv(file_path)\n",
    "    \n",
    "    # Convert NaN values to empty strings\n",
    "    df[value_column] = df[value_column].apply(lambda x: '' if pd.isna(x) else str(x))\n",
    "    \n",
    "    return df.set_index(key_column)[value_column].to_dict()\n",
    "\n",
    "def ensure_string(text, string_length_threshold):\n",
    "    \"\"\"\n",
    "    Ensure the input is converted to a string representation.\n",
    "\n",
    "    Parameters:\n",
    "    text: The input to be checked and converted to a string if necessary.\n",
    "    string_length_threshold: The minimum length of the string. If the length of the string is less than this threshold, return an empty string.\n",
    "\n",
    "    Returns:\n",
    "    str: The input converted to a string or an empty string if the length is less than the threshold or conversion fails.\n",
    "    \"\"\"\n",
    "    if pd.isnull(text):\n",
    "        return \"\"\n",
    "    elif isinstance(text, str):\n",
    "        text = text.strip()  # Strip whitespace from both ends\n",
    "        if len(text) < string_length_threshold:\n",
    "            return \"\"\n",
    "        return text\n",
    "    \n",
    "    try:\n",
    "        # Attempt to convert non-string types to string\n",
    "        text = str(text).strip()\n",
    "        if len(text) < string_length_threshold:\n",
    "            return \"\"\n",
    "        return text\n",
    "    except (ValueError, TypeError):\n",
    "        return \"\"\n",
    "    \n",
    "def clean_whitespace(text):\n",
    "    # Remove leading and trailing whitespace\n",
    "    cleaned_text = text.strip()\n",
    "    \n",
    "    # Replace multiple spaces with a single space\n",
    "    cleaned_text = ' '.join(cleaned_text.split())\n",
    "    \n",
    "    return cleaned_text\n",
    "\n",
    "\n",
    "def save_dataframe_with_incremented_filename(file_path):\n",
    "    \"\"\"\n",
    "    Check if a filename already exists. If it does, it returns an incremented filename.\n",
    "\n",
    "    Parameters:\n",
    "    df (pd.DataFrame): The DataFrame to be saved.\n",
    "    file_path (str): The initial file path for the CSV file.\n",
    "    \"\"\"\n",
    "    base, extension = os.path.splitext(file_path)\n",
    "    counter = 1\n",
    "\n",
    "    # Check if the file already exists\n",
    "    while os.path.exists(file_path):\n",
    "        # Increment the file name\n",
    "        file_path = f\"{base}_{counter}{extension}\"\n",
    "        counter += 1\n",
    "\n",
    "    return file_path"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Text Analysis Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Lists and Settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stopword lisit\n",
    "external_stopword_list = r\"D:\\Data Analysis\\Qualitative Analysis\\Cleaning Text\\lists and dictionaries\\stopwords_seo.csv\"\n",
    "\n",
    "# Contractions dictionary\n",
    "external_contractions_dictionary = r\"D:\\Data Analysis\\Qualitative Analysis\\Cleaning Text\\lists and dictionaries\\contractions_list.csv\"\n",
    "\n",
    "# External search and replace file\n",
    "custom_search_and_replace_dictionary = r\"D:\\Data Analysis\\Qualitative Analysis\\Cleaning Text\\lists and dictionaries\\custom search-and-replace dictionary.csv\"\n",
    "\n",
    "# Load Custom Dictionaries\n",
    "external_stopword_list_dict = load_csv_to_dict(external_stopword_list, \"stopword\", \"replace\")\n",
    "contractions_dict = load_csv_to_dict(external_contractions_dictionary, \"expansion\", \"contraction\")\n",
    "custom_search_and_replace_dict = load_csv_to_dict(custom_search_and_replace_dictionary, \"search_term\", \"replace_term\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is a dictionary of common encoding errors\n",
    "\n",
    "common_encoding_errors = {\n",
    "    \"&lt;\": \"<\",        # Less-than sign\n",
    "    \"&gt;\": \">\",        # Greater-than sign\n",
    "    \"&amp;\": \"&\",       # Ampersand\n",
    "    \"&quot;\": \"\\\"\",     # Double quotation mark\n",
    "    \"&apos;\": \"'\",      # Apostrophe\n",
    "    \"&nbsp;\": \" \",      # Non-breaking space\n",
    "    \"&copy;\": \"(c)\",    # Copyright sign\n",
    "    \"&reg;\": \"(r)\",     # Registered sign\n",
    "    \"&euro;\": \"EUR\",    # Euro sign\n",
    "    \"&pound;\": \"GBP\",   # Pound sign\n",
    "    \"&yen;\": \"Yen\",     # Yen sign\n",
    "    \"&cent;\": \"c\",      # Cent sign\n",
    "    \"&sect;\": \"§\",      # Section sign\n",
    "    \"&para;\": \"¶\",      # Pilcrow sign\n",
    "    \"&deg;\": \"°\",       # Degree sign\n",
    "    \"&plusmn;\": \"±\",    # Plus-minus sign\n",
    "    \"&micro;\": \"µ\",     # Micro sign\n",
    "    \"&sup2;\": \"^2\",     # Superscript two\n",
    "    \"&sup3;\": \"^3\",     # Superscript three\n",
    "    \"&frac14;\": \"1/4\",  # Fraction one quarter\n",
    "    \"&frac12;\": \"1/2\",  # Fraction one half\n",
    "    \"&frac34;\": \"3/4\",  # Fraction three quarters\n",
    "    \"&times;\": \"x\",     # Multiplication sign\n",
    "    \"&divide;\": \"/\",    # Division sign\n",
    "    \"&bull;\": \"•\",      # Bullet\n",
    "    \"&ndash;\": \"-\",     # En dash\n",
    "    \"&mdash;\": \"-\",     # Em dash\n",
    "    \"&lsquo;\": \"'\",     # Left single quotation mark\n",
    "    \"&rsquo;\": \"'\",     # Right single quotation mark\n",
    "    \"&sbquo;\": \",\",     # Single low-9 quotation mark\n",
    "    \"&ldquo;\": \"\\\"\",    # Left double quotation mark\n",
    "    \"&rdquo;\": \"\\\"\",    # Right double quotation mark\n",
    "    \"&bdquo;\": \"\\\"\",    # Double low-9 quotation mark\n",
    "    \"&hellip;\": \"...\",  # Horizontal ellipsis\n",
    "    \"%20\": \" \",         # Space\n",
    "    \"%21\": \"!\",         # Exclamation mark\n",
    "    \"%22\": \"\\\"\",        # Double quotation mark\n",
    "    \"%23\": \"#\",         # Number sign\n",
    "    \"%24\": \"$\",         # Dollar sign\n",
    "    \"%25\": \"%\",         # Percent sign\n",
    "    \"%26\": \"&\",         # Ampersand\n",
    "    \"%27\": \"'\",         # Apostrophe\n",
    "    \"%28\": \"(\",         # Left parenthesis\n",
    "    \"%29\": \")\",         # Right parenthesis\n",
    "    \"%2A\": \"*\",         # Asterisk\n",
    "    \"%2B\": \"+\",         # Plus sign\n",
    "    \"%2C\": \",\",         # Comma\n",
    "    \"%2D\": \"-\",         # Hyphen\n",
    "    \"%2E\": \".\",         # Period\n",
    "    \"%2F\": \"/\",         # Slash\n",
    "    \"%3A\": \":\",         # Colon\n",
    "    \"%3B\": \";\",         # Semicolon\n",
    "    \"%3C\": \"<\",         # Less-than sign\n",
    "    \"%3D\": \"=\",         # Equal sign\n",
    "    \"%3E\": \">\",         # Greater-than sign\n",
    "    \"%3F\": \"?\",         # Question mark\n",
    "    \"%40\": \"@\",         # At sign\n",
    "    \"%5B\": \"[\",         # Left square bracket\n",
    "    \"%5C\": \"\\\\\",        # Backslash\n",
    "    \"%5D\": \"]\",         # Right square bracket\n",
    "    \"%5E\": \"^\",         # Caret\n",
    "    \"%5F\": \"_\",         # Underscore\n",
    "    \"%60\": \"`\",         # Grave accent\n",
    "    \"%7B\": \"{\",         # Left curly bracket\n",
    "    \"%7C\": \"|\",         # Vertical bar\n",
    "    \"%7D\": \"}\",         # Right curly bracket\n",
    "    \"%7E\": \"~\",         # Tilde\n",
    "    \"â€”\": \"-\",         # Em dash\n",
    "    \"â€“\": \"–\",         # En dash\n",
    "    \"â€˜\": \"'\",         # Left single quotation mark\n",
    "    \"â€™\": \"'\",         # Right single quotation mark\n",
    "    \"â€œ\": \"\\\"\",        # Left double quotation mark\n",
    "    \"â€\": \"\\\"\",         # Right double quotation mark\n",
    "    \"â€¦\": \"...\",       # Ellipsis\n",
    "    \"â€\": \"\\\"\",         # Right double quotation mark (alternate)\n",
    "    \"â€™\": \"'\",         # Right single quotation mark (alternate)\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Text Cleaning Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Main Text cleaning function\n",
    "def clean_text(text):\n",
    "    \"\"\"\n",
    "    Clean and preprocess the input text based on specified cleaning options.\n",
    "\n",
    "    This function applies a series of text cleaning operations to the input `text` based on the \n",
    "    global `text_cleaning_options` dictionary. The cleaning operations include:\n",
    "    - Converting text to lowercase\n",
    "    - Expanding or contracting contractions\n",
    "    - Replacing common encoding errors\n",
    "    - Standardizing highway and DriveBC references\n",
    "    - Using a custom search-and-replace dictionary\n",
    "    - Removing stopwords\n",
    "    - Stripping or replacing punctuation\n",
    "    - Performing spell check\n",
    "    - Stemming or lemmatizing words\n",
    "    - Cleaning up extra whitespace\n",
    "\n",
    "    Args:\n",
    "        text (str): The text to be cleaned and preprocessed.\n",
    "\n",
    "    Returns:\n",
    "        str: The cleaned text after applying the specified text cleaning operations.\n",
    "\n",
    "    Raises:\n",
    "        Exception: If any error occurs during text processing, an error message is printed, \n",
    "                   and an empty string is returned.\n",
    "\n",
    "    Notes:\n",
    "        - The function uses global dictionaries for contractions, common encoding errors, \n",
    "          custom search-and-replace terms, and stopwords.\n",
    "        - The `text_cleaning_options` dictionary determines which cleaning operations are applied.\n",
    "        - The `ensure_string` function is assumed to ensure that `text` is a string. It is not defined here.\n",
    "        - The `clean_whitespace` function is assumed to handle extra whitespace. It is not defined here.\n",
    "    \"\"\"\n",
    "\n",
    "\n",
    "    global text_cleaning_options\n",
    "\n",
    "    # Set Cleaning Options\n",
    "    transform_contractions = text_cleaning_options[\"Transform contractions\"]        \n",
    "    convert_to_lowercase = text_cleaning_options[\"Convert to lowercase\"]\n",
    "    covert_common_encoding_errors = text_cleaning_options[\"Convert Common Encoding Errors\"]\n",
    "    \n",
    "    strip_punctuation = text_cleaning_options[\"Strip punctuation\"]\n",
    "    remove_stopwords = text_cleaning_options[\"Remove stopwords\"]\n",
    "\n",
    "    use_custom_sar_dict = text_cleaning_options[\"Use custom Search-and-Replace dictionary\"]\n",
    "    standardize_highway_references = text_cleaning_options[\"Standardize highway references\"]\n",
    "    standardize_drivebc_references  = text_cleaning_options[\"Standardize drivebc references\"]\n",
    "    spell_check = text_cleaning_options[\"Spell check\"]\n",
    "    stem_words  = text_cleaning_options[\"Stem words\"]\n",
    "    lemmatize_words = text_cleaning_options[\"Lemmatize words\"]\n",
    "\n",
    "    try:\n",
    "        # Make sure the data is actually text\n",
    "        text = ensure_string(text,3)\n",
    "\n",
    "        # Convert all text to lower case\n",
    "        if convert_to_lowercase is True:\n",
    "            text = text.lower()\n",
    "\n",
    "        # Replace contractions\n",
    "        if transform_contractions == \"Expand\":\n",
    "            # Expand contractions\n",
    "            for key, value in contractions_dict.items():\n",
    "                text = text.replace(value, key)\n",
    "            # Contract expansions\n",
    "        elif transform_contractions == \"Contract\":\n",
    "                for key, value in contractions_dict.items():\n",
    "                    text = text.replace(key, value)\n",
    "\n",
    "        if covert_common_encoding_errors is True:\n",
    "            # Perform character replacement using the dictionary\n",
    "            for improper, proper in common_encoding_errors.items():\n",
    "                text = text.replace(improper, proper)\n",
    "       \n",
    "        # Standardize Highway References\n",
    "        if standardize_highway_references is True:\n",
    "            regex_highway_search1 = re.compile(\n",
    "                r\"#*\\b(bc *)*h[ighway]*[\\s#]*(\\d+[abcd]*)\\b\",\n",
    "                re.IGNORECASE,\n",
    "            )\n",
    "            regex_highway_replace1 = r\"BCHwy\\2\"\n",
    "            text = re.sub(regex_highway_search1, regex_highway_replace1, text)\n",
    "\n",
    "            regex_highway_search2 = re.compile(\n",
    "                r\"\\b(bc *)*h[ighway]*[\\s#]*\\b\",\n",
    "                re.IGNORECASE,\n",
    "            )\n",
    "            regex_highway_replace2 = r\"highway\"\n",
    "            text = re.sub(regex_highway_search2, regex_highway_replace2, text)\n",
    "\n",
    "        # Standardize DriveBC references\n",
    "        if standardize_drivebc_references is True:\n",
    "            regex_drivebc_search = re.compile(\n",
    "                r\"\\bdrive.*bc\\b\",\n",
    "                re.IGNORECASE,\n",
    "            )\n",
    "            regex_drivebc_replace = r\"drivebc\"\n",
    "            text = re.sub(regex_drivebc_search, regex_drivebc_replace, text)\n",
    "\n",
    "        # Use a custom search and replace dictionary\n",
    "        if use_custom_sar_dict is True:\n",
    "            for key, value in custom_search_and_replace_dict.items():\n",
    "                text = text.replace(key, value)\n",
    "\n",
    "        # Remove stop words\n",
    "        if remove_stopwords is True:\n",
    "            for key, value in external_stopword_list_dict.items():\n",
    "                text = text.replace(key, value)\n",
    "\n",
    "        # strip punctuation\n",
    "        if strip_punctuation is True:\n",
    "            # # This removed punctuation marks\n",
    "            # text = \" \".join([char for char in text if char not in string.punctuation])\n",
    "            \n",
    "            # This replaces punctuation marks with a space\n",
    "            translation_table = str.maketrans(string.punctuation, ' ' * len(string.punctuation))\n",
    "            # Replace punctuation with spaces\n",
    "            text = text.translate(translation_table)\n",
    "\n",
    "\n",
    "        if spell_check is True:\n",
    "            text = text.split()\n",
    "            text = [spell(word) for word in text]\n",
    "            text = \" \".join(text)\n",
    "\n",
    "        # Stem words\n",
    "        if stem_words is True:\n",
    "            text = text.split()\n",
    "            text = [ps.stem(word) for word in text]\n",
    "            text = \" \".join(text)\n",
    "\n",
    "        # Lemmatize words\n",
    "        if lemmatize_words is True:\n",
    "            text = text.split()\n",
    "            text = [wn.lemmatize(word) for word in text]\n",
    "            text = \" \".join(text)\n",
    "\n",
    "        # Clean Whitespace\n",
    "        text = clean_whitespace(text)\n",
    "\n",
    "        return text\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing text: {e}\")\n",
    "        return \"\"\n",
    "\n",
    "\n",
    "# Add clean text to dataframe\n",
    "def add_clean_text_column(df,comments):\n",
    "    \"\"\"\n",
    "    Adds a cleaned text column to a DataFrame by applying text cleaning operations.\n",
    "\n",
    "    This function adds a new column to the input DataFrame, `df`, which contains cleaned text\n",
    "    derived from the `comments` Series. The cleaning is performed using the `clean_text` function.\n",
    "\n",
    "    Args:\n",
    "        df (pd.DataFrame): The DataFrame to which the cleaned text column will be added.\n",
    "        comments (pd.Series): A Series containing the text data to be cleaned and added to the DataFrame.\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: The original DataFrame with an additional column named 'clean_text' that contains the cleaned text.\n",
    "\n",
    "    Raises:\n",
    "        ValueError: If `comments` is not a Series or if its length does not match the number of rows in `df`.\n",
    "        Exception: If any error occurs during the application of the `clean_text` function.\n",
    "\n",
    "    Notes:\n",
    "        - The `comments` Series should have the same length as the number of rows in the DataFrame `df`.\n",
    "        - The `clean_text` function is expected to be defined elsewhere and handle the text cleaning process.\n",
    "    \"\"\"\n",
    "    if not isinstance(comments, pd.Series):\n",
    "        raise ValueError(\"The 'comments' argument must be a pandas Series.\")\n",
    "    if len(comments) != len(df):\n",
    "        raise ValueError(\"The length of 'comments' must match the number of rows in the DataFrame.\")\n",
    "\n",
    "    try:\n",
    "        df[\"clean_text\"] = comments.apply(lambda x: clean_text(x))\n",
    "        return df\n",
    "    except Exception as e:\n",
    "        print(f\"Error adding clean text column: {e}\")\n",
    "        raise"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### N-Gram Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ngram_counter(series):\n",
    "    \"\"\"\n",
    "    Counts and returns the most common n-grams in a pandas Series of text data.\n",
    "\n",
    "    The function prompts the user to input the starting and ending n-gram lengths, \n",
    "    as well as the number of top n-grams to find for each length. It then processes \n",
    "    the text data, tokenizes it, generates n-grams for the specified lengths, and \n",
    "    counts their occurrences. Finally, it returns a DataFrame with the n-grams, \n",
    "    their types, and counts.\n",
    "\n",
    "    Parameters:\n",
    "    series (pandas.Series): A pandas Series containing text data.\n",
    "\n",
    "    Returns:\n",
    "    pandas.DataFrame: A DataFrame with columns 'N-Gram', 'N-Gram Type', and 'Count',\n",
    "                      containing the most common n-grams and their counts for each\n",
    "                      specified n-gram length.\n",
    "\n",
    "    User Inputs:\n",
    "    - Starting n-gram length: An integer specifying the smallest n-gram length (e.g., \n",
    "                              1 for monogram, 2 for bigram, etc.).\n",
    "    - Ending n-gram length: An integer specifying the largest n-gram length.\n",
    "    - Number of n-grams: An integer specifying the number of top n-grams to find \n",
    "                         for each n-gram length.\n",
    "\n",
    "    Example:\n",
    "    >>> import pandas as pd\n",
    "    >>> from nltk.tokenize import word_tokenize\n",
    "    >>> from nltk.util import ngrams\n",
    "    >>> from collections import Counter\n",
    "    >>> series = pd.Series([\"This is a sample text.\", \"This text is for testing purposes.\"])\n",
    "    >>> result = ngram_counter(series)\n",
    "    >>> print(result)\n",
    "               N-Gram N-Gram Type  Count\n",
    "    0            this       1-gram      2\n",
    "    1              is       1-gram      2\n",
    "    2              a       1-gram      1\n",
    "    3          sample       1-gram      1\n",
    "    4            text       1-gram      2\n",
    "    5           this is     2-gram      2\n",
    "    6              is a     2-gram      1\n",
    "    7          sample text  2-gram      1\n",
    "    8            text is    2-gram      1\n",
    "    9              is for   2-gram      1\n",
    "    10           for testing 2-gram     1\n",
    "    11 testing purposes 2-gram  1\n",
    "    \"\"\"\n",
    "    # Prompt user for the starting and ending n-gram lengths\n",
    "    os.system(\"cls\")\n",
    "    while True:\n",
    "        try:\n",
    "            start_length = int(input(\"Enter the starting n-gram length (1 for monogram, 2 for bigram, etc.): \"))\n",
    "            break\n",
    "        except ValueError:\n",
    "            print(\"Invalid input. Please enter a valid integer.\")\n",
    "    \n",
    "    while True:\n",
    "        try:\n",
    "            end_length = int(input(\"Enter the ending n-gram length: \"))\n",
    "            break\n",
    "        except ValueError:\n",
    "            print(\"Invalid input. Please enter a valid integer.\")\n",
    "\n",
    "    # Prompt user for the number of n-grams to find\n",
    "    while True:\n",
    "        try:\n",
    "            num_ngrams = int(input(\"Enter the number of n-grams to find for each type: \"))\n",
    "            break\n",
    "        except ValueError:\n",
    "            print(\"Invalid input. Please enter a valid integer.\")\n",
    "\n",
    "    ngrams_list = []\n",
    "\n",
    "    # Process each n-gram length\n",
    "    for n in range(start_length, end_length + 1):\n",
    "        all_tokens = []\n",
    "\n",
    "        # Tokenize each row and filter out rows with fewer words than the n-gram length\n",
    "        for text in series.dropna().str.lower():\n",
    "            tokens = word_tokenize(text)\n",
    "            if len(tokens) >= n:\n",
    "                all_tokens.extend(tokens)\n",
    "        \n",
    "        print(f\"Tokens for n={n}:\", all_tokens)  # Debug: Print tokens\n",
    "\n",
    "        # Create n-grams\n",
    "        generated_ngrams = list(ngrams(all_tokens, n))\n",
    "        print(f\"Generated {n}-grams:\", generated_ngrams)  # Debug: Print generated n-grams\n",
    "\n",
    "        ngram_counts = Counter(generated_ngrams)\n",
    "        print(f\"N-gram counts for n={n}:\", ngram_counts)  # Debug: Print n-gram counts\n",
    "\n",
    "        # Get the top n-grams of each type\n",
    "        top_ngrams = ngram_counts.most_common(num_ngrams)\n",
    "        for ngram, count in top_ngrams:\n",
    "            ngrams_list.append([' '.join(ngram), f\"{n}-gram\", count])\n",
    "\n",
    "    # Create a DataFrame with the n-grams, their types, and counts\n",
    "    result_df = pd.DataFrame(ngrams_list, columns=['N-Gram', 'N-Gram Type', 'Count'])\n",
    "\n",
    "    return result_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Isolate Parts of Speech Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def isolate_pos(series, selected_parts_of_speech, string_length_threshold):\n",
    "    \"\"\"\n",
    "    Isolates and counts words from a text series based on selected parts of speech and a string length threshold.\n",
    "\n",
    "    This function processes the input `series` by tokenizing it, tagging the tokens with their parts of speech,\n",
    "    and filtering the words based on the selected parts of speech. It then removes stop words, applies a string \n",
    "    length threshold, and returns a DataFrame with word counts for the remaining words.\n",
    "\n",
    "    Args:\n",
    "        series (pd.Series or str): The input text data, either as a pandas Series or a single string. \n",
    "        selected_parts_of_speech (list of str): A list of parts of speech to filter the tokens. \n",
    "        string_length_threshold (int): The minimum length of words to include in the final word count.\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: A DataFrame containing two columns:\n",
    "            - 'word': The words filtered by the selected parts of speech and length threshold.\n",
    "            - 'count': The frequency of each word.\n",
    "\n",
    "    Raises:\n",
    "        ValueError: If `selected_parts_of_speech` is empty or `string_length_threshold` is not a positive integer.\n",
    "        TypeError: If `series` is neither a pandas Series nor a string.\n",
    "\n",
    "    Notes:\n",
    "        - The `series` is converted to a string if it is a pandas Series.\n",
    "        - NLTK's `word_tokenize` and `pos_tag` functions are used for tokenization and part-of-speech tagging.\n",
    "        - The function uses a predefined list of stop words to filter out common, non-informative words.\n",
    "        - Words are filtered based on the specified length threshold before counting their occurrences.\n",
    "\n",
    "    Example:\n",
    "        >>> df = pd.Series([\"The quick brown fox jumps over the lazy dog.\"])\n",
    "        >>> pos_list = ['NN', 'JJ']  # Example parts of speech: Noun and Adjective\n",
    "        >>> threshold = 3\n",
    "        >>> isolate_pos(df, pos_list, threshold)\n",
    "           word  count\n",
    "        0   quick      1\n",
    "        1   brown      1\n",
    "        2   jumps      1\n",
    "        3   over       1\n",
    "    \"\"\"\n",
    "\n",
    "\n",
    "    if not selected_parts_of_speech:\n",
    "        print(\"No Parts of Speech have been selected. Please select them and try again.\")\n",
    "        input(f\"Press 'Enter' to continue.\")\n",
    "        return\n",
    "\n",
    "    if isinstance(series, pd.Series):\n",
    "        series = series.to_string()\n",
    "\n",
    "    # Tokenize the sentence into individual words\n",
    "    tokens = nltk.word_tokenize(series)\n",
    "\n",
    "    # Use NLTK's POS tagger to tag each word with its part of speech\n",
    "    pos_tags = nltk.pos_tag(tokens)\n",
    "\n",
    "    # Create a list of words filtered by chosen Part-of-Speech (PoS) type\n",
    "    word_list = []\n",
    "    print(\"Tagging parts of speech...\")\n",
    "    for word, pos in pos_tags:\n",
    "        if pos in selected_parts_of_speech:\n",
    "            word_list.append(word)\n",
    "\n",
    "    # Remove any stop words from the list\n",
    "    stop_words = [\n",
    "        \"..\", \"'m\", \"'s\", \"#name?\", \"are\", \"be\", \"been\", \"being\", \"did\", \"do\", \"does\", \"done\",\n",
    "        \"get\", \"go\", \"had\", \"has\", \"have\", \"i\", \"ì\", \"is\", \"make\", \"n/a\", \"needs\", \"put\", \"Q\",\n",
    "        \"seems\", \"take\", \"use\", \"vs\", \"was\", \"way\", \"were\"\n",
    "    ]\n",
    "    \n",
    "    cleaned_word_list = [element for element in word_list if element not in stop_words]\n",
    "\n",
    "    # Apply string length threshold\n",
    "    filtered_word_list = [word for word in cleaned_word_list if len(word) >= string_length_threshold]\n",
    "\n",
    "    # Count the frequency of each word\n",
    "    word_counts = pd.Series(filtered_word_list).value_counts().reset_index()\n",
    "\n",
    "    # Rename the columns of the DataFrame\n",
    "    word_counts.columns = [\"word\", \"count\"]\n",
    "\n",
    "    # Sort the DataFrame by count in descending order\n",
    "    word_counts = word_counts.sort_values(by=\"count\", ascending=False).reset_index(drop=True)\n",
    "\n",
    "    return word_counts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sentiment Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_sentiment_column(df, text_column):\n",
    "    \"\"\"\n",
    "    Add a sentiment score column to the DataFrame based on the specified text column using VADER.\n",
    "\n",
    "    Parameters:\n",
    "    df (pd.DataFrame): The original DataFrame.\n",
    "    text_column (str): The name of the column containing the text for sentiment analysis.\n",
    "\n",
    "    Returns:\n",
    "    pd.DataFrame: The DataFrame with an added \"sentiment\" score column.\n",
    "    \"\"\"\n",
    "    # Ensure the text_column exists in the DataFrame\n",
    "    if text_column not in df.columns:\n",
    "        raise ValueError(f\"Column '{text_column}' not found in DataFrame columns.\")\n",
    "\n",
    "    # Initialize the VADER sentiment intensity analyzer\n",
    "    analyzer = SentimentIntensityAnalyzer()\n",
    "\n",
    "    # Define a function to calculate the sentiment score\n",
    "    def get_sentiment(text):\n",
    "        if pd.isnull(text) or not isinstance(text, str) or text.strip() == \"\":\n",
    "            return None  # Return None for rows without valid text\n",
    "        sentiment = analyzer.polarity_scores(text)\n",
    "        return sentiment['compound']  # Compound sentiment score\n",
    "\n",
    "    # Apply the sentiment function to the text column and add the results to a new column\n",
    "    df['sentiment'] = df[text_column].apply(get_sentiment)\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Thematic Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_themes_and_comments(theme_list_path, comments_df, comments_column):\n",
    "    \"\"\"\n",
    "    Processes comments in a DataFrame by mapping terms to associated themes and adding new columns for each theme.\n",
    "\n",
    "    This function reads a CSV file containing a list of terms and their associated themes, then processes the \n",
    "    comments in the specified column of a DataFrame to indicate which themes are present based on the terms. \n",
    "    It adds new columns to the DataFrame for each unique theme, setting the value to 1 if the theme is present \n",
    "    in the comment and 0 otherwise.\n",
    "\n",
    "    Args:\n",
    "        theme_list_path (str): Path to the CSV file containing terms and their associated themes. The file should \n",
    "                               have two columns: 'term' and 'theme', where 'term' contains comma-separated terms \n",
    "                               and 'theme' contains comma-separated themes associated with those terms.\n",
    "        comments_df (pd.DataFrame): DataFrame containing comments. It must have at least one column, specified \n",
    "                                    by `comments_column`, which contains the text comments to be processed.\n",
    "        comments_column (str): The name of the column in `comments_df` that contains the comments to be processed.\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: The input DataFrame `comments_df` with additional columns for each unique theme found in \n",
    "                      the `theme_list_path`. Each theme column will contain 1 if the theme is present in the \n",
    "                      corresponding comment, and 0 otherwise.\n",
    "\n",
    "    Raises:\n",
    "        FileNotFoundError: If the CSV file specified by `theme_list_path` cannot be found.\n",
    "        KeyError: If the specified `comments_column` does not exist in `comments_df`.\n",
    "        ValueError: If the CSV file does not contain the required columns 'term' and 'theme'.\n",
    "\n",
    "    Notes:\n",
    "        - The function assumes that the 'term' and 'theme' columns in the CSV file are comma-separated.\n",
    "        - Themes are added as new columns to the original DataFrame, initialized with 0. The presence of each \n",
    "          theme in the comments is indicated with a 1.\n",
    "        - Comments are processed in a case-insensitive manner.\n",
    "        - The function does not handle multi-word terms or themes; it looks for exact matches of terms in comments.\n",
    "\n",
    "    Example:\n",
    "        >>> theme_list_path = 'themes.csv'\n",
    "        >>> comments_df = pd.DataFrame({'comment': ['The quick brown fox', 'Jumps over the lazy dog']})\n",
    "        >>> comments_column = 'comment'\n",
    "        >>> process_themes_and_comments(theme_list_path, comments_df, comments_column)\n",
    "           comment  theme1  theme2\n",
    "        0  The quick brown fox       1       0\n",
    "        1  Jumps over the lazy dog   0       1\n",
    "    \"\"\"\n",
    "    # Load the theme list CSV file into a DataFrame\n",
    "    theme_list_df = pd.read_csv(theme_list_path)\n",
    "\n",
    "    # Create a dictionary to map terms to their associated themes\n",
    "    term_to_themes = {}\n",
    "    for _, row in theme_list_df.iterrows():\n",
    "        terms = row['term'].split(',')\n",
    "        themes = row['theme'].split(',')\n",
    "        for term in terms:\n",
    "            term = term.strip()\n",
    "            if term not in term_to_themes:\n",
    "                term_to_themes[term] = set()\n",
    "            for theme in themes:\n",
    "                term_to_themes[term].add(theme.strip())\n",
    "\n",
    "    # Create new columns in comments_df for each unique theme\n",
    "    unique_themes = set()\n",
    "    for themes in term_to_themes.values():\n",
    "        unique_themes.update(themes)\n",
    "\n",
    "    for theme in unique_themes:\n",
    "        comments_df[theme] = 0  # Initialize all theme columns with 0\n",
    "\n",
    "    # Define a function to process each comment\n",
    "    def process_comment(comment):\n",
    "        if pd.isnull(comment):\n",
    "            return {theme: 0 for theme in unique_themes}\n",
    "        comment = comment.lower()\n",
    "        words = set(comment.split())  # Split the comment into words\n",
    "        result = {theme: 0 for theme in unique_themes}\n",
    "        for term, themes in term_to_themes.items():\n",
    "            term = term.lower()\n",
    "            if term in words:  # Check for exact match\n",
    "                for theme in themes:\n",
    "                    result[theme] = 1\n",
    "        return result\n",
    "\n",
    "    # Check if comments_column exists\n",
    "    if comments_column not in comments_df.columns:\n",
    "        raise KeyError(f\"Column '{comments_column}' not found in comments_df\")\n",
    "\n",
    "    # Apply the function to each row in the comments column\n",
    "    results = comments_df[comments_column].apply(process_comment)\n",
    "\n",
    "    # Update the comments_df with the results\n",
    "    for theme in unique_themes:\n",
    "        comments_df[theme] = results.apply(lambda x: x.get(theme, 0))\n",
    "\n",
    "    return comments_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Correlation Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_correlation(df, threshold):\n",
    "    \"\"\"\n",
    "    Analyzes the correlation between numeric columns in a DataFrame and visualizes the results.\n",
    "\n",
    "    This function computes the correlation matrix for numeric columns in the provided DataFrame, applies a threshold \n",
    "    to filter the correlations, and generates a heatmap to visualize the filtered correlations. It also returns a list \n",
    "    of column pairs with their correlation scores that meet or exceed the specified threshold.\n",
    "\n",
    "    Args:\n",
    "        df (pd.DataFrame): The input DataFrame containing the data. It must include at least two numeric columns for \n",
    "                           correlation analysis.\n",
    "        threshold (float): The correlation threshold used to filter the matrix. Only correlations with an absolute value \n",
    "                           greater than or equal to this threshold will be included in the results.\n",
    "\n",
    "    Returns:\n",
    "        list of tuples: A list of tuples, each containing:\n",
    "            - A tuple of two column names (`col1`, `col2`).\n",
    "            - The correlation score between these two columns.\n",
    "          The list is sorted by the absolute value of the correlation score in descending order.\n",
    "\n",
    "    Raises:\n",
    "        ValueError: If there are fewer than two numeric columns in the DataFrame.\n",
    "\n",
    "    Notes:\n",
    "        - The function uses Seaborn and Matplotlib to create a heatmap of the filtered correlation matrix.\n",
    "        - Correlations below the specified threshold are replaced with NaN in the heatmap.\n",
    "        - The correlation matrix is computed using Pearson correlation by default.\n",
    "        - Ensure that the DataFrame contains numeric data in the columns to be analyzed.\n",
    "\n",
    "    Example:\n",
    "        >>> import pandas as pd\n",
    "        >>> import numpy as np\n",
    "        >>> import seaborn as sns\n",
    "        >>> import matplotlib.pyplot as plt\n",
    "        >>> df = pd.DataFrame({\n",
    "        >>>     'A': np.random.rand(100),\n",
    "        >>>     'B': np.random.rand(100),\n",
    "        >>>     'C': np.random.rand(100),\n",
    "        >>>     'D': np.random.rand(100)\n",
    "        >>> })\n",
    "        >>> threshold = 0.5\n",
    "        >>> analyze_correlation(df, threshold)\n",
    "        [(('A', 'B'), 0.6), (('C', 'D'), 0.55)]  # Example output, actual result may vary\n",
    "    \"\"\"\n",
    "\n",
    "    # Identify numeric columns\n",
    "    numeric_cols = df.select_dtypes(include=['number']).columns\n",
    "    \n",
    "    if len(numeric_cols) < 2:\n",
    "        raise ValueError(\"Not enough numeric columns to compute correlations.\")\n",
    "    \n",
    "    # Compute the correlation matrix\n",
    "    corr_matrix = df[numeric_cols].corr()\n",
    "    \n",
    "    # Apply the threshold to the correlation matrix\n",
    "    filtered_corr_matrix = corr_matrix.applymap(lambda x: x if abs(x) >= threshold else np.nan)\n",
    "    \n",
    "    # Create a heatmap of the filtered correlation matrix\n",
    "    plt.figure(figsize=(10, 8))\n",
    "    sns.heatmap(filtered_corr_matrix, annot=True, cmap='coolwarm', vmin=-1, vmax=1, mask=filtered_corr_matrix.isnull())\n",
    "    plt.title('Correlation Matrix Heatmap (Filtered by Threshold)')\n",
    "    plt.show()\n",
    "    \n",
    "    # Create a list of column pairs with their correlation scores\n",
    "    corr_pairs = []\n",
    "    for i in range(len(numeric_cols)):\n",
    "        for j in range(i+1, len(numeric_cols)):\n",
    "            col1 = numeric_cols[i]\n",
    "            col2 = numeric_cols[j]\n",
    "            corr_score = corr_matrix.loc[col1, col2]\n",
    "            if abs(corr_score) >= threshold:\n",
    "                corr_pairs.append(((col1, col2), corr_score))\n",
    "    \n",
    "    # Sort the list of column pairs by correlation score in descending order\n",
    "    corr_pairs_sorted = sorted(corr_pairs, key=lambda x: abs(x[1]), reverse=True)\n",
    "    \n",
    "    return corr_pairs_sorted\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Latent Dirichlet Allocation (LDA) - Not used in this notebook. Requires additional packages. Kept here for reference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lda_analysis(series, num_topics=3, passes=10):\n",
    "    \"\"\"\n",
    "    Perform LDA analysis on a pandas Series of text data.\n",
    "\n",
    "    Parameters:\n",
    "    series (pd.Series): A pandas Series containing text data.\n",
    "    num_topics (int): The number of topics to extract. Default is 3.\n",
    "    passes (int): The number of passes through the corpus during training. Default is 10.\n",
    "\n",
    "    Returns:\n",
    "    dict: A dictionary containing the LDA model, the document-topic matrix, \n",
    "          and the topic-term matrix for visualization.\n",
    "    \"\"\"\n",
    "    # Define stopwords\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "\n",
    "    # Preprocess text: tokenize, remove stopwords, punctuation, and non-alphabetic tokens\n",
    "    def preprocess(text):\n",
    "        tokens = word_tokenize(text.lower())\n",
    "        tokens = [word for word in tokens if word.isalpha() and word not in stop_words]\n",
    "        return tokens\n",
    "\n",
    "    # Apply preprocessing to the series\n",
    "    processed_docs = series.dropna().apply(preprocess)\n",
    "\n",
    "    # Create a dictionary and a corpus\n",
    "    dictionary = corpora.Dictionary(processed_docs)\n",
    "    corpus = [dictionary.doc2bow(doc) for doc in processed_docs]\n",
    "\n",
    "    # Train the LDA model\n",
    "    lda_model = LdaModel(corpus, num_topics=num_topics, id2word=dictionary, passes=passes)\n",
    "\n",
    "    # Assign topics to documents\n",
    "    document_topics = lda_model.get_document_topics(corpus)\n",
    "    document_topic_df = pd.DataFrame([[dict(doc)] for doc in document_topics])\n",
    "    document_topic_df.columns = [f'Topic_{i + 1}' for i in range(num_topics)]\n",
    "    document_topic_df['Document'] = series.reset_index(drop=True)\n",
    "\n",
    "    # Prepare the visualization\n",
    "    lda_vis = gensimvis.prepare(lda_model, corpus, dictionary)\n",
    "\n",
    "    # Collect results in a dictionary\n",
    "    results = {\n",
    "        \"lda_model\": lda_model,\n",
    "        \"document_topic_df\": document_topic_df,\n",
    "        \"lda_vis\": lda_vis\n",
    "    }\n",
    "\n",
    "    return results\n",
    "\n",
    "# # Example usage:\n",
    "# series = pd.Series([\n",
    "#     \"This is the first document.\",\n",
    "#     \"This document is the second document.\",\n",
    "#     \"And this is the third one.\",\n",
    "#     \"Is this the first document?\"\n",
    "# ])\n",
    "\n",
    "# # Perform LDA analysis\n",
    "# results = lda_analysis(series, num_topics=3, passes=10)\n",
    "\n",
    "# # Access the results\n",
    "# lda_model = results['lda_model']\n",
    "# document_topic_df = results['document_topic_df']\n",
    "# lda_vis = results['lda_vis']\n",
    "\n",
    "# # Print the topics\n",
    "# for idx, topic in lda_model.print_topics(-1):\n",
    "#     print(f\"Topic {idx + 1}: {topic}\")\n",
    "\n",
    "# # Display the visualization in a Jupyter notebook (uncomment if using Jupyter)\n",
    "# # pyLDAvis.display(lda_vis)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Demo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load CSV Comments to DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = load_file_to_dataframe(r\"D:\\Data Analysis\\Qualitative Analysis\\comments_for_demo.csv\")\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Select the Comments Column From DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define series\n",
    "comments = df[\"Comment\"]\n",
    "comments"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## N-Grams - First Pass (Noisy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NGrams - Uncleaned\n",
    "base_ngram_report = r\"D:\\Data Analysis\\Qualitative Analysis\\reports\\ngram_report.csv\"\n",
    "ngram_report = save_dataframe_with_incremented_filename(base_ngram_report)\n",
    "return_ngrams = ngram_counter(comments)\n",
    "return_ngrams.to_csv(ngram_report,index=False)\n",
    "open_in_excel(ngram_report)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Text Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_cleaning_options = {\n",
    "    \"Convert to lowercase\": True,\n",
    "    \"Convert Common Encoding Errors\": True,\n",
    "    \"Strip punctuation\": True,\n",
    "    \"Remove stopwords\": False,\n",
    "    \"Transform contractions\": \"Expand\", \n",
    "    \"Use custom Search-and-Replace dictionary\": True,\n",
    "    \"Standardize highway references\": True,\n",
    "    \"Standardize drivebc references\": True,\n",
    "    \"Spell check\": False,\n",
    "    \"Stem words\": False,\n",
    "    \"Lemmatize words\": False,\n",
    "}\n",
    "cleaned_text = add_clean_text_column(df, comments)\n",
    "cleaned_comments = cleaned_text[\"clean_text\"]\n",
    "\n",
    "base_cleaned_text_report = r\"D:\\Data Analysis\\Qualitative Analysis\\reports\\cleaned_text_report.csv\"\n",
    "cleaned_text_report = save_dataframe_with_incremented_filename(base_cleaned_text_report)\n",
    "\n",
    "remove_columns = [\"ResponseID\", \"Question Text\", \"Question Number\"]\n",
    "\n",
    "# Check which columns in the list are present in the DataFrame\n",
    "columns_present = [col for col in remove_columns if col in cleaned_text.columns]\n",
    "\n",
    "# Drop the columns that are present\n",
    "df.drop(columns=columns_present, inplace=True)\n",
    "\n",
    "print(cleaned_text)\n",
    "\n",
    "cleaned_text.to_csv(cleaned_text_report,index=False)\n",
    "open_in_excel(cleaned_text_report)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## N-Grams - Second Pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NGrams - Cleaned\n",
    "# print(type(comments))\n",
    "# print(type(cleaned_comments))\n",
    "base_ngram_report_cleaned = r\"D:\\Data Analysis\\Qualitative Analysis\\reports\\ngram_report_cleaned.csv\"\n",
    "return_ngrams = ngram_counter(cleaned_comments)\n",
    "ngram_report = save_dataframe_with_incremented_filename(base_ngram_report_cleaned)\n",
    "return_ngrams.to_csv(ngram_report,index=False)\n",
    "open_in_excel(ngram_report)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parts of Speech"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_verbs = [\"VB\",\"VBD\",\"VBG\",\"VBN\",\"VBP\",\"VBZ\"]\n",
    "all_nouns = [\"NN\",\"NNP\",\"NNS\"]\n",
    "all_nouns_and_verbs = [\"NN\",\"NNP\",\"NNS\",\"VB\",\"VBD\",\"VBG\",\"VBN\",\"VBP\",\"VBZ\"]\n",
    "\n",
    "parts_of_speech = [\n",
    "    (\"CC\", \"conjunction, coordinating\"),\n",
    "    (\"CD\", \"numeral, cardinal\"),\n",
    "    (\"DT\", \"determiner\"),\n",
    "    (\"EX\", \"existential there\"),\n",
    "    (\"IN\", \"preposition or conjunction, subordinating\"),\n",
    "    (\"JJ\", \"adjective or numeral, ordinal\"),\n",
    "    (\"JJR\", \"adjective, comparative\"),\n",
    "    (\"JJS\", \"adjective, superlative\"),\n",
    "    (\"LS\", \"list item marker\"),\n",
    "    (\"MD\", \"modal auxiliary\"),\n",
    "    (\"NN\", \"noun, common, singular or mass\"),\n",
    "    (\"NNP\", \"noun, proper, singular\"),\n",
    "    (\"NNS\", \"noun, common, plural\"),\n",
    "    (\"PDT\", \"pre-determiner\"),\n",
    "    (\"POS\", \"genitive marker\"),\n",
    "    (\"PRP\", \"pronoun, personal\"),\n",
    "    (\"RB\", \"adverb\"),\n",
    "    (\"RBR\", \"adverb, comparative\"),\n",
    "    (\"RBS\", \"adverb, superlative\"),\n",
    "    (\"RP\", \"particle\"),\n",
    "    (\"TO\", \"to as preposition or infinitive marker\"),\n",
    "    (\"UH\", \"interjection\"),\n",
    "    (\"VB\", \"verb, base form\"),\n",
    "    (\"VBD\", \"verb, past tense\"),\n",
    "    (\"VBG\", \"verb, present participle or gerund\"),\n",
    "    (\"VBN\", \"verb, past participle\"),\n",
    "    (\"VBP\", \"verb, present tense, not 3rd person singular\"),\n",
    "    (\"VBZ\", \"verb, present tense, 3rd person singular\"),\n",
    "    (\"WDT\", \"WH-determiner\"),\n",
    "    (\"WP\", \"WH-pronoun\"),\n",
    "    (\"WRB\", \"Wh-adverb\")\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pos_df = isolate_pos(cleaned_comments, all_nouns_and_verbs, 3)\n",
    "\n",
    "# Specify the length threshold\n",
    "string_length_threshold = 3\n",
    "\n",
    "# Filter out rows where the length of the string in the \"word\" column is less than the threshold\n",
    "filtered_pos_df = pos_df[pos_df['word'].apply(lambda x: len(x) >= string_length_threshold)]\n",
    "\n",
    "base_pos_text_report = r\"D:\\Data Analysis\\Qualitative Analysis\\reports\\parts_of_speech_text_report.csv\"\n",
    "pos_text_report = save_dataframe_with_incremented_filename(base_pos_text_report)\n",
    "\n",
    "filtered_pos_df.to_csv(pos_text_report,index=False)\n",
    "open_in_excel(pos_text_report)   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sentiment Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add sentiment column to the DataFrame\n",
    "df_with_sentiment = add_sentiment_column(cleaned_text, 'Comment')\n",
    "\n",
    "base_sentiment_report = r\"D:\\Data Analysis\\Qualitative Analysis\\reports\\sentiment_report.csv\"\n",
    "sentiment_report = save_dataframe_with_incremented_filename(base_sentiment_report)\n",
    "\n",
    "df_with_sentiment.to_csv(sentiment_report,index=False)\n",
    "open_in_excel(sentiment_report)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Thematic Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "theme_list_path = r\"D:\\Data Analysis\\Qualitative Analysis\\Cleaning Text\\lists and dictionaries\\theme_list.csv\"\n",
    "comments_df = df_with_sentiment\n",
    "comments_column = \"clean_text\"\n",
    "theme_df = process_themes_and_comments(theme_list_path, comments_df, comments_column)\n",
    "\n",
    "base_thematic_analysis_report = r\"D:\\Data Analysis\\Qualitative Analysis\\reports\\thematic_analysis_report.csv\"\n",
    "thematic_analysis_report = save_dataframe_with_incremented_filename(base_thematic_analysis_report)\n",
    "\n",
    "theme_df.to_csv(thematic_analysis_report,index=False)\n",
    "open_in_excel(thematic_analysis_report)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Correlation Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "correlation_df = pd.read_csv(r\"D:\\Data Analysis\\Qualitative Analysis\\reports\\thematic_analysis_report.csv\")\n",
    "threshold = 0.1\n",
    "\n",
    "correlations = analyze_correlation(df, threshold)\n",
    "for (col1, col2), score in correlations:\n",
    "    print(f\"{col1} and {col2}: {score:.2f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
