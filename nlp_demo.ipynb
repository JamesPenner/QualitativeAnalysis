{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Qualitative Analysis: Ideas for Content and Thematic Analysis Using Python\n",
    "\n",
    "* Load Notebook into Chat\n",
    "* View notebook directly by uploading to: [Jupyter Lap](https://jupyter.org/try-jupyter/lab)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you want to run this code yourself:\n",
    "* Install VS Code, Python\n",
    "* Install dependencies\n",
    "* change path / header references to match your data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Background Information"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Overview of Content and Thematic Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Content Analysis\n",
    "* **Purpose**: To systematically analyze the content of qualitative data, often to quantify and categorize specific elements.\n",
    "\n",
    "* **Approach**:\n",
    "    * **Quantitative Focus**: Involves coding the text into categories or themes and often quantifies the frequency of these categories.\n",
    "    * **Focus**: Concentrates on identifying and counting the occurrence of specific words, phrases, or themes within the text. It can be either qualitative or quantitative, but often involves quantifying textual elements to identify trends or patterns.\n",
    "    * **Outcome**: Provides a structured overview of the content, highlighting how often specific elements appear and how they relate to each other.\n",
    "    * **Process**:\n",
    "        * Define categories or codes.\n",
    "        * Systematically code the text according to predefined criteria.\n",
    "        * Quantify the occurrence of categories.\n",
    "        * Analyze patterns and trends in the data.\n",
    "\n",
    "* **Application**: Useful for obtaining a systematic and objective overview of the content, identifying frequently occurring themes or elements, and analyzing trends across large volumes of text.\n",
    "\n",
    "#### Thematic Analysis\n",
    "* **Purpose**: To identify, analyze, and report patterns (themes) within qualitative data.\n",
    "\n",
    "* **Approach**:\n",
    "\n",
    "    * **Inductive or Deductive**: Themes can emerge from the data (inductive) or be predefined (deductive).\n",
    "    * **Focus**: Concentrates on understanding the underlying meanings and patterns within the text. It involves coding the data into themes and sub-themes that capture significant patterns or ideas.\n",
    "    * **Outcome**: Provides insights into how different themes relate to each other and to the overall research questions, often aiming to interpret and understand the context and meaning behind the data.\n",
    "    * **Process**:\n",
    "        * Familiarize with the data.\n",
    "        * Generate initial codes.\n",
    "        * Search for themes.\n",
    "        * Review and refine themes.\n",
    "        * Define and name themes.\n",
    "        * Write up the analysis.\n",
    "\n",
    "* **Application**: Useful for exploring complex patterns and meanings within data, and understanding how different themes contribute to the overall narrative.\n",
    "\n",
    "#### Summary\n",
    "Thematic Analysis focuses on identifying and interpreting themes and patterns within qualitative data to understand meanings and context.\n",
    "Content Analysis involves systematically categorizing and quantifying elements of the text to identify trends and frequencies, often with a more structured and objective approach.\n",
    "Both methods provide valuable insights but serve different purposes: thematic analysis is more exploratory and interpretive, while content analysis is more systematic and quantifiable."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tools Used in This Presentation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* [VS Code](https://code.visualstudio.com/)\n",
    "    * [Jupyter Overview](https://jupyter.org)\n",
    "        * [Jupyter in VS Code](https://code.visualstudio.com/docs/datascience/jupyter-notebooks)\n",
    "        * [Markdown](https://www.markdownguide.org/basic-syntax)\n",
    "    \n",
    "\n",
    "\n",
    "* [Python](https://www.python.org/)\n",
    "    * Packages\n",
    "        * [Pandas](https://pandas.pydata.org/docs)\n",
    "        * [NLTK](https://www.nltk.org)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Additional Resources"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Python\n",
    "\n",
    "* [Getting Started with Python in VS Code](https://code.visualstudio.com/docs/python/python-tutorial)\n",
    "* [How to set up Python on Visual Studio Code](https://www.youtube.com/watch?v=9o4gDQvVkLU) (YouTube)\n",
    "\n",
    "\n",
    "\n",
    "* [How to install Python Packages in VS Code](https://www.youtube.com/watch?v=InRmKECJK3s) (YouTube)\n",
    "* [Python Package Index (PyPI)](https://pypi.org/) (additional packages for Python to extend it functionality)\n",
    "\n",
    "#### Qualitative Research\n",
    "* [The Practical Guide to Qualitative Content Analysis](https://delvetool.com/blog/guide-qualitative-content-analysis)\n",
    "* [How to Analyze Qualitative Data from UX Research: Thematic Analysis](https://www.nngroup.com/articles/thematic-analysis)\n",
    "* [How to do a content analysis](https://paperpile.com/g/content-analysis/#what-is-content-analysis)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Presentation Overview"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* **[Data Preprocessing](https://en.wikipedia.org/wiki/Data_preprocessing)**: Clean data and ensure its properly standardized.\n",
    "* **[N-Grams](https://en.wikipedia.org/wiki/N-gram)**: Provides quantitative data on word patterns and frequencies.\n",
    "* **[Parts-of-Speech](https://en.wikipedia.org/wiki/Part_of_speech) Analysis**: Analyzes grammatical elements and their frequency.\n",
    "    * The results from these analyses can be used to identify broader themes and patterns within the comments.\n",
    "* **[Sentiment Analysis](https://en.wikipedia.org/wiki/Sentiment_analysis)**: Quantifies emotional tones across comments.\n",
    "\n",
    "* **Counting occurrences and creating [indicator/dummy variables](https://en.wikipedia.org/wiki/Dummy_variable_(statistics))**: This quantifies the presence of each theme in the comments.\n",
    "* **Calculating a [correlation](https://en.wikipedia.org/wiki/Correlation) matrix**: This step analyzes the relationships between different themes to understand how they co-occur."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import os\n",
    "import pandas as pd\n",
    "import string\n",
    "import nltk\n",
    "from nltk import ngrams, word_tokenize\n",
    "ps = nltk.PorterStemmer()\n",
    "wn = nltk.WordNetLemmatizer()\n",
    "from autocorrect import Speller\n",
    "from collections import Counter\n",
    "import subprocess # Used to launch Excel\n",
    "import openpyxl #Used by Pandas to open Excel files\n",
    "# from textblob import TextBlob #Used \n",
    "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download NLTK resources if not already downloaded. This only has to be done once.\n",
    "# nltk.download('stopwords')\n",
    "# nltk.download('punkt')\n",
    "# nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load to dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_file_to_dataframe(file_path):\n",
    "    # Extract the file extension\n",
    "    file_extension = file_path.split('.')[-1].lower()\n",
    "\n",
    "    # Check the file extension and open the file accordingly\n",
    "    if file_extension == 'csv':\n",
    "        df = pd.read_csv(file_path)\n",
    "    elif file_extension in ['xls', 'xlsx']:\n",
    "        df = pd.read_excel(file_path)\n",
    "    elif file_extension == 'json':\n",
    "        df = pd.read_json(file_path)\n",
    "    elif file_extension in ['txt', 'tsv']:\n",
    "        delimiter = '\\t' if file_extension == 'tsv' else ','\n",
    "        df = pd.read_csv(file_path, delimiter=delimiter)\n",
    "    elif file_extension == 'parquet':\n",
    "        df = pd.read_parquet(file_path)\n",
    "    elif file_extension == 'hdf':\n",
    "        df = pd.read_hdf(file_path)\n",
    "    elif file_extension == 'feather':\n",
    "        df = pd.read_feather(file_path)\n",
    "    else:\n",
    "        raise ValueError(\"Unsupported file type. Supported types are: CSV, Excel (xls, xlsx), JSON, TXT, TSV, Parquet, HDF, Feather\")\n",
    "\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Open in Excel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def open_csv_in_excel(file_path):\n",
    "    # check_excel_path_1 = r\"C:\\Program Files (x86)\\Microsoft Office\\root\\Office16\\EXCEL.EXE\"\n",
    "    # check_excel_path_2 = r\"C:\\Program Files\\Microsoft Office\\root\\Office16\\EXCEL.EXE\"\n",
    "    excel_paths = [r\"C:\\Program Files (x86)\\Microsoft Office\\root\\Office16\\EXCEL.EXE\", r\"C:\\Program Files\\Microsoft Office\\root\\Office16\\EXCEL.EXE\"]\n",
    "    \n",
    "    excel_program_path = None\n",
    "\n",
    "    for excel_path in excel_paths:\n",
    "\n",
    "        # Check if EXCEL.EXE exists in check_excel_path_1\n",
    "        if os.path.exists(excel_path):\n",
    "            excel_program_path = excel_path\n",
    "\n",
    "    # # Check if EXCEL.EXE exists in check_excel_path_2\n",
    "    # if not excel_path and os.path.exists(check_excel_path_2):\n",
    "    #     excel_path = check_excel_path_2\n",
    "\n",
    "    if not excel_program_path:\n",
    "        # Directories to search for EXCEL.EXE\n",
    "        directories_to_search = [\n",
    "            r\"C:\\Program Files\",\n",
    "            r\"C:\\Program Files (x86)\",\n",
    "            r\"C:\\\\\",\n",
    "            # Add more directories to search if needed\n",
    "        ]\n",
    "\n",
    "        excel_program_path = find_excel_exe(directories_to_search)\n",
    "\n",
    "    if excel_program_path:\n",
    "        subprocess.Popen([excel_program_path, file_path])\n",
    "    else:\n",
    "        print(\"Sorry. Cannot open file directly. Excel cannot be found\")\n",
    "\n",
    "def find_excel_exe(directories):\n",
    "    for directory in directories:\n",
    "        for root, dirs, files in os.walk(directory):\n",
    "            for file in files:\n",
    "                if file.lower() == 'excel.exe':\n",
    "                    return os.path.join(root, file)\n",
    "    return None\n",
    "\n",
    "def open_in_excel(filename):\n",
    "    if os.path.exists(filename):\n",
    "        print(f\"Report written to: {filename}\")\n",
    "        open_csv = input(f\"Open file in Excel? Y/N\")\n",
    "        if open_csv.lower() == \"y\":\n",
    "            open_csv_in_excel(filename)\n",
    "    else:\n",
    "        print(\"Error writing report.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Misc Support Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load a 2-column csv file as a pandas dataframe and return a dictionary \n",
    "\n",
    "def load_csv_to_dict(file_path, key_column, value_column):\n",
    "    \"\"\"\n",
    "    Load a CSV file and create a dictionary from specified key and value columns.\n",
    "\n",
    "    Parameters:\n",
    "    file_path (str): The path to the CSV file.\n",
    "    key_column (str): The name of the column to use as keys in the dictionary.\n",
    "    value_column (str): The name of the column to use as values in the dictionary.\n",
    "\n",
    "    Returns:\n",
    "    dict: A dictionary with keys from the key_column and values from the value_column.\n",
    "    \"\"\"\n",
    "    df = pd.read_csv(file_path)\n",
    "    \n",
    "    # Convert NaN values to empty strings\n",
    "    df[value_column] = df[value_column].apply(lambda x: '' if pd.isna(x) else str(x))\n",
    "    \n",
    "    return df.set_index(key_column)[value_column].to_dict()\n",
    "\n",
    "def ensure_string(text, string_length_threshold):\n",
    "    \"\"\"\n",
    "    Ensure the input is converted to a string representation.\n",
    "\n",
    "    Parameters:\n",
    "    text: The input to be checked and converted to a string if necessary.\n",
    "    string_length_threshold: The minimum length of the string. If the length of the string is less than this threshold, return an empty string.\n",
    "\n",
    "    Returns:\n",
    "    str: The input converted to a string or an empty string if the length is less than the threshold or conversion fails.\n",
    "    \"\"\"\n",
    "    if pd.isnull(text):\n",
    "        return \"\"\n",
    "    elif isinstance(text, str):\n",
    "        text = text.strip()  # Strip whitespace from both ends\n",
    "        if len(text) < string_length_threshold:\n",
    "            return \"\"\n",
    "        return text\n",
    "    \n",
    "    try:\n",
    "        # Attempt to convert non-string types to string\n",
    "        text = str(text).strip()\n",
    "        if len(text) < string_length_threshold:\n",
    "            return \"\"\n",
    "        return text\n",
    "    except (ValueError, TypeError):\n",
    "        return \"\"\n",
    "    \n",
    "def clean_whitespace(text):\n",
    "    # Remove leading and trailing whitespace\n",
    "    cleaned_text = text.strip()\n",
    "    \n",
    "    # Replace multiple spaces with a single space\n",
    "    cleaned_text = ' '.join(cleaned_text.split())\n",
    "    \n",
    "    return cleaned_text\n",
    "\n",
    "\n",
    "def save_dataframe_with_incremented_filename(file_path):\n",
    "    \"\"\"\n",
    "    Check if a filename already exists. If it does, it returns an incremented filename.\n",
    "\n",
    "    Parameters:\n",
    "    df (pd.DataFrame): The DataFrame to be saved.\n",
    "    file_path (str): The initial file path for the CSV file.\n",
    "    \"\"\"\n",
    "    base, extension = os.path.splitext(file_path)\n",
    "    counter = 1\n",
    "\n",
    "    # Check if the file already exists\n",
    "    while os.path.exists(file_path):\n",
    "        # Increment the file name\n",
    "        file_path = f\"{base}_{counter}{extension}\"\n",
    "        counter += 1\n",
    "\n",
    "    return file_path"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Text Analysis Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Lists and Settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stopword lisit\n",
    "external_stopword_list = r\"D:\\Data Analysis\\Qualitative Analysis\\Cleaning Text\\lists and dictionaries\\stopwords_seo.csv\"\n",
    "\n",
    "# Contractions dictionary\n",
    "external_contractions_dictionary = r\"D:\\Data Analysis\\Qualitative Analysis\\Cleaning Text\\lists and dictionaries\\contractions_list.csv\"\n",
    "\n",
    "# External search and replace file\n",
    "custom_search_and_replace_dictionary = r\"D:\\Data Analysis\\Qualitative Analysis\\Cleaning Text\\lists and dictionaries\\custom search-and-replace dictionary.csv\"\n",
    "\n",
    "# Load Custom Dictionaries\n",
    "external_stopword_list_dict = load_csv_to_dict(external_stopword_list, \"stopword\", \"replace\")\n",
    "contractions_dict = load_csv_to_dict(external_contractions_dictionary, \"expansion\", \"contraction\")\n",
    "custom_search_and_replace_dict = load_csv_to_dict(custom_search_and_replace_dictionary, \"search_term\", \"replace_term\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "common_encoding_errors = {\n",
    "    \"&lt;\": \"<\",        # Less-than sign\n",
    "    \"&gt;\": \">\",        # Greater-than sign\n",
    "    \"&amp;\": \"&\",       # Ampersand\n",
    "    \"&quot;\": \"\\\"\",     # Double quotation mark\n",
    "    \"&apos;\": \"'\",      # Apostrophe\n",
    "    \"&nbsp;\": \" \",      # Non-breaking space\n",
    "    \"&copy;\": \"(c)\",    # Copyright sign\n",
    "    \"&reg;\": \"(r)\",     # Registered sign\n",
    "    \"&euro;\": \"EUR\",    # Euro sign\n",
    "    \"&pound;\": \"GBP\",   # Pound sign\n",
    "    \"&yen;\": \"Yen\",     # Yen sign\n",
    "    \"&cent;\": \"c\",      # Cent sign\n",
    "    \"&sect;\": \"§\",      # Section sign\n",
    "    \"&para;\": \"¶\",      # Pilcrow sign\n",
    "    \"&deg;\": \"°\",       # Degree sign\n",
    "    \"&plusmn;\": \"±\",    # Plus-minus sign\n",
    "    \"&micro;\": \"µ\",     # Micro sign\n",
    "    \"&sup2;\": \"^2\",     # Superscript two\n",
    "    \"&sup3;\": \"^3\",     # Superscript three\n",
    "    \"&frac14;\": \"1/4\",  # Fraction one quarter\n",
    "    \"&frac12;\": \"1/2\",  # Fraction one half\n",
    "    \"&frac34;\": \"3/4\",  # Fraction three quarters\n",
    "    \"&times;\": \"x\",     # Multiplication sign\n",
    "    \"&divide;\": \"/\",    # Division sign\n",
    "    \"&bull;\": \"•\",      # Bullet\n",
    "    \"&ndash;\": \"-\",     # En dash\n",
    "    \"&mdash;\": \"-\",     # Em dash\n",
    "    \"&lsquo;\": \"'\",     # Left single quotation mark\n",
    "    \"&rsquo;\": \"'\",     # Right single quotation mark\n",
    "    \"&sbquo;\": \",\",     # Single low-9 quotation mark\n",
    "    \"&ldquo;\": \"\\\"\",    # Left double quotation mark\n",
    "    \"&rdquo;\": \"\\\"\",    # Right double quotation mark\n",
    "    \"&bdquo;\": \"\\\"\",    # Double low-9 quotation mark\n",
    "    \"&hellip;\": \"...\",  # Horizontal ellipsis\n",
    "    \"%20\": \" \",         # Space\n",
    "    \"%21\": \"!\",         # Exclamation mark\n",
    "    \"%22\": \"\\\"\",        # Double quotation mark\n",
    "    \"%23\": \"#\",         # Number sign\n",
    "    \"%24\": \"$\",         # Dollar sign\n",
    "    \"%25\": \"%\",         # Percent sign\n",
    "    \"%26\": \"&\",         # Ampersand\n",
    "    \"%27\": \"'\",         # Apostrophe\n",
    "    \"%28\": \"(\",         # Left parenthesis\n",
    "    \"%29\": \")\",         # Right parenthesis\n",
    "    \"%2A\": \"*\",         # Asterisk\n",
    "    \"%2B\": \"+\",         # Plus sign\n",
    "    \"%2C\": \",\",         # Comma\n",
    "    \"%2D\": \"-\",         # Hyphen\n",
    "    \"%2E\": \".\",         # Period\n",
    "    \"%2F\": \"/\",         # Slash\n",
    "    \"%3A\": \":\",         # Colon\n",
    "    \"%3B\": \";\",         # Semicolon\n",
    "    \"%3C\": \"<\",         # Less-than sign\n",
    "    \"%3D\": \"=\",         # Equal sign\n",
    "    \"%3E\": \">\",         # Greater-than sign\n",
    "    \"%3F\": \"?\",         # Question mark\n",
    "    \"%40\": \"@\",         # At sign\n",
    "    \"%5B\": \"[\",         # Left square bracket\n",
    "    \"%5C\": \"\\\\\",        # Backslash\n",
    "    \"%5D\": \"]\",         # Right square bracket\n",
    "    \"%5E\": \"^\",         # Caret\n",
    "    \"%5F\": \"_\",         # Underscore\n",
    "    \"%60\": \"`\",         # Grave accent\n",
    "    \"%7B\": \"{\",         # Left curly bracket\n",
    "    \"%7C\": \"|\",         # Vertical bar\n",
    "    \"%7D\": \"}\",         # Right curly bracket\n",
    "    \"%7E\": \"~\",         # Tilde\n",
    "    \"â€”\": \"-\",         # Em dash\n",
    "    \"â€“\": \"–\",         # En dash\n",
    "    \"â€˜\": \"'\",         # Left single quotation mark\n",
    "    \"â€™\": \"'\",         # Right single quotation mark\n",
    "    \"â€œ\": \"\\\"\",        # Left double quotation mark\n",
    "    \"â€\": \"\\\"\",         # Right double quotation mark\n",
    "    \"â€¦\": \"...\",       # Ellipsis\n",
    "    \"â€\": \"\\\"\",         # Right double quotation mark (alternate)\n",
    "    \"â€™\": \"'\",         # Right single quotation mark (alternate)\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Text Cleaning Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Main Text cleaning function\n",
    "def clean_text(text):\n",
    "    global text_cleaning_options\n",
    "\n",
    "    # Set Cleaning Options\n",
    "    transform_contractions = text_cleaning_options[\"Transform contractions\"]        \n",
    "    convert_to_lowercase = text_cleaning_options[\"Convert to lowercase\"]\n",
    "    covert_common_encoding_errors = text_cleaning_options[\"Convert Common Encoding Errors\"]\n",
    "    \n",
    "    strip_punctuation = text_cleaning_options[\"Strip punctuation\"]\n",
    "    remove_stopwords = text_cleaning_options[\"Remove stopwords\"]\n",
    "\n",
    "    use_custom_sar_dict = text_cleaning_options[\"Use custom Search-and-Replace dictionary\"]\n",
    "    standardize_highway_references = text_cleaning_options[\"Standardize highway references\"]\n",
    "    standardize_drivebc_references  = text_cleaning_options[\"Standardize drivebc references\"]\n",
    "    spell_check = text_cleaning_options[\"Spell check\"]\n",
    "    stem_words  = text_cleaning_options[\"Stem words\"]\n",
    "    lemmatize_words = text_cleaning_options[\"Lemmatize words\"]\n",
    "\n",
    "    try:\n",
    "        # Make sure the data is actually text\n",
    "        text = ensure_string(text,3)\n",
    "\n",
    "        # Replace contractions\n",
    "        if transform_contractions == \"Expand\":\n",
    "            # Expand contractions\n",
    "            for key, value in contractions_dict.items():\n",
    "                text = text.replace(value, key)\n",
    "            # Contract expansions\n",
    "        elif transform_contractions == \"Contract\":\n",
    "                for key, value in contractions_dict.items():\n",
    "                    text = text.replace(key, value)\n",
    "\n",
    "        if covert_common_encoding_errors is True:\n",
    "            # Perform character replacement using the dictionary\n",
    "            for improper, proper in common_encoding_errors.items():\n",
    "                text = text.replace(improper, proper)\n",
    "        \n",
    "        # Convert all text to lower case\n",
    "        if convert_to_lowercase is True:\n",
    "            text = text.lower()\n",
    "\n",
    "        \n",
    "        # Standardize Highway References\n",
    "        if standardize_highway_references is True:\n",
    "            regex_highway_search1 = re.compile(\n",
    "                r\"#*\\b(bc *)*h[ighway]*[\\s#]*(\\d+[abcd]*)\\b\",\n",
    "                re.IGNORECASE,\n",
    "            )\n",
    "            regex_highway_replace1 = r\"BCHwy\\2\"\n",
    "            text = re.sub(regex_highway_search1, regex_highway_replace1, text)\n",
    "\n",
    "            regex_highway_search2 = re.compile(\n",
    "                r\"\\b(bc *)*h[ighway]*[\\s#]*\\b\",\n",
    "                re.IGNORECASE,\n",
    "            )\n",
    "            regex_highway_replace2 = r\"highway\"\n",
    "            text = re.sub(regex_highway_search2, regex_highway_replace2, text)\n",
    "\n",
    "        # Standardize DriveBC references\n",
    "        if standardize_drivebc_references is True:\n",
    "            regex_drivebc_search = re.compile(\n",
    "                r\"\\bdrive.*bc\\b\",\n",
    "                re.IGNORECASE,\n",
    "            )\n",
    "            regex_drivebc_replace = r\"drivebc\"\n",
    "            text = re.sub(regex_drivebc_search, regex_drivebc_replace, text)\n",
    "\n",
    "        # Use a custom search and replace dictionary\n",
    "        if use_custom_sar_dict is True:\n",
    "            for key, value in custom_search_and_replace_dict.items():\n",
    "                text = text.replace(key, value)\n",
    "\n",
    "        # Remove stop words\n",
    "        if remove_stopwords is True:\n",
    "            for key, value in external_stopword_list_dict.items():\n",
    "                text = text.replace(key, value)\n",
    "\n",
    "        # strip punctuation\n",
    "        if strip_punctuation is True:\n",
    "            # # This removed punctuation marks\n",
    "            # text = \" \".join([char for char in text if char not in string.punctuation])\n",
    "            \n",
    "            # This replaces punctuation marks with a space\n",
    "            translation_table = str.maketrans(string.punctuation, ' ' * len(string.punctuation))\n",
    "            # Replace punctuation with spaces\n",
    "            text = text.translate(translation_table)\n",
    "\n",
    "\n",
    "        if spell_check is True:\n",
    "            text = text.split()\n",
    "            text = [spell(word) for word in text]\n",
    "            text = \" \".join(text)\n",
    "\n",
    "        # Stem words\n",
    "        if stem_words is True:\n",
    "            text = text.split()\n",
    "            text = [ps.stem(word) for word in text]\n",
    "            text = \" \".join(text)\n",
    "\n",
    "        # Lemmatize words\n",
    "        if lemmatize_words is True:\n",
    "            text = text.split()\n",
    "            text = [wn.lemmatize(word) for word in text]\n",
    "            text = \" \".join(text)\n",
    "\n",
    "        # Clean Whitespace\n",
    "        text = clean_whitespace(text)\n",
    "\n",
    "        return text\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing text: {e}\")\n",
    "        return \"\"\n",
    "\n",
    "\n",
    "# Add clean text to dataframe\n",
    "def add_clean_text_column(df,comments):\n",
    "    # print(df)\n",
    "    df[\"clean_text\"] = comments.apply(lambda x: clean_text(x))\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### N-Gram Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ngram_counter(series):\n",
    "    \"\"\"\n",
    "    Counts and returns the most common n-grams in a pandas Series of text data.\n",
    "\n",
    "    The function prompts the user to input the starting and ending n-gram lengths, \n",
    "    as well as the number of top n-grams to find for each length. It then processes \n",
    "    the text data, tokenizes it, generates n-grams for the specified lengths, and \n",
    "    counts their occurrences. Finally, it returns a DataFrame with the n-grams, \n",
    "    their types, and counts.\n",
    "\n",
    "    Parameters:\n",
    "    series (pandas.Series): A pandas Series containing text data.\n",
    "\n",
    "    Returns:\n",
    "    pandas.DataFrame: A DataFrame with columns 'N-Gram', 'N-Gram Type', and 'Count',\n",
    "                      containing the most common n-grams and their counts for each\n",
    "                      specified n-gram length.\n",
    "\n",
    "    User Inputs:\n",
    "    - Starting n-gram length: An integer specifying the smallest n-gram length (e.g., \n",
    "                              1 for monogram, 2 for bigram, etc.).\n",
    "    - Ending n-gram length: An integer specifying the largest n-gram length.\n",
    "    - Number of n-grams: An integer specifying the number of top n-grams to find \n",
    "                         for each n-gram length.\n",
    "\n",
    "    Example:\n",
    "    >>> import pandas as pd\n",
    "    >>> from nltk.tokenize import word_tokenize\n",
    "    >>> from nltk.util import ngrams\n",
    "    >>> from collections import Counter\n",
    "    >>> series = pd.Series([\"This is a sample text.\", \"This text is for testing purposes.\"])\n",
    "    >>> result = ngram_counter(series)\n",
    "    >>> print(result)\n",
    "               N-Gram N-Gram Type  Count\n",
    "    0            this       1-gram      2\n",
    "    1              is       1-gram      2\n",
    "    2              a       1-gram      1\n",
    "    3          sample       1-gram      1\n",
    "    4            text       1-gram      2\n",
    "    5           this is     2-gram      2\n",
    "    6              is a     2-gram      1\n",
    "    7          sample text  2-gram      1\n",
    "    8            text is    2-gram      1\n",
    "    9              is for   2-gram      1\n",
    "    10           for testing 2-gram     1\n",
    "    11 testing purposes 2-gram  1\n",
    "    \"\"\"\n",
    "    # Prompt user for the starting and ending n-gram lengths\n",
    "    os.system(\"cls\")\n",
    "    while True:\n",
    "        try:\n",
    "            start_length = int(input(\"Enter the starting n-gram length (1 for monogram, 2 for bigram, etc.): \"))\n",
    "            break\n",
    "        except ValueError:\n",
    "            print(\"Invalid input. Please enter a valid integer.\")\n",
    "    \n",
    "    while True:\n",
    "        try:\n",
    "            end_length = int(input(\"Enter the ending n-gram length: \"))\n",
    "            break\n",
    "        except ValueError:\n",
    "            print(\"Invalid input. Please enter a valid integer.\")\n",
    "\n",
    "    # Prompt user for the number of n-grams to find\n",
    "    while True:\n",
    "        try:\n",
    "            num_ngrams = int(input(\"Enter the number of n-grams to find for each type: \"))\n",
    "            break\n",
    "        except ValueError:\n",
    "            print(\"Invalid input. Please enter a valid integer.\")\n",
    "\n",
    "    ngrams_list = []\n",
    "\n",
    "    # Process each n-gram length\n",
    "    for n in range(start_length, end_length + 1):\n",
    "        all_tokens = []\n",
    "\n",
    "        # Tokenize each row and filter out rows with fewer words than the n-gram length\n",
    "        for text in series.dropna().str.lower():\n",
    "            tokens = word_tokenize(text)\n",
    "            if len(tokens) >= n:\n",
    "                all_tokens.extend(tokens)\n",
    "        \n",
    "        print(f\"Tokens for n={n}:\", all_tokens)  # Debug: Print tokens\n",
    "\n",
    "        # Create n-grams\n",
    "        generated_ngrams = list(ngrams(all_tokens, n))\n",
    "        print(f\"Generated {n}-grams:\", generated_ngrams)  # Debug: Print generated n-grams\n",
    "\n",
    "        ngram_counts = Counter(generated_ngrams)\n",
    "        print(f\"N-gram counts for n={n}:\", ngram_counts)  # Debug: Print n-gram counts\n",
    "\n",
    "        # Get the top n-grams of each type\n",
    "        top_ngrams = ngram_counts.most_common(num_ngrams)\n",
    "        for ngram, count in top_ngrams:\n",
    "            ngrams_list.append([' '.join(ngram), f\"{n}-gram\", count])\n",
    "\n",
    "    # Create a DataFrame with the n-grams, their types, and counts\n",
    "    result_df = pd.DataFrame(ngrams_list, columns=['N-Gram', 'N-Gram Type', 'Count'])\n",
    "\n",
    "    return result_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Isolate Parts of Speech Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def isolate_pos(series, selected_parts_of_speech, string_length_threshold):\n",
    "    if not selected_parts_of_speech:\n",
    "        print(\"No Parts of Speech have been selected. Please select them and try again.\")\n",
    "        input(f\"Press 'Enter' to continue.\")\n",
    "        return\n",
    "\n",
    "    if isinstance(series, pd.Series):\n",
    "        series = series.to_string()\n",
    "\n",
    "    # Tokenize the sentence into individual words\n",
    "    tokens = nltk.word_tokenize(series)\n",
    "\n",
    "    # Use NLTK's POS tagger to tag each word with its part of speech\n",
    "    pos_tags = nltk.pos_tag(tokens)\n",
    "\n",
    "    # Create a list of words filtered by chosen Part-of-Speech (PoS) type\n",
    "    word_list = []\n",
    "    print(\"Tagging parts of speech...\")\n",
    "    for word, pos in pos_tags:\n",
    "        if pos in selected_parts_of_speech:\n",
    "            word_list.append(word)\n",
    "\n",
    "    # Remove any stop words from the list\n",
    "    stop_words = [\n",
    "        \"..\",\n",
    "        \"'m\",\n",
    "        \"'s\",\n",
    "        \"#name?\",\n",
    "        \"are\",\n",
    "        \"be\",\n",
    "        \"been\",\n",
    "        \"being\",\n",
    "        \"did\",\n",
    "        \"do\",\n",
    "        \"does\",\n",
    "        \"done\",\n",
    "        \"get\",\n",
    "        \"go\",\n",
    "        \"had\",\n",
    "        \"has\",\n",
    "        \"have\",\n",
    "        \"i\",\n",
    "        \"ì\",\n",
    "        \"is\",\n",
    "        \"make\",\n",
    "        \"n/a\",\n",
    "        \"needs\",\n",
    "        \"put\",\n",
    "        \"Q\",\n",
    "        \"seems\",\n",
    "        \"take\",\n",
    "        \"use\",\n",
    "        \"vs\",\n",
    "        \"was\",\n",
    "        \"way\",\n",
    "        \"were\",\n",
    "    ]\n",
    "    cleaned_word_list = [element for element in word_list if element not in stop_words]\n",
    "\n",
    "    # Apply string length threshold\n",
    "    filtered_word_list = [word for word in cleaned_word_list if len(word) >= string_length_threshold]\n",
    "\n",
    "    # Count the frequency of each word\n",
    "    word_counts = pd.Series(filtered_word_list).value_counts().reset_index()\n",
    "\n",
    "    # Rename the columns of the DataFrame\n",
    "    word_counts.columns = [\"word\", \"count\"]\n",
    "\n",
    "    # Sort the DataFrame by count in descending order\n",
    "    word_counts = word_counts.sort_values(by=\"count\", ascending=False).reset_index(drop=True)\n",
    "\n",
    "    return word_counts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sentiment Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_sentiment_column(df, text_column):\n",
    "    \"\"\"\n",
    "    Add a sentiment score column to the DataFrame based on the specified text column using VADER.\n",
    "\n",
    "    Parameters:\n",
    "    df (pd.DataFrame): The original DataFrame.\n",
    "    text_column (str): The name of the column containing the text for sentiment analysis.\n",
    "\n",
    "    Returns:\n",
    "    pd.DataFrame: The DataFrame with an added \"sentiment\" score column.\n",
    "    \"\"\"\n",
    "    # Ensure the text_column exists in the DataFrame\n",
    "    if text_column not in df.columns:\n",
    "        raise ValueError(f\"Column '{text_column}' not found in DataFrame columns.\")\n",
    "\n",
    "    # Initialize the VADER sentiment intensity analyzer\n",
    "    analyzer = SentimentIntensityAnalyzer()\n",
    "\n",
    "    # Define a function to calculate the sentiment score\n",
    "    def get_sentiment(text):\n",
    "        if pd.isnull(text) or not isinstance(text, str) or text.strip() == \"\":\n",
    "            return None  # Return None for rows without valid text\n",
    "        sentiment = analyzer.polarity_scores(text)\n",
    "        return sentiment['compound']  # Compound sentiment score\n",
    "\n",
    "    # Apply the sentiment function to the text column and add the results to a new column\n",
    "    df['sentiment'] = df[text_column].apply(get_sentiment)\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Thematic Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_themes_and_comments(theme_list_path, comments_df, comments_column):\n",
    "    # Load the theme list CSV file into a DataFrame\n",
    "    theme_list_df = pd.read_csv(theme_list_path)\n",
    "\n",
    "    # Create a dictionary to map terms to their associated themes\n",
    "    term_to_themes = {}\n",
    "    for _, row in theme_list_df.iterrows():\n",
    "        terms = row['term'].split(',')\n",
    "        themes = row['theme'].split(',')\n",
    "        for term in terms:\n",
    "            term = term.strip()\n",
    "            if term not in term_to_themes:\n",
    "                term_to_themes[term] = set()\n",
    "            for theme in themes:\n",
    "                term_to_themes[term].add(theme.strip())\n",
    "\n",
    "    # Create new columns in comments_df for each unique theme\n",
    "    unique_themes = set()\n",
    "    for themes in term_to_themes.values():\n",
    "        unique_themes.update(themes)\n",
    "\n",
    "    for theme in unique_themes:\n",
    "        comments_df[theme] = 0  # Initialize all theme columns with 0\n",
    "\n",
    "    # Define a function to process each comment\n",
    "    def process_comment(comment):\n",
    "        if pd.isnull(comment):\n",
    "            return {theme: 0 for theme in unique_themes}\n",
    "        comment = comment.lower()\n",
    "        result = {theme: 0 for theme in unique_themes}\n",
    "        for term, themes in term_to_themes.items():\n",
    "            if term.lower() in comment:\n",
    "                for theme in themes:\n",
    "                    result[theme] = 1\n",
    "        return result\n",
    "\n",
    "    # Check if comments_column exists\n",
    "    if comments_column not in comments_df.columns:\n",
    "        raise KeyError(f\"Column '{comments_column}' not found in comments_df\")\n",
    "\n",
    "    # Apply the function to each row in the comments column\n",
    "    results = comments_df[comments_column].apply(process_comment)\n",
    "\n",
    "    # Update the comments_df with the results\n",
    "    for theme in unique_themes:\n",
    "        comments_df[theme] = results.apply(lambda x: x.get(theme, 0))\n",
    "\n",
    "    return comments_df\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Correlation Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_correlation(df, threshold):\n",
    "    # Identify numeric columns\n",
    "    numeric_cols = df.select_dtypes(include=['number']).columns\n",
    "    \n",
    "    if len(numeric_cols) < 2:\n",
    "        raise ValueError(\"Not enough numeric columns to compute correlations.\")\n",
    "    \n",
    "    # Compute the correlation matrix\n",
    "    corr_matrix = df[numeric_cols].corr()\n",
    "    \n",
    "    # Apply the threshold to the correlation matrix\n",
    "    filtered_corr_matrix = corr_matrix.applymap(lambda x: x if abs(x) >= threshold else np.nan)\n",
    "    \n",
    "    # Create a heatmap of the filtered correlation matrix\n",
    "    plt.figure(figsize=(10, 8))\n",
    "    sns.heatmap(filtered_corr_matrix, annot=True, cmap='coolwarm', vmin=-1, vmax=1, mask=filtered_corr_matrix.isnull())\n",
    "    plt.title('Correlation Matrix Heatmap (Filtered by Threshold)')\n",
    "    plt.show()\n",
    "    \n",
    "    # Create a list of column pairs with their correlation scores\n",
    "    corr_pairs = []\n",
    "    for i in range(len(numeric_cols)):\n",
    "        for j in range(i+1, len(numeric_cols)):\n",
    "            col1 = numeric_cols[i]\n",
    "            col2 = numeric_cols[j]\n",
    "            corr_score = corr_matrix.loc[col1, col2]\n",
    "            if abs(corr_score) >= threshold:\n",
    "                corr_pairs.append(((col1, col2), corr_score))\n",
    "    \n",
    "    # Sort the list of column pairs by correlation score in descending order\n",
    "    corr_pairs_sorted = sorted(corr_pairs, key=lambda x: abs(x[1]), reverse=True)\n",
    "    \n",
    "    return corr_pairs_sorted\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Latent Dirichlet Allocation (LDA) - Not used in this notebook. Requires additional packages. Kept here for reference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lda_analysis(series, num_topics=3, passes=10):\n",
    "    \"\"\"\n",
    "    Perform LDA analysis on a pandas Series of text data.\n",
    "\n",
    "    Parameters:\n",
    "    series (pd.Series): A pandas Series containing text data.\n",
    "    num_topics (int): The number of topics to extract. Default is 3.\n",
    "    passes (int): The number of passes through the corpus during training. Default is 10.\n",
    "\n",
    "    Returns:\n",
    "    dict: A dictionary containing the LDA model, the document-topic matrix, \n",
    "          and the topic-term matrix for visualization.\n",
    "    \"\"\"\n",
    "    # Define stopwords\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "\n",
    "    # Preprocess text: tokenize, remove stopwords, punctuation, and non-alphabetic tokens\n",
    "    def preprocess(text):\n",
    "        tokens = word_tokenize(text.lower())\n",
    "        tokens = [word for word in tokens if word.isalpha() and word not in stop_words]\n",
    "        return tokens\n",
    "\n",
    "    # Apply preprocessing to the series\n",
    "    processed_docs = series.dropna().apply(preprocess)\n",
    "\n",
    "    # Create a dictionary and a corpus\n",
    "    dictionary = corpora.Dictionary(processed_docs)\n",
    "    corpus = [dictionary.doc2bow(doc) for doc in processed_docs]\n",
    "\n",
    "    # Train the LDA model\n",
    "    lda_model = LdaModel(corpus, num_topics=num_topics, id2word=dictionary, passes=passes)\n",
    "\n",
    "    # Assign topics to documents\n",
    "    document_topics = lda_model.get_document_topics(corpus)\n",
    "    document_topic_df = pd.DataFrame([[dict(doc)] for doc in document_topics])\n",
    "    document_topic_df.columns = [f'Topic_{i + 1}' for i in range(num_topics)]\n",
    "    document_topic_df['Document'] = series.reset_index(drop=True)\n",
    "\n",
    "    # Prepare the visualization\n",
    "    lda_vis = gensimvis.prepare(lda_model, corpus, dictionary)\n",
    "\n",
    "    # Collect results in a dictionary\n",
    "    results = {\n",
    "        \"lda_model\": lda_model,\n",
    "        \"document_topic_df\": document_topic_df,\n",
    "        \"lda_vis\": lda_vis\n",
    "    }\n",
    "\n",
    "    return results\n",
    "\n",
    "# # Example usage:\n",
    "# series = pd.Series([\n",
    "#     \"This is the first document.\",\n",
    "#     \"This document is the second document.\",\n",
    "#     \"And this is the third one.\",\n",
    "#     \"Is this the first document?\"\n",
    "# ])\n",
    "\n",
    "# # Perform LDA analysis\n",
    "# results = lda_analysis(series, num_topics=3, passes=10)\n",
    "\n",
    "# # Access the results\n",
    "# lda_model = results['lda_model']\n",
    "# document_topic_df = results['document_topic_df']\n",
    "# lda_vis = results['lda_vis']\n",
    "\n",
    "# # Print the topics\n",
    "# for idx, topic in lda_model.print_topics(-1):\n",
    "#     print(f\"Topic {idx + 1}: {topic}\")\n",
    "\n",
    "# # Display the visualization in a Jupyter notebook (uncomment if using Jupyter)\n",
    "# # pyLDAvis.display(lda_vis)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Demo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load CSV Comments to DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = load_file_to_dataframe(r\"D:\\Data Analysis\\Qualitative Analysis\\comments_for_demo.csv\")\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Select the Comments Column From DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define series\n",
    "comments = df[\"Comment\"]\n",
    "comments"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## N-Grams - First Pass (Noisy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NGrams - Uncleaned\n",
    "base_ngram_report = r\"D:\\Data Analysis\\Qualitative Analysis\\reports\\ngram_report.csv\"\n",
    "ngram_report = save_dataframe_with_incremented_filename(base_ngram_report)\n",
    "return_ngrams = ngram_counter(comments)\n",
    "return_ngrams.to_csv(ngram_report,index=False)\n",
    "open_in_excel(ngram_report)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Text Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_cleaning_options = {\n",
    "    \"Convert to lowercase\": True,\n",
    "    \"Convert Common Encoding Errors\": True,\n",
    "    \"Strip punctuation\": True,\n",
    "    \"Remove stopwords\": False,\n",
    "    \"Transform contractions\": \"Expand\", \n",
    "    \"Use custom Search-and-Replace dictionary\": True,\n",
    "    \"Standardize highway references\": True,\n",
    "    \"Standardize drivebc references\": True,\n",
    "    \"Spell check\": False,\n",
    "    \"Stem words\": False,\n",
    "    \"Lemmatize words\": False,\n",
    "}\n",
    "cleaned_text = add_clean_text_column(df, comments)\n",
    "cleaned_comments = cleaned_text[\"clean_text\"]\n",
    "\n",
    "base_cleaned_text_report = r\"D:\\Data Analysis\\Qualitative Analysis\\reports\\cleaned_text_report.csv\"\n",
    "cleaned_text_report = save_dataframe_with_incremented_filename(base_cleaned_text_report)\n",
    "\n",
    "cleaned_text.to_csv(cleaned_text_report,index=False)\n",
    "open_in_excel(cleaned_text_report)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## N-Grams - Second Pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NGrams - Cleaned\n",
    "# print(type(comments))\n",
    "# print(type(cleaned_comments))\n",
    "base_ngram_report_cleaned = r\"D:\\Data Analysis\\Qualitative Analysis\\reports\\ngram_report_cleaned.csv\"\n",
    "return_ngrams = ngram_counter(cleaned_comments)\n",
    "ngram_report = save_dataframe_with_incremented_filename(base_ngram_report_cleaned)\n",
    "return_ngrams.to_csv(ngram_report,index=False)\n",
    "open_in_excel(ngram_report)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parts of Speech"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_verbs = [\"VB\",\"VBD\",\"VBG\",\"VBN\",\"VBP\",\"VBZ\"]\n",
    "all_nouns = [\"NN\",\"NNP\",\"NNS\"]\n",
    "all_nouns_and_verbs = [\"NN\",\"NNP\",\"NNS\",\"VB\",\"VBD\",\"VBG\",\"VBN\",\"VBP\",\"VBZ\"]\n",
    "\n",
    "parts_of_speech = [\n",
    "    (\"CC\", \"conjunction, coordinating\"),\n",
    "    (\"CD\", \"numeral, cardinal\"),\n",
    "    (\"DT\", \"determiner\"),\n",
    "    (\"EX\", \"existential there\"),\n",
    "    (\"IN\", \"preposition or conjunction, subordinating\"),\n",
    "    (\"JJ\", \"adjective or numeral, ordinal\"),\n",
    "    (\"JJR\", \"adjective, comparative\"),\n",
    "    (\"JJS\", \"adjective, superlative\"),\n",
    "    (\"LS\", \"list item marker\"),\n",
    "    (\"MD\", \"modal auxiliary\"),\n",
    "    (\"NN\", \"noun, common, singular or mass\"),\n",
    "    (\"NNP\", \"noun, proper, singular\"),\n",
    "    (\"NNS\", \"noun, common, plural\"),\n",
    "    (\"PDT\", \"pre-determiner\"),\n",
    "    (\"POS\", \"genitive marker\"),\n",
    "    (\"PRP\", \"pronoun, personal\"),\n",
    "    (\"RB\", \"adverb\"),\n",
    "    (\"RBR\", \"adverb, comparative\"),\n",
    "    (\"RBS\", \"adverb, superlative\"),\n",
    "    (\"RP\", \"particle\"),\n",
    "    (\"TO\", \"to as preposition or infinitive marker\"),\n",
    "    (\"UH\", \"interjection\"),\n",
    "    (\"VB\", \"verb, base form\"),\n",
    "    (\"VBD\", \"verb, past tense\"),\n",
    "    (\"VBG\", \"verb, present participle or gerund\"),\n",
    "    (\"VBN\", \"verb, past participle\"),\n",
    "    (\"VBP\", \"verb, present tense, not 3rd person singular\"),\n",
    "    (\"VBZ\", \"verb, present tense, 3rd person singular\"),\n",
    "    (\"WDT\", \"WH-determiner\"),\n",
    "    (\"WP\", \"WH-pronoun\"),\n",
    "    (\"WRB\", \"Wh-adverb\")\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pos_df = isolate_pos(cleaned_comments, all_nouns_and_verbs, 3)\n",
    "\n",
    "# Specify the length threshold\n",
    "string_length_threshold = 3\n",
    "\n",
    "# Filter out rows where the length of the string in the \"word\" column is less than the threshold\n",
    "filtered_pos_df = pos_df[pos_df['word'].apply(lambda x: len(x) >= string_length_threshold)]\n",
    "\n",
    "base_pos_text_report = r\"D:\\Data Analysis\\Qualitative Analysis\\reports\\parts_of_speech_text_report.csv\"\n",
    "pos_text_report = save_dataframe_with_incremented_filename(base_pos_text_report)\n",
    "\n",
    "filtered_pos_df.to_csv(pos_text_report,index=False)\n",
    "open_in_excel(pos_text_report)   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sentiment Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add sentiment column to the DataFrame\n",
    "df_with_sentiment = add_sentiment_column(cleaned_text, 'Comment')\n",
    "\n",
    "base_sentiment_report = r\"D:\\Data Analysis\\Qualitative Analysis\\reports\\sentiment_report.csv\"\n",
    "sentiment_report = save_dataframe_with_incremented_filename(base_sentiment_report)\n",
    "\n",
    "df_with_sentiment.to_csv(sentiment_report,index=False)\n",
    "open_in_excel(sentiment_report)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Thematic Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "theme_list_path = r\"D:\\Data Analysis\\Qualitative Analysis\\Cleaning Text\\lists and dictionaries\\theme_list.csv\"\n",
    "comments_df = df_with_sentiment\n",
    "comments_column = \"clean_text\"\n",
    "theme_df = process_themes_and_comments(theme_list_path, comments_df, comments_column)\n",
    "\n",
    "base_thematic_analysis_report = r\"D:\\Data Analysis\\Qualitative Analysis\\reports\\thematic_analysis_report.csv\"\n",
    "thematic_analysis_report = save_dataframe_with_incremented_filename(base_thematic_analysis_report)\n",
    "\n",
    "theme_df.to_csv(thematic_analysis_report,index=False)\n",
    "open_in_excel(thematic_analysis_report)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Correlation Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "correlation_df = pd.read_csv(r\"D:\\Data Analysis\\Qualitative Analysis\\reports\\thematic_analysis_report.csv\")\n",
    "threshold = 0.1\n",
    "correlations = analyze_correlation(df, threshold)\n",
    "for (col1, col2), score in correlations:\n",
    "    print(f\"{col1} and {col2}: {score:.2f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
