{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the Code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports and Paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import chardet\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import sys\n",
    "# sys.path.append(r'C:\\Users\\Windows\\Dropbox\\James\\Python\\01_Media Archive Scripts')\n",
    "from dataframe_handler import DataFrameHandler\n",
    "read_file = r'C:\\Users\\Windows\\Downloads\\archive (7)\\netflix1.csv'\n",
    "from IPython.display import display"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set Pandas Display Options"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set options to display more columns and rows\n",
    "pd.set_option('display.max_columns', None)  # Show all columns\n",
    "pd.set_option('display.max_rows', None)     # Show all rows (if applicable)\n",
    "pd.set_option('display.max_colwidth', None) # Show full column width\n",
    "pd.set_option('display.width', 1000)        # Set the width of the display"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load to dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def detect_encoding(file_path):\n",
    "    if not os.path.isfile(file_path):\n",
    "        raise FileNotFoundError(f\"The file at {file_path} does not exist.\")\n",
    "    \n",
    "    try:\n",
    "        with open(file_path, 'rb') as file:\n",
    "            result = chardet.detect(file.read(10000))\n",
    "        return result['encoding']\n",
    "    except IOError as e:\n",
    "        raise IOError(f\"An error occurred while reading the file: {e}\")\n",
    "\n",
    "def detect_delimiter(file_path, sample_size=1000):\n",
    "    delimiters = [',', '\\t', ';', '|', ' ']\n",
    "    delimiter_counts = {delim: 0 for delim in delimiters}\n",
    "    \n",
    "    with open(file_path, 'r', encoding='utf-8', errors='ignore') as file:\n",
    "        sample = file.read(sample_size)\n",
    "    \n",
    "    for delim in delimiters:\n",
    "        delimiter_counts[delim] = sample.count(delim)\n",
    "    \n",
    "    # Handle the case where multiple delimiters have similar counts\n",
    "    most_common_delim = max(delimiter_counts, key=delimiter_counts.get)\n",
    "    \n",
    "    return most_common_delim\n",
    "\n",
    "def read_file(file_path, encoding='utf-8', sheet_name=None, force_plain_text=False):\n",
    "    _, file_extension = os.path.splitext(file_path)\n",
    "    file_extension = file_extension.lower()[1:]\n",
    "    \n",
    "    try:\n",
    "        if file_extension in ['csv', 'tsv', 'txt', 'log']:\n",
    "            if force_plain_text:\n",
    "                print(f\"Loading {file_path} as plain text\")\n",
    "                with open(file_path, 'r', encoding=encoding, errors='ignore') as file:\n",
    "                    content = file.read()\n",
    "                return pd.DataFrame({'content': [content]})\n",
    "            else:\n",
    "                delimiter = detect_delimiter(file_path)\n",
    "                if delimiter:\n",
    "                    print(f\"Detected delimiter for {file_path}: '{delimiter}'\")\n",
    "                    return pd.read_csv(file_path, delimiter=delimiter, encoding=encoding, on_bad_lines='skip')\n",
    "                else:\n",
    "                    print(f\"No clear delimiter detected for {file_path}, reading as single column\")\n",
    "                    return pd.read_csv(file_path, delimiter='\\n', encoding=encoding, header=None, names=['content'])\n",
    "        elif file_extension in ['xls', 'xlsx']:\n",
    "            return pd.read_excel(file_path, sheet_name=sheet_name, engine='openpyxl')\n",
    "        elif file_extension == 'json':\n",
    "            return pd.read_json(file_path, encoding=encoding)\n",
    "        elif file_extension == 'parquet':\n",
    "            return pd.read_parquet(file_path)\n",
    "        elif file_extension == 'hdf':\n",
    "            return pd.read_hdf(file_path)\n",
    "        elif file_extension == 'feather':\n",
    "            return pd.read_feather(file_path)\n",
    "        else:\n",
    "            raise ValueError(\"Unsupported file type.\")\n",
    "    except (UnicodeDecodeError, pd.errors.ParserError) as e:\n",
    "        detected_encoding = detect_encoding(file_path)\n",
    "        try:\n",
    "            if file_extension in ['csv', 'json', 'txt', 'log']:\n",
    "                if force_plain_text:\n",
    "                    print(f\"Reattempting with detected encoding '{detected_encoding}' as plain text\")\n",
    "                    with open(file_path, 'r', encoding=detected_encoding, errors='ignore') as file:\n",
    "                        content = file.read()\n",
    "                    return pd.DataFrame({'content': [content]})\n",
    "                else:\n",
    "                    delimiter = detect_delimiter(file_path)\n",
    "                    if delimiter:\n",
    "                        print(f\"Reattempting with detected encoding '{detected_encoding}' and delimiter '{delimiter}'\")\n",
    "                        return pd.read_csv(file_path, delimiter=delimiter, encoding=detected_encoding, on_bad_lines='skip')\n",
    "                    else:\n",
    "                        print(f\"Reattempting with detected encoding '{detected_encoding}' and single column\")\n",
    "                        return pd.read_csv(file_path, delimiter='\\n', encoding=detected_encoding, header=None, names=['content'])\n",
    "            else:\n",
    "                raise RuntimeError(\"Unsupported file type for encoding detection.\")\n",
    "        except Exception as e:\n",
    "            raise RuntimeError(f\"An error occurred while loading the file: {e}\")\n",
    "\n",
    "def load_to_dataframe(file_or_folder_path, file_type=None, sheet_name=None, force_plain_text=False):\n",
    "    if os.path.isdir(file_or_folder_path):\n",
    "        if file_type is None:\n",
    "            raise ValueError(\"File type must be specified when a folder is provided.\")\n",
    "        \n",
    "        dfs = []\n",
    "        columns_set = set()  # To keep track of all columns across files\n",
    "        \n",
    "        for root, _, files in os.walk(file_or_folder_path):\n",
    "            for file in files:\n",
    "                if file.lower().endswith(file_type.lower()):\n",
    "                    file_path = os.path.join(root, file)\n",
    "                    print(f\"Processing file: {file_path}\")\n",
    "                    try:\n",
    "                        df = read_file(file_path, sheet_name=sheet_name, force_plain_text=force_plain_text)\n",
    "                        columns_set.update(df.columns)\n",
    "                        dfs.append(df)\n",
    "                    except Exception as e:\n",
    "                        print(f\"Warning: Could not process file {file_path}: {e}\")\n",
    "                        try:\n",
    "                            with open(file_path, 'r', encoding='utf-8', errors='ignore') as file:\n",
    "                                content = file.read()\n",
    "                            df = pd.DataFrame({'content': [content]})\n",
    "                            dfs.append(df)\n",
    "                        except Exception as fallback_e:\n",
    "                            print(f\"Fallback failed for file {file_path}: {fallback_e}\")\n",
    "\n",
    "        if not dfs:\n",
    "            raise ValueError(\"No valid files found in the specified folder.\")\n",
    "        \n",
    "        # Standardize columns across all DataFrames\n",
    "        standardized_dfs = []\n",
    "        for df in dfs:\n",
    "            # Add missing columns\n",
    "            missing_cols = columns_set - set(df.columns)\n",
    "            for col in missing_cols:\n",
    "                df[col] = pd.NA\n",
    "            # Reorder columns\n",
    "            df = df[list(columns_set)]\n",
    "            standardized_dfs.append(df)\n",
    "\n",
    "        combined_df = pd.concat(standardized_dfs, ignore_index=True)\n",
    "        return combined_df\n",
    "\n",
    "    elif os.path.isfile(file_or_folder_path):\n",
    "        return read_file(file_or_folder_path, sheet_name=sheet_name, force_plain_text=force_plain_text)\n",
    "    \n",
    "    else:\n",
    "        raise ValueError(f\"The path {file_or_folder_path} is neither a file nor a folder.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Open in Excel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def open_csv_in_excel(file_path):\n",
    "    \"\"\"\n",
    "    Opens a CSV file in Microsoft Excel.\n",
    "\n",
    "    This function attempts to find the Microsoft Excel executable (`EXCEL.EXE`) on the system\n",
    "    and uses it to open the specified CSV file. It first checks common installation paths for\n",
    "    Excel and then searches additional directories if necessary.\n",
    "\n",
    "    Args:\n",
    "        file_path (str): The path to the CSV file that needs to be opened in Excel.\n",
    "\n",
    "    Returns:\n",
    "        None\n",
    "\n",
    "    Raises:\n",
    "        FileNotFoundError: If the file_path does not point to a valid file.\n",
    "        RuntimeError: If Excel cannot be found on the system or if there is an issue opening the file.\n",
    "\n",
    "    Notes:\n",
    "        - This function assumes that Microsoft Excel is installed on the system.\n",
    "        - The `find_excel_exe` function is used to search for Excel in common installation directories.\n",
    "        - The function prints an error message if Excel cannot be found or if there is an issue opening the file.\n",
    "    \"\"\"\n",
    "\n",
    "    excel_paths = [r\"C:\\Program Files (x86)\\Microsoft Office\\root\\Office16\\EXCEL.EXE\", r\"C:\\Program Files\\Microsoft Office\\root\\Office16\\EXCEL.EXE\"]\n",
    "    \n",
    "    excel_program_path = None\n",
    "\n",
    "    for excel_path in excel_paths:\n",
    "\n",
    "        # Check if EXCEL.EXE exists in excel_paths\n",
    "        if os.path.exists(excel_path):\n",
    "            excel_program_path = excel_path\n",
    "\n",
    "    if not excel_program_path:\n",
    "        # Directories to search for EXCEL.EXE\n",
    "        directories_to_search = [\n",
    "            r\"C:\\Program Files\",\n",
    "            r\"C:\\Program Files (x86)\",\n",
    "            r\"C:\\\\\",\n",
    "            # Add more directories to search if needed\n",
    "        ]\n",
    "\n",
    "        excel_program_path = find_excel_exe(directories_to_search)\n",
    "\n",
    "    if excel_program_path:\n",
    "        subprocess.Popen([excel_program_path, file_path])\n",
    "    else:\n",
    "        print(\"Sorry. Cannot open file directly. Excel cannot be found\")\n",
    "\n",
    "def find_excel_exe(directories):\n",
    "    for directory in directories:\n",
    "        for root, dirs, files in os.walk(directory):\n",
    "            for file in files:\n",
    "                if file.lower() == 'excel.exe':\n",
    "                    return os.path.join(root, file)\n",
    "    return None\n",
    "\n",
    "def open_in_excel(filename):\n",
    "    if os.path.exists(filename):\n",
    "        print(f\"Report written to: {filename}\")\n",
    "        open_csv = input(f\"Open file in Excel? Y/N\")\n",
    "        if open_csv.lower() == \"y\":\n",
    "            open_csv_in_excel(filename)\n",
    "    else:\n",
    "        print(\"Error writing report.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Dataframe Summaries"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Overall Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def overall_summary(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Get a comprehensive summary of the DataFrame including overall metrics.\n",
    "\n",
    "    Parameters:\n",
    "        df (pd.DataFrame): The DataFrame to summarize.\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: Summary including overall metrics about the DataFrame.\n",
    "    \"\"\"\n",
    "    if df is None:\n",
    "        raise ValueError(\"DataFrame is not set.\")\n",
    "\n",
    "    # Compute overall metrics\n",
    "    total_rows = df.shape[0]\n",
    "    total_columns = df.shape[1]\n",
    "    total_memory_kb = (df.memory_usage(deep=True).sum() / 1024).round(1)\n",
    "    total_missing_values = df.isna().sum().sum()\n",
    "    total_duplicates = df.duplicated().sum()\n",
    "    total_unique_values = df.nunique().sum()\n",
    "    total_non_null_values = df.notna().sum().sum()\n",
    "    data_types_count = df.dtypes.value_counts()\n",
    "\n",
    "    # Format the metrics for readability\n",
    "    formatted_total_rows = f\"{total_rows:,}\"\n",
    "    formatted_total_columns = f\"{total_columns:,}\"\n",
    "    formatted_total_memory_kb = f\"{total_memory_kb:,.1f}\"\n",
    "    formatted_total_missing_values = f\"{total_missing_values:,}\"\n",
    "    formatted_total_duplicates = f\"{total_duplicates:,}\"\n",
    "    formatted_total_unique_values = f\"{total_unique_values:,}\"\n",
    "    formatted_total_non_null_values = f\"{total_non_null_values:,}\"\n",
    "\n",
    "    # Format the data types count for human readability\n",
    "    data_types_summary = \"\\n\".join([f\"{dtype}: {count:,}\" for dtype, count in data_types_count.items()])\n",
    "\n",
    "    # Create overall summary DataFrame\n",
    "    overall_summary_df = pd.DataFrame({\n",
    "        'Metric': ['Total Rows', 'Total Columns', 'Total Memory Usage (KB)', 'Total Missing Values', \n",
    "                   'Total Duplicates', 'Total Unique Values', 'Total Non-Null Values', 'Data Types Count'],\n",
    "        'Value': [formatted_total_rows, formatted_total_columns, formatted_total_memory_kb, formatted_total_missing_values, \n",
    "                  formatted_total_duplicates, formatted_total_unique_values, formatted_total_non_null_values, data_types_summary]\n",
    "    })\n",
    "\n",
    "    return overall_summary_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Column Summaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_types_summary(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Get the data types of each column along with additional metrics and return them as a DataFrame.\n",
    "\n",
    "    Args:\n",
    "        df (pd.DataFrame): The DataFrame for which to generate the data types summary.\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: Data types and various metrics for each column.\n",
    "    \"\"\"\n",
    "    if df is None:\n",
    "        raise ValueError(\"DataFrame is not set.\")\n",
    "\n",
    "    # Initialize the summary DataFrame\n",
    "    summary_dict = {\n",
    "        'Column Name': df.columns,\n",
    "        'Data Type': df.dtypes,\n",
    "        'Non-Null Count': df.notna().sum(),\n",
    "        'Null Count': df.isna().sum(),\n",
    "        'Total Count': [df.shape[0]] * len(df.columns),  # Ensure same length\n",
    "        'Percentage Non-Null': (df.notna().sum() / df.shape[0] * 100).round(2),\n",
    "        'Distinct Count': df.nunique(),\n",
    "        'Memory Usage (KB)': (df.memory_usage(deep=True, index=False) / 1024).round(1)  # Round to 1 decimal place\n",
    "    }\n",
    "\n",
    "    # Add Most Frequent Value (Mode)\n",
    "    def get_most_frequent(column: pd.Series):\n",
    "        try:\n",
    "            mode = column.mode()\n",
    "            return mode.iloc[0] if not mode.empty else None\n",
    "        except:\n",
    "            return None\n",
    "\n",
    "    summary_dict['Most Frequent Value'] = df.apply(get_most_frequent)\n",
    "\n",
    "    # Add Unique Count (number of values that appear only once)\n",
    "    def unique_count(column: pd.Series) -> int:\n",
    "        return (column.value_counts() == 1).sum()\n",
    "    \n",
    "    summary_dict['Unique Count'] = df.apply(unique_count)\n",
    "\n",
    "    # Add Empty String Count for string columns\n",
    "    def empty_string_count(column: pd.Series) -> int:\n",
    "        if pd.api.types.is_string_dtype(column):\n",
    "            return column.str.strip().eq('').sum()\n",
    "        return 0\n",
    "\n",
    "    summary_dict['Empty String Count'] = df.apply(empty_string_count)\n",
    "\n",
    "    # Create DataFrame from dictionary\n",
    "    summary = pd.DataFrame(summary_dict)\n",
    "\n",
    "    # Ensure the DataFrame's index length matches the number of columns\n",
    "    assert len(summary) == len(df.columns), \"Mismatch in DataFrame column lengths.\"\n",
    "\n",
    "    return summary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Column Details"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def numeric_metrics(series: pd.Series) -> pd.Series:\n",
    "    \"\"\"\n",
    "    Calculate various metrics for a numeric series.\n",
    "\n",
    "    Args:\n",
    "        series (pd.Series): The numeric series to analyze.\n",
    "\n",
    "    Returns:\n",
    "        pd.Series: Various calculated metrics for the series.\n",
    "    \"\"\"\n",
    "    # Calculate basic statistics\n",
    "    mean = series.mean()\n",
    "    median = series.median()\n",
    "    min_val = series.min()\n",
    "    max_val = series.max()\n",
    "    range_val = max_val - min_val\n",
    "    total_sum = series.sum()\n",
    "    \n",
    "    # Calculate dispersion metrics\n",
    "    std_dev = series.std()\n",
    "    variance = series.var()\n",
    "    abs_mean_dev = (series - mean).abs().mean()\n",
    "    cv = std_dev / mean if mean != 0 else float('inf')\n",
    "    \n",
    "    # Calculate quartiles and IQR\n",
    "    q1 = series.quantile(0.25)\n",
    "    q3 = series.quantile(0.75)\n",
    "    iqr = q3 - q1\n",
    "    \n",
    "    # Calculate outlier bounds\n",
    "    lower_bound = q1 - 1.5 * iqr\n",
    "    upper_bound = q3 + 1.5 * iqr\n",
    "    \n",
    "    # Calculate mode(s)\n",
    "    mode_values = series.mode()\n",
    "    mode_value = mode_values[0] if not mode_values.empty else np.nan\n",
    "    \n",
    "    # Calculate additional metrics\n",
    "    skewness = series.skew()\n",
    "    kurtosis = series.kurtosis()\n",
    "    num_outliers = ((series < lower_bound) | (series > upper_bound)).sum()\n",
    "    missing_values = series.isna().sum()\n",
    "    unique_values = series.nunique()\n",
    "\n",
    "    # Format the metrics with thousands separators and up to two decimal places\n",
    "    def format_number(value):\n",
    "        if pd.isna(value):\n",
    "            return 'NaN'\n",
    "        return f\"{value:,.2f}\"\n",
    "    \n",
    "    # Compile all metrics into a pandas Series with logical grouping\n",
    "    return pd.Series({\n",
    "        # Basic Statistics\n",
    "        'Mean': format_number(mean),\n",
    "        'Median': format_number(median),\n",
    "        'Min': format_number(min_val),\n",
    "        'Max': format_number(max_val),\n",
    "        'Range': format_number(range_val),\n",
    "        'Sum': format_number(total_sum),\n",
    "\n",
    "        # Dispersion\n",
    "        'Std Dev': format_number(std_dev),\n",
    "        'Variance': format_number(variance),\n",
    "        'Absolute Mean Deviation': format_number(abs_mean_dev),\n",
    "        'Coefficient of Variation (CV)': format_number(cv),\n",
    "\n",
    "        # Distribution\n",
    "        'Skewness': format_number(skewness),\n",
    "        'Kurtosis': format_number(kurtosis),\n",
    "\n",
    "        # Quartiles and IQR\n",
    "        '1st Quartile (Q1)': format_number(q1),\n",
    "        '3rd Quartile (Q3)': format_number(q3),\n",
    "        'Interquartile Range (IQR)': format_number(iqr),\n",
    "\n",
    "        # Outliers\n",
    "        'Lower Bound for Outliers': format_number(lower_bound),\n",
    "        'Upper Bound for Outliers': format_number(upper_bound),\n",
    "        'Number of Outliers': num_outliers,\n",
    "\n",
    "        # Other\n",
    "        'Mode': mode_value,\n",
    "        'Missing Values': missing_values,\n",
    "        'Unique Values': unique_values\n",
    "    })\n",
    "\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "def datetime_metrics(series: pd.Series) -> pd.Series:\n",
    "    \"\"\"\n",
    "    Calculate various metrics for a datetime series.\n",
    "\n",
    "    Args:\n",
    "        series (pd.Series): The datetime series to analyze.\n",
    "\n",
    "    Returns:\n",
    "        pd.Series: Various calculated metrics for the series.\n",
    "    \"\"\"\n",
    "    # Convert series to datetime format, if not already\n",
    "    series = pd.to_datetime(series, errors='coerce')\n",
    "    \n",
    "    # Calculate basic statistics\n",
    "    min_date = series.min()\n",
    "    max_date = series.max()\n",
    "    mean_date = series.mean()\n",
    "    median_date = series.median()\n",
    "\n",
    "    # Convert datetime to string format for reporting\n",
    "    min_date_str = pd.Timestamp(min_date).strftime('%Y-%m-%d') if pd.notna(min_date) else None\n",
    "    max_date_str = pd.Timestamp(max_date).strftime('%Y-%m-%d') if pd.notna(max_date) else None\n",
    "    mean_date_str = pd.Timestamp(mean_date).strftime('%Y-%m-%d') if pd.notna(mean_date) else None\n",
    "    median_date_str = pd.Timestamp(median_date).strftime('%Y-%m-%d') if pd.notna(median_date) else None\n",
    "    \n",
    "    date_range = (max_date - min_date).days if pd.notna(max_date) and pd.notna(min_date) else None\n",
    "    total_days = date_range  # Same as date_range in this context\n",
    "    missing_values = series.isna().sum()\n",
    "    unique_values = series.nunique()\n",
    "    most_common_date = series.mode().values[0] if not series.mode().empty else None\n",
    "    \n",
    "    # Convert most_common_date to string format\n",
    "    most_common_date_str = pd.Timestamp(most_common_date).strftime('%Y-%m-%d') if pd.notna(most_common_date) else None\n",
    "\n",
    "    # Return all metrics as a pandas Series\n",
    "    return pd.Series({\n",
    "        'Min Date': min_date_str,\n",
    "        'Max Date': max_date_str,\n",
    "        'Mean Date': mean_date_str,\n",
    "        'Median Date': median_date_str,\n",
    "        'Date Range (Days)': date_range,\n",
    "        'Total Days': total_days,\n",
    "        'Missing Values': missing_values,\n",
    "        'Unique Values': unique_values,\n",
    "        'Most Common Date': most_common_date_str\n",
    "    })\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "def string_metrics(series: pd.Series) -> pd.Series:\n",
    "    \"\"\"\n",
    "    Calculate various metrics for a string series.\n",
    "\n",
    "    Args:\n",
    "        series (pd.Series): The string series to analyze.\n",
    "\n",
    "    Returns:\n",
    "        pd.Series: Various calculated metrics for the series.\n",
    "    \"\"\"\n",
    "    # Clean the series\n",
    "    series = series.fillna('')\n",
    "    \n",
    "    # Calculate basic string statistics\n",
    "    most_frequent = series.mode()[0] if not series.mode().empty else np.nan\n",
    "    unique_values = series.nunique()\n",
    "    missing_values = series.isna().sum()\n",
    "    empty_strings = (series == '').sum()\n",
    "    \n",
    "    # String length statistics\n",
    "    lengths = series.apply(len)\n",
    "    longest_string = lengths.max()\n",
    "    shortest_string = lengths.min()\n",
    "    average_length = lengths.mean()\n",
    "    \n",
    "    # Character frequency (top 10 characters)\n",
    "    char_freq = pd.Series(''.join(series).replace(' ', '')).value_counts().head(10)\n",
    "    \n",
    "    # Word frequency (top 10 words, if applicable)\n",
    "    words = series.str.split(expand=True).stack()\n",
    "    word_freq = words.value_counts().head(10)\n",
    "    \n",
    "    # Return all metrics as a pandas Series\n",
    "    return pd.Series({\n",
    "        'Most Frequent': most_frequent,\n",
    "        'Unique Values': unique_values,\n",
    "        'Missing Values': missing_values,\n",
    "        'Empty Strings': empty_strings,\n",
    "        'Longest String Length': longest_string,\n",
    "        'Shortest String Length': shortest_string,\n",
    "        'Average String Length': average_length,\n",
    "        # 'Top 10 Characters': char_freq.to_dict(),\n",
    "        'Top 10 Words': word_freq.to_dict()\n",
    "    })\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "def boolean_metrics(series: pd.Series) -> pd.Series:\n",
    "    \"\"\"\n",
    "    Calculate metrics for a boolean series.\n",
    "\n",
    "    Args:\n",
    "        series (pd.Series): The boolean series to analyze.\n",
    "\n",
    "    Returns:\n",
    "        pd.Series: Various calculated metrics for the series.\n",
    "    \"\"\"\n",
    "    return pd.Series({\n",
    "        'True Count': (series == True).sum(),\n",
    "        'False Count': (series == False).sum(),\n",
    "        'Missing Values': series.isna().sum()\n",
    "    })\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "def data_type_summaries(df: pd.DataFrame) -> dict:\n",
    "    \"\"\"\n",
    "    Get comprehensive metrics for each data type in the DataFrame and return them as a dictionary of DataFrames.\n",
    "\n",
    "    Args:\n",
    "        df (pd.DataFrame): The DataFrame for which to generate the summaries.\n",
    "\n",
    "    Returns:\n",
    "        dict: A dictionary with data type as the key and DataFrame of metrics as the value.\n",
    "    \"\"\"\n",
    "    if df is None:\n",
    "        raise ValueError(\"DataFrame is not set.\")\n",
    "\n",
    "    # Dictionary to hold DataFrames for each data type\n",
    "    summaries = {}\n",
    "\n",
    "    # Metrics calculation functions for each data type\n",
    "    metrics_funcs = {\n",
    "        np.number: numeric_metrics,\n",
    "        'datetime64[ns]': datetime_metrics,\n",
    "        'object': string_metrics,\n",
    "        'bool': boolean_metrics\n",
    "    }\n",
    "\n",
    "    # Calculate and store metrics for each data type\n",
    "    for dtype, metrics_func in metrics_funcs.items():\n",
    "        type_columns = df.select_dtypes(include=[dtype])\n",
    "        summary_list = []\n",
    "\n",
    "        for column in type_columns:\n",
    "            metrics = metrics_func(df[column])\n",
    "            metrics.name = column\n",
    "            summary_list.append(pd.DataFrame(metrics).T)\n",
    "\n",
    "        if summary_list:\n",
    "            summary_df = pd.concat(summary_list)\n",
    "            summary_df.reset_index(inplace=True)\n",
    "            summary_df.rename(columns={'index': 'Column Name'}, inplace=True)\n",
    "            summary_df = summary_df.sort_values(by='Column Name').reset_index(drop=True)\n",
    "            summaries[str(dtype)] = summary_df\n",
    "\n",
    "    # Display DataFrames in a Jupyter Notebook\n",
    "    for dtype, summary_df in summaries.items():\n",
    "        print(f\"\\nData Type: {dtype}\")\n",
    "        if get_ipython() is not None:  # Check if running in Jupyter Notebook\n",
    "            display(summary_df)\n",
    "        else:\n",
    "            print(summary_df.to_string(index=False))  # Print DataFrame as string\n",
    "\n",
    "    return summaries\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Dataframe Correlation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def correlation_function(df: pd.DataFrame, dummy_variables: bool = False, dummy_threshold: int = 10, report_threshold: float = 0.3) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Calculate the correlation between columns in a DataFrame, optionally creating dummy variables for categorical columns.\n",
    "    \n",
    "    Parameters:\n",
    "    - df (pd.DataFrame): The input DataFrame.\n",
    "    - dummy_variables (bool): If True, create dummy variables for string columns with unique values <= dummy_threshold.\n",
    "    - dummy_threshold (int): Maximum number of unique values in a column to create dummy variables.\n",
    "    - report_threshold (float): Threshold for reporting correlated pairs (both positive and negative).\n",
    "    \n",
    "    Returns:\n",
    "    - pd.DataFrame: DataFrame with columns of the correlated pairs, their correlation values.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Step 1: Convert string/categorical columns to dummy variables if needed\n",
    "    df_copy = df.copy()\n",
    "    \n",
    "    if dummy_variables:\n",
    "        # Identify string/categorical columns\n",
    "        non_numeric_cols = df_copy.select_dtypes(include=['object', 'category']).columns\n",
    "        \n",
    "        for col in non_numeric_cols:\n",
    "            unique_vals = df_copy[col].nunique()\n",
    "            if unique_vals <= dummy_threshold:\n",
    "                # Convert to dummy variables\n",
    "                df_copy = pd.get_dummies(df_copy, columns=[col], drop_first=True)\n",
    "    \n",
    "    # Step 2: Identify numeric columns (ignoring non-numeric columns)\n",
    "    numeric_df = df_copy.select_dtypes(include=[np.number])\n",
    "    \n",
    "    if numeric_df.empty:\n",
    "        raise ValueError(\"No numeric columns found in the DataFrame for correlation analysis.\")\n",
    "    \n",
    "    # Step 3: Calculate correlation matrix\n",
    "    corr_matrix = numeric_df.corr()\n",
    "\n",
    "    # Step 4: Extract correlation pairs greater than the report_threshold\n",
    "    corr_pairs = corr_matrix.unstack().reset_index()\n",
    "    corr_pairs.columns = ['Variable1', 'Variable2', 'Correlation']\n",
    "    \n",
    "    # Remove self-correlations\n",
    "    corr_pairs = corr_pairs[corr_pairs['Variable1'] != corr_pairs['Variable2']]\n",
    "    \n",
    "    # Ensure we only report each pair once (e.g., always sort Variable1 and Variable2 alphabetically)\n",
    "    corr_pairs['sorted_pair'] = corr_pairs.apply(lambda x: tuple(sorted([x['Variable1'], x['Variable2']])), axis=1)\n",
    "    corr_pairs = corr_pairs.drop_duplicates(subset='sorted_pair').drop(columns='sorted_pair')\n",
    "\n",
    "    # Step 5: Filter pairs based on the report_threshold\n",
    "    corr_pairs['Abs_Correlation'] = corr_pairs['Correlation'].abs()\n",
    "    filtered_pairs = corr_pairs[corr_pairs['Abs_Correlation'] >= report_threshold]\n",
    "    \n",
    "    # Sort by absolute correlation in descending order\n",
    "    filtered_pairs = filtered_pairs.sort_values(by='Abs_Correlation', ascending=False).drop(columns='Abs_Correlation')\n",
    "\n",
    "    return filtered_pairs.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Misc Support Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load a 2-column csv file as a pandas dataframe and return a dictionary \n",
    "\n",
    "def load_csv_to_dict(file_path, key_column, value_column):\n",
    "    \"\"\"\n",
    "    Load a CSV file and create a dictionary from specified key and value columns.\n",
    "\n",
    "    Parameters:\n",
    "    file_path (str): The path to the CSV file.\n",
    "    key_column (str): The name of the column to use as keys in the dictionary.\n",
    "    value_column (str): The name of the column to use as values in the dictionary.\n",
    "\n",
    "    Returns:\n",
    "    dict: A dictionary with keys from the key_column and values from the value_column.\n",
    "    \"\"\"\n",
    "    df = pd.read_csv(file_path)\n",
    "    \n",
    "    # Convert NaN values to empty strings\n",
    "    df[value_column] = df[value_column].apply(lambda x: '' if pd.isna(x) else str(x))\n",
    "    \n",
    "    return df.set_index(key_column)[value_column].to_dict()\n",
    "\n",
    "def ensure_string(text, string_length_threshold):\n",
    "    \"\"\"\n",
    "    Ensure the input is converted to a string representation.\n",
    "\n",
    "    Parameters:\n",
    "    text: The input to be checked and converted to a string if necessary.\n",
    "    string_length_threshold: The minimum length of the string. If the length of the string is less than this threshold, return an empty string.\n",
    "\n",
    "    Returns:\n",
    "    str: The input converted to a string or an empty string if the length is less than the threshold or conversion fails.\n",
    "    \"\"\"\n",
    "    if pd.isnull(text):\n",
    "        return \"\"\n",
    "    elif isinstance(text, str):\n",
    "        text = text.strip()  # Strip whitespace from both ends\n",
    "        if len(text) < string_length_threshold:\n",
    "            return \"\"\n",
    "        return text\n",
    "    \n",
    "    try:\n",
    "        # Attempt to convert non-string types to string\n",
    "        text = str(text).strip()\n",
    "        if len(text) < string_length_threshold:\n",
    "            return \"\"\n",
    "        return text\n",
    "    except (ValueError, TypeError):\n",
    "        return \"\"\n",
    "    \n",
    "def clean_whitespace(text):\n",
    "    # Remove leading and trailing whitespace\n",
    "    cleaned_text = text.strip()\n",
    "    \n",
    "    # Replace multiple spaces with a single space\n",
    "    cleaned_text = ' '.join(cleaned_text.split())\n",
    "    \n",
    "    return cleaned_text\n",
    "\n",
    "\n",
    "def save_dataframe_with_incremented_filename(file_path):\n",
    "    \"\"\"\n",
    "    Check if a filename already exists. If it does, it returns an incremented filename.\n",
    "\n",
    "    Parameters:\n",
    "    df (pd.DataFrame): The DataFrame to be saved.\n",
    "    file_path (str): The initial file path for the CSV file.\n",
    "    \"\"\"\n",
    "    base, extension = os.path.splitext(file_path)\n",
    "    counter = 1\n",
    "\n",
    "    # Check if the file already exists\n",
    "    while os.path.exists(file_path):\n",
    "        # Increment the file name\n",
    "        file_path = f\"{base}_{counter}{extension}\"\n",
    "        counter += 1\n",
    "\n",
    "    return file_path"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Enter the file (or folder) with the data to load into a dataframe\n",
    "# read_file_path = r'C:\\Users\\Windows\\Downloads\\archive (7)\\netflix1.csv'\n",
    "read_file_path = r'C:\\Users\\Windows\\Downloads\\happyscore_income.csv'\n",
    "df = load_to_dataframe(read_file_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read a Single File\n",
    "\n",
    "Force a file to load as text with force_plain_text=True </br>\n",
    "When loading an Excel file, use sheet_name=\"Sheet1\" to specify the sheet name to load </br>\n",
    "\n",
    " - df = load_to_dataframe(\"path/to/file.csv\", file_type=\"csv\")\n",
    " - df = load_to_dataframe(\"path/to/file.txt\", file_type=\"txt\", force_plain_text=True)\n",
    " - df = load_to_dataframe(\"path/to/file.xlsx\", file_type=\"xlsx\", sheet_name=\"Sheet1\")\n",
    " - df = load_to_dataframe(\"path/to/file.json\", file_type=\"json\")\n",
    " - df = load_to_dataframe(\"path/to/file.parquet\", file_type=\"parquet\")\n",
    " - df = load_to_dataframe(\"path/to/file.hdf\", file_type=\"hdf\")\n",
    " - df = load_to_dataframe(\"path/to/file.feather\", file_type=\"feather\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read Multiple Files\n",
    " - df = load_to_dataframe(\"path/to/folder\", file_type=\"csv\")\n",
    " - df = load_to_dataframe(\"path/to/folder\", file_type=\"log\", force_plain_text=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summarize Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataframe Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "summary_df = overall_summary(df)\n",
    "print(summary_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Column Summaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the summary\n",
    "summary_df = data_types_summary(df)\n",
    "print(summary_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Column Details"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run the function\n",
    "summaries = data_type_summaries(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Correlation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = correlation_function(df, dummy_variables=True, dummy_threshold=100, report_threshold=0)\n",
    "print(result)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
