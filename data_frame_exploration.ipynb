{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Notes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "files or dataframes can be opened with this function:</br>\n",
    "open_in_excel(df, output_csv_path=r'd:\\output.csv', excel_path=None, encoding='utf-8', verbose=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load the Code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Packages, Settings, and External Links"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports and Paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import chardet\n",
    "import string\n",
    "import platform\n",
    "import subprocess\n",
    "import numpy as np\n",
    "\n",
    "import pandas as pd\n",
    "from IPython.display import display\n",
    "from dataframe_handler import DataFrameHandler\n",
    "\n",
    "import nltk\n",
    "from nltk import ngrams, word_tokenize\n",
    "ps = nltk.PorterStemmer()\n",
    "wn = nltk.WordNetLemmatizer()\n",
    "from autocorrect import Speller\n",
    "from collections import Counter\n",
    "import openpyxl #Used by Pandas to open Excel files\n",
    "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "\n",
    "read_file = r'C:\\Users\\Windows\\Downloads\\archive (7)\\netflix1.csv'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Qualitative Analysis Lists and Settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stopword lisit\n",
    "external_stopword_list = r\"D:\\Data Analysis\\Qualitative Analysis\\Cleaning Text\\lists and dictionaries\\stopwords_seo.csv\"\n",
    "\n",
    "# Contractions dictionary\n",
    "external_contractions_dictionary = r\"D:\\Data Analysis\\Qualitative Analysis\\Cleaning Text\\lists and dictionaries\\contractions_list.csv\"\n",
    "\n",
    "# External search and replace file\n",
    "custom_search_and_replace_dictionary = r\"D:\\Data Analysis\\Qualitative Analysis\\Cleaning Text\\lists and dictionaries\\custom search-and-replace dictionary.csv\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set Pandas Display Options"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set options to display more columns and rows\n",
    "pd.set_option('display.max_columns', None)  # Show all columns\n",
    "pd.set_option('display.max_rows', None)     # Show all rows (if applicable)\n",
    "pd.set_option('display.max_colwidth', None) # Show full column width\n",
    "pd.set_option('display.width', 1000)        # Set the width of the display"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Misc Support Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load a 2-column csv file as a pandas dataframe and return a dictionary \n",
    "\n",
    "def load_csv_to_dict(file_path, key_column, value_column):\n",
    "    \"\"\"\n",
    "    Load a CSV file and create a dictionary from specified key and value columns.\n",
    "\n",
    "    Parameters:\n",
    "    file_path (str): The path to the CSV file.\n",
    "    key_column (str): The name of the column to use as keys in the dictionary.\n",
    "    value_column (str): The name of the column to use as values in the dictionary.\n",
    "\n",
    "    Returns:\n",
    "    dict: A dictionary with keys from the key_column and values from the value_column.\n",
    "    \"\"\"\n",
    "    df = pd.read_csv(file_path)\n",
    "    \n",
    "    # Convert NaN values to empty strings\n",
    "    df[value_column] = df[value_column].apply(lambda x: '' if pd.isna(x) else str(x))\n",
    "    \n",
    "    return df.set_index(key_column)[value_column].to_dict()\n",
    "\n",
    "def ensure_string(text, string_length_threshold):\n",
    "    \"\"\"\n",
    "    Ensure the input is converted to a string representation.\n",
    "\n",
    "    Parameters:\n",
    "    text: The input to be checked and converted to a string if necessary.\n",
    "    string_length_threshold: The minimum length of the string. If the length of the string is less than this threshold, return an empty string.\n",
    "\n",
    "    Returns:\n",
    "    str: The input converted to a string or an empty string if the length is less than the threshold or conversion fails.\n",
    "    \"\"\"\n",
    "    if pd.isnull(text):\n",
    "        return \"\"\n",
    "    elif isinstance(text, str):\n",
    "        text = text.strip()  # Strip whitespace from both ends\n",
    "        if len(text) < string_length_threshold:\n",
    "            return \"\"\n",
    "        return text\n",
    "    \n",
    "    try:\n",
    "        # Attempt to convert non-string types to string\n",
    "        text = str(text).strip()\n",
    "        if len(text) < string_length_threshold:\n",
    "            return \"\"\n",
    "        return text\n",
    "    except (ValueError, TypeError):\n",
    "        return \"\"\n",
    "    \n",
    "def clean_whitespace(text):\n",
    "    # Remove leading and trailing whitespace\n",
    "    cleaned_text = text.strip()\n",
    "    \n",
    "    # Replace multiple spaces with a single space\n",
    "    cleaned_text = ' '.join(cleaned_text.split())\n",
    "    \n",
    "    return cleaned_text\n",
    "\n",
    "\n",
    "def save_dataframe_with_incremented_filename(file_path):\n",
    "    \"\"\"\n",
    "    Check if a filename already exists. If it does, it returns an incremented filename.\n",
    "\n",
    "    Parameters:\n",
    "    df (pd.DataFrame): The DataFrame to be saved.\n",
    "    file_path (str): The initial file path for the CSV file.\n",
    "    \"\"\"\n",
    "    base, extension = os.path.splitext(file_path)\n",
    "    counter = 1\n",
    "\n",
    "    # Check if the file already exists\n",
    "    while os.path.exists(file_path):\n",
    "        # Increment the file name\n",
    "        file_path = f\"{base}_{counter}{extension}\"\n",
    "        counter += 1\n",
    "\n",
    "    return file_path"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load to dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import chardet\n",
    "\n",
    "def detect_encoding(file_path):\n",
    "    if not os.path.isfile(file_path):\n",
    "        raise FileNotFoundError(f\"The file at {file_path} does not exist.\")\n",
    "    \n",
    "    try:\n",
    "        with open(file_path, 'rb') as file:\n",
    "            result = chardet.detect(file.read(10000))\n",
    "        return result['encoding']\n",
    "    except IOError as e:\n",
    "        raise IOError(f\"An error occurred while reading the file: {e}\")\n",
    "\n",
    "def detect_delimiter(file_path, sample_size=1000):\n",
    "    delimiters = [',', '\\t', ';', '|', ' ']\n",
    "    delimiter_counts = {delim: 0 for delim in delimiters}\n",
    "    \n",
    "    with open(file_path, 'r', encoding='utf-8', errors='ignore') as file:\n",
    "        sample = file.read(sample_size)\n",
    "    \n",
    "    for delim in delimiters:\n",
    "        delimiter_counts[delim] = sample.count(delim)\n",
    "    \n",
    "    # Handle the case where multiple delimiters have similar counts\n",
    "    most_common_delim = max(delimiter_counts, key=delimiter_counts.get)\n",
    "    \n",
    "    return most_common_delim\n",
    "\n",
    "def read_file(file_path, encoding='utf-8', sheet_name=None, force_plain_text=False):\n",
    "    _, file_extension = os.path.splitext(file_path)\n",
    "    file_extension = file_extension.lower()[1:]\n",
    "    \n",
    "    try:\n",
    "        if file_extension in ['csv']:\n",
    "            print(f\"Loading {file_path} as CSV with default comma delimiter\")\n",
    "            return pd.read_csv(file_path, encoding=encoding, on_bad_lines='skip')\n",
    "        elif file_extension in ['tsv']:\n",
    "            print(f\"Loading {file_path} as TSV with tab delimiter\")\n",
    "            return pd.read_csv(file_path, delimiter='\\t', encoding=encoding, on_bad_lines='skip')\n",
    "        elif file_extension in ['txt', 'log']:\n",
    "            if force_plain_text:\n",
    "                print(f\"Loading {file_path} as plain text\")\n",
    "                with open(file_path, 'r', encoding=encoding, errors='ignore') as file:\n",
    "                    content = file.read()\n",
    "                return pd.DataFrame({'content': [content]})\n",
    "            else:\n",
    "                delimiter = detect_delimiter(file_path)\n",
    "                print(f\"Detected delimiter for {file_path}: '{delimiter}'\")\n",
    "                return pd.read_csv(file_path, delimiter=delimiter, encoding=encoding, on_bad_lines='skip')\n",
    "        elif file_extension in ['xls', 'xlsx']:\n",
    "            return pd.read_excel(file_path, sheet_name=sheet_name, engine='openpyxl')\n",
    "        elif file_extension == 'json':\n",
    "            return pd.read_json(file_path, encoding=encoding)\n",
    "        elif file_extension == 'parquet':\n",
    "            return pd.read_parquet(file_path)\n",
    "        elif file_extension == 'hdf':\n",
    "            return pd.read_hdf(file_path)\n",
    "        elif file_extension == 'feather':\n",
    "            return pd.read_feather(file_path)\n",
    "        else:\n",
    "            raise ValueError(\"Unsupported file type.\")\n",
    "    except (UnicodeDecodeError, pd.errors.ParserError) as e:\n",
    "        detected_encoding = detect_encoding(file_path)\n",
    "        try:\n",
    "            if file_extension in ['csv', 'json', 'txt', 'log']:\n",
    "                if force_plain_text:\n",
    "                    print(f\"Reattempting with detected encoding '{detected_encoding}' as plain text\")\n",
    "                    with open(file_path, 'r', encoding=detected_encoding, errors='ignore') as file:\n",
    "                        content = file.read()\n",
    "                    return pd.DataFrame({'content': [content]})\n",
    "                else:\n",
    "                    delimiter = detect_delimiter(file_path)\n",
    "                    print(f\"Reattempting with detected encoding '{detected_encoding}' and delimiter '{delimiter}'\")\n",
    "                    return pd.read_csv(file_path, delimiter=delimiter, encoding=detected_encoding, on_bad_lines='skip')\n",
    "            else:\n",
    "                raise RuntimeError(\"Unsupported file type for encoding detection.\")\n",
    "        except Exception as e:\n",
    "            raise RuntimeError(f\"An error occurred while loading the file: {e}\")\n",
    "\n",
    "def load_to_dataframe(file_or_folder_path, file_type=None, sheet_name=None, force_plain_text=False):\n",
    "    if os.path.isdir(file_or_folder_path):\n",
    "        if file_type is None:\n",
    "            raise ValueError(\"File type must be specified when a folder is provided.\")\n",
    "        \n",
    "        dfs = []\n",
    "        columns_set = set()  # To keep track of all columns across files\n",
    "        \n",
    "        for root, _, files in os.walk(file_or_folder_path):\n",
    "            for file in files:\n",
    "                if file.lower().endswith(file_type.lower()):\n",
    "                    file_path = os.path.join(root, file)\n",
    "                    print(f\"Processing file: {file_path}\")\n",
    "                    try:\n",
    "                        df = read_file(file_path, sheet_name=sheet_name, force_plain_text=force_plain_text)\n",
    "                        columns_set.update(df.columns)\n",
    "                        dfs.append(df)\n",
    "                    except Exception as e:\n",
    "                        print(f\"Warning: Could not process file {file_path}: {e}\")\n",
    "                        try:\n",
    "                            with open(file_path, 'r', encoding='utf-8', errors='ignore') as file:\n",
    "                                content = file.read()\n",
    "                            df = pd.DataFrame({'content': [content]})\n",
    "                            dfs.append(df)\n",
    "                        except Exception as fallback_e:\n",
    "                            print(f\"Fallback failed for file {file_path}: {fallback_e}\")\n",
    "\n",
    "        if not dfs:\n",
    "            raise ValueError(\"No valid files found in the specified folder.\")\n",
    "        \n",
    "        # Standardize columns across all DataFrames\n",
    "        standardized_dfs = []\n",
    "        for df in dfs:\n",
    "            # Add missing columns\n",
    "            missing_cols = columns_set - set(df.columns)\n",
    "            for col in missing_cols:\n",
    "                df[col] = pd.NA\n",
    "            # Reorder columns\n",
    "            df = df[list(columns_set)]\n",
    "            standardized_dfs.append(df)\n",
    "\n",
    "        combined_df = pd.concat(standardized_dfs, ignore_index=True)\n",
    "        return combined_df\n",
    "\n",
    "    elif os.path.isfile(file_or_folder_path):\n",
    "        return read_file(file_or_folder_path, sheet_name=sheet_name, force_plain_text=force_plain_text)\n",
    "    \n",
    "    else:\n",
    "        raise ValueError(f\"The path {file_or_folder_path} is neither a file nor a folder.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataframe Diagnostics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dataframe_diagnostics(df: pd.DataFrame, encoding_check: bool = True) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Provides diagnostics for potential issues in the DataFrame including missing values, incorrect types, encoding issues, etc.\n",
    "\n",
    "    Parameters:\n",
    "    - df (pd.DataFrame): The input DataFrame to analyze.\n",
    "    - encoding_check (bool): If True, checks for columns with potential encoding issues.\n",
    "\n",
    "    Returns:\n",
    "    - pd.DataFrame: A DataFrame summarizing potential issues with the data.\n",
    "    \"\"\"\n",
    "    \n",
    "    issues = []\n",
    "\n",
    "    # Step 1: Check for missing values\n",
    "    missing_vals = df.isnull().sum()\n",
    "    missing_percent = (df.isnull().sum() / len(df)) * 100\n",
    "    for col, count, percent in zip(df.columns, missing_vals, missing_percent):\n",
    "        if count > 0:\n",
    "            issues.append({\n",
    "                'Column': col, \n",
    "                'Issue': f\"Missing values: {count} ({percent:.2f}%)\"\n",
    "            })\n",
    "\n",
    "    # Step 2: Check for potential data type issues\n",
    "    for col in df.columns:\n",
    "        if df[col].dtype == 'object':\n",
    "            # Check if object column contains numeric values\n",
    "            try:\n",
    "                pd.to_numeric(df[col])\n",
    "                issues.append({\n",
    "                    'Column': col,\n",
    "                    'Issue': 'Potential data type issue: Object column contains numeric values'\n",
    "                })\n",
    "            except:\n",
    "                pass\n",
    "\n",
    "    # Step 3: Check for constant columns (columns with a single unique value)\n",
    "    for col in df.columns:\n",
    "        if df[col].nunique() == 1:\n",
    "            issues.append({\n",
    "                'Column': col, \n",
    "                'Issue': 'Constant column: Only one unique value'\n",
    "            })\n",
    "\n",
    "    # Step 4: Check for columns with high cardinality (many unique values)\n",
    "    for col in df.columns:\n",
    "        if df[col].nunique() > 0.9 * len(df):\n",
    "            issues.append({\n",
    "                'Column': col, \n",
    "                'Issue': 'High cardinality: Large number of unique values'\n",
    "            })\n",
    "\n",
    "    # Step 5: Check for potential encoding issues (non-UTF8 characters or odd encodings)\n",
    "    if encoding_check:\n",
    "        for col in df.select_dtypes(include=['object']).columns:\n",
    "            try:\n",
    "                df[col].apply(lambda x: x.encode('utf-8').decode('utf-8') if isinstance(x, str) else x)\n",
    "            except UnicodeDecodeError:\n",
    "                issues.append({\n",
    "                    'Column': col,\n",
    "                    'Issue': 'Potential encoding issue: Non-UTF8 characters detected'\n",
    "                })\n",
    "\n",
    "    # Step 6: Check for duplicate columns\n",
    "    duplicate_columns = df.columns[df.T.duplicated()]\n",
    "    for col in duplicate_columns:\n",
    "        issues.append({\n",
    "            'Column': col,\n",
    "            'Issue': 'Duplicate column: Identical to another column'\n",
    "        })\n",
    "\n",
    "    # Step 7: Check for potential outliers (using basic IQR method)\n",
    "    for col in df.select_dtypes(include=[np.number]).columns:\n",
    "        q1 = df[col].quantile(0.25)\n",
    "        q3 = df[col].quantile(0.75)\n",
    "        iqr = q3 - q1\n",
    "        lower_bound = q1 - 1.5 * iqr\n",
    "        upper_bound = q3 + 1.5 * iqr\n",
    "        outliers = df[(df[col] < lower_bound) | (df[col] > upper_bound)].shape[0]\n",
    "        if outliers > 0:\n",
    "            issues.append({\n",
    "                'Column': col,\n",
    "                'Issue': f\"Potential outliers: {outliers} rows outside IQR\"\n",
    "            })\n",
    "\n",
    "    # Convert issues list to DataFrame for easier viewing\n",
    "    issues_df = pd.DataFrame(issues)\n",
    "\n",
    "    if issues_df.empty:\n",
    "        print(\"No significant issues detected in the DataFrame.\")\n",
    "    else:\n",
    "        display(issues_df)\n",
    "\n",
    "    return issues_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Open in Excel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "def open_csv_in_excel(file_path: str, excel_path=None):\n",
    "    \"\"\"\n",
    "    Opens a CSV file in Microsoft Excel or other default programs based on the platform.\n",
    "    \n",
    "    Args:\n",
    "        file_path (str): The path to the CSV file that needs to be opened.\n",
    "        excel_path (str): Optional manual path to the Excel executable.\n",
    "    \n",
    "    Returns:\n",
    "        None\n",
    "    \"\"\"\n",
    "    if platform.system() == 'Windows':\n",
    "        # Step 1: Check if Excel path is provided\n",
    "        if not excel_path:\n",
    "            excel_paths = [\n",
    "                r\"C:\\Program Files (x86)\\Microsoft Office\\root\\Office16\\EXCEL.EXE\",\n",
    "                r\"C:\\Program Files\\Microsoft Office\\root\\Office16\\EXCEL.EXE\"\n",
    "            ]\n",
    "            excel_program_path = None\n",
    "\n",
    "            # Step 2: Check common Excel paths\n",
    "            for path in excel_paths:\n",
    "                if os.path.exists(path):\n",
    "                    excel_program_path = path\n",
    "                    break\n",
    "\n",
    "            # Step 3: If Excel is not found in the predefined paths, search for it\n",
    "            if not excel_program_path:\n",
    "                directories_to_search = [\n",
    "                    r\"C:\\Program Files\",\n",
    "                    r\"C:\\Program Files (x86)\",\n",
    "                    r\"C:\\\\\"\n",
    "                ]\n",
    "                excel_program_path = find_excel_exe(directories_to_search)\n",
    "\n",
    "        else:\n",
    "            excel_program_path = excel_path\n",
    "\n",
    "        # Step 4: Open file with Excel if path is found\n",
    "        if excel_program_path:\n",
    "            subprocess.Popen([excel_program_path, file_path])\n",
    "        else:\n",
    "            print(\"Excel cannot be found on the system. Opening file with the default CSV viewer.\")\n",
    "            open_file_with_default_program(file_path)\n",
    "\n",
    "    elif platform.system() == 'Darwin':  # macOS\n",
    "        subprocess.call(['open', file_path])\n",
    "    elif platform.system() == 'Linux':  # Linux\n",
    "        subprocess.call(['xdg-open', file_path])\n",
    "    else:\n",
    "        print(\"Unsupported operating system. Unable to open the file.\")\n",
    "\n",
    "\n",
    "def find_excel_exe(directories: list) -> str:\n",
    "    \"\"\"\n",
    "    Searches for EXCEL.EXE in the given directories.\n",
    "    \n",
    "    Args:\n",
    "        directories (list): List of directories to search for the Excel executable.\n",
    "    \n",
    "    Returns:\n",
    "        str: The full path to the Excel executable, or None if not found.\n",
    "    \"\"\"\n",
    "    for directory in directories:\n",
    "        for root, dirs, files in os.walk(directory):\n",
    "            for file in files:\n",
    "                if file.lower() == 'excel.exe':\n",
    "                    return os.path.join(root, file)\n",
    "    return None\n",
    "\n",
    "\n",
    "def open_file_with_default_program(file_path):\n",
    "    \"\"\"\n",
    "    Opens a file using the default program associated with the file type.\n",
    "    \n",
    "    Args:\n",
    "        file_path (str): The path to the file.\n",
    "    \n",
    "    Returns:\n",
    "        None\n",
    "    \"\"\"\n",
    "    try:\n",
    "        if platform.system() == 'Windows':\n",
    "            os.startfile(file_path)\n",
    "        elif platform.system() == 'Darwin':  # macOS\n",
    "            subprocess.call(['open', file_path])\n",
    "        elif platform.system() == 'Linux':  # Linux\n",
    "            subprocess.call(['xdg-open', file_path])\n",
    "    except Exception as e:\n",
    "        print(f\"Error opening file: {e}\")\n",
    "\n",
    "\n",
    "def open_in_excel(input_data, output_csv_path='output.csv', excel_path=None, encoding='utf-8', verbose=True):\n",
    "    \"\"\"\n",
    "    Opens a CSV file in Excel or the default program. If a DataFrame is provided, it saves it as a CSV first.\n",
    "    \n",
    "    Args:\n",
    "        input_data (str or pd.DataFrame): Either a file path to a CSV or a Pandas DataFrame.\n",
    "        output_csv_path (str): The file path to save the DataFrame if one is provided.\n",
    "        excel_path (str): Optional path to Excel executable.\n",
    "        encoding (str): Encoding to use when saving the CSV file.\n",
    "        verbose (bool): If True, provides more detailed output on the process.\n",
    "    \n",
    "    Returns:\n",
    "        None\n",
    "    \"\"\"\n",
    "    file_to_open = None\n",
    "\n",
    "    # Check if input_data is a DataFrame\n",
    "    if isinstance(input_data, pd.DataFrame):\n",
    "        if verbose:\n",
    "            print(f\"Saving DataFrame to CSV: {output_csv_path}\")\n",
    "        try:\n",
    "            input_data.to_csv(output_csv_path, index=False, encoding=encoding)\n",
    "            file_to_open = output_csv_path\n",
    "        except Exception as e:\n",
    "            print(f\"Error saving DataFrame as CSV: {e}\")\n",
    "            return\n",
    "\n",
    "    elif isinstance(input_data, str):\n",
    "        # Assume it's a file path\n",
    "        if os.path.exists(input_data):\n",
    "            file_to_open = input_data\n",
    "        else:\n",
    "            print(f\"The provided file path does not exist: {input_data}\")\n",
    "            return\n",
    "    else:\n",
    "        print(\"Invalid input. Please provide a valid DataFrame or file path.\")\n",
    "        return\n",
    "\n",
    "    # Prompt the user if they want to open the file in Excel\n",
    "    if verbose:\n",
    "        print(f\"File written to: {file_to_open}\")\n",
    "    open_csv = input(\"Open file in Excel? (Y/N): \").strip().lower()\n",
    "    \n",
    "    if open_csv == 'y':\n",
    "        open_csv_in_excel(file_to_open, excel_path=excel_path)\n",
    "    else:\n",
    "        print(\"File not opened in Excel.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### General Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Overall Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "def overall_summary(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Get a comprehensive summary of the DataFrame including overall metrics.\n",
    "\n",
    "    Parameters:\n",
    "        df (pd.DataFrame): The DataFrame to summarize.\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: Summary including overall metrics about the DataFrame.\n",
    "    \"\"\"\n",
    "    if df is None:\n",
    "        raise ValueError(\"DataFrame is not set.\")\n",
    "\n",
    "    # Compute overall metrics\n",
    "    total_rows = df.shape[0]\n",
    "    total_columns = df.shape[1]\n",
    "    total_memory_kb = (df.memory_usage(deep=True).sum() / 1024).round(1)\n",
    "    total_missing_values = df.isna().sum().sum()\n",
    "    total_duplicates = df.duplicated().sum()\n",
    "    total_unique_values = df.nunique().sum()\n",
    "    total_non_null_values = df.notna().sum().sum()\n",
    "    data_types_count = df.dtypes.value_counts()\n",
    "\n",
    "    # Format the metrics for readability\n",
    "    formatted_total_rows = f\"{total_rows:,}\"\n",
    "    formatted_total_columns = f\"{total_columns:,}\"\n",
    "    formatted_total_memory_kb = f\"{total_memory_kb:,.1f}\"\n",
    "    formatted_total_missing_values = f\"{total_missing_values:,}\"\n",
    "    formatted_total_duplicates = f\"{total_duplicates:,}\"\n",
    "    formatted_total_unique_values = f\"{total_unique_values:,}\"\n",
    "    formatted_total_non_null_values = f\"{total_non_null_values:,}\"\n",
    "\n",
    "    # Format the data types count for human readability\n",
    "    data_types_summary = \"\\n\".join([f\"{dtype}: {count:,}\" for dtype, count in data_types_count.items()])\n",
    "\n",
    "    # Create overall summary DataFrame\n",
    "    overall_summary_df = pd.DataFrame({\n",
    "        'Metric': ['Total Rows', 'Total Columns', 'Total Memory Usage (KB)', 'Total Missing Values', \n",
    "                   'Total Duplicates', 'Total Unique Values', 'Total Non-Null Values', 'Data Types Count'],\n",
    "        'Value': [formatted_total_rows, formatted_total_columns, formatted_total_memory_kb, formatted_total_missing_values, \n",
    "                  formatted_total_duplicates, formatted_total_unique_values, formatted_total_non_null_values, data_types_summary]\n",
    "    })\n",
    "\n",
    "    return overall_summary_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Column Summaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_types_summary(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Get the data types of each column along with additional metrics and return them as a DataFrame.\n",
    "\n",
    "    Args:\n",
    "        df (pd.DataFrame): The DataFrame for which to generate the data types summary.\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: Data types and various metrics for each column.\n",
    "    \"\"\"\n",
    "    if df is None:\n",
    "        raise ValueError(\"DataFrame is not set.\")\n",
    "\n",
    "    # Initialize the summary DataFrame\n",
    "    summary_dict = {\n",
    "        'Column Name': df.columns,\n",
    "        'Data Type': df.dtypes,\n",
    "        'Non-Null Count': df.notna().sum(),\n",
    "        'Null Count': df.isna().sum(),\n",
    "        'Total Count': [df.shape[0]] * len(df.columns),  # Ensure same length\n",
    "        'Percentage Non-Null': (df.notna().sum() / df.shape[0] * 100).round(2),\n",
    "        'Distinct Count': df.nunique(),\n",
    "        'Memory Usage (KB)': (df.memory_usage(deep=True, index=False) / 1024).round(1)  # Round to 1 decimal place\n",
    "    }\n",
    "\n",
    "    # Add Most Frequent Value (Mode)\n",
    "    def get_most_frequent(column: pd.Series):\n",
    "        try:\n",
    "            mode = column.mode()\n",
    "            return mode.iloc[0] if not mode.empty else None\n",
    "        except:\n",
    "            return None\n",
    "\n",
    "    summary_dict['Most Frequent Value'] = df.apply(get_most_frequent)\n",
    "\n",
    "    # Add Unique Count (number of values that appear only once)\n",
    "    def unique_count(column: pd.Series) -> int:\n",
    "        return (column.value_counts() == 1).sum()\n",
    "    \n",
    "    summary_dict['Unique Count'] = df.apply(unique_count)\n",
    "\n",
    "    # Add Empty String Count for string columns\n",
    "    def empty_string_count(column: pd.Series) -> int:\n",
    "        if pd.api.types.is_string_dtype(column):\n",
    "            return column.str.strip().eq('').sum()\n",
    "        return 0\n",
    "\n",
    "    summary_dict['Empty String Count'] = df.apply(empty_string_count)\n",
    "\n",
    "    # Create DataFrame from dictionary\n",
    "    summary = pd.DataFrame(summary_dict)\n",
    "\n",
    "    # Ensure the DataFrame's index length matches the number of columns\n",
    "    assert len(summary) == len(df.columns), \"Mismatch in DataFrame column lengths.\"\n",
    "\n",
    "    return summary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Column Details"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "def numeric_metrics(series: pd.Series) -> pd.Series:\n",
    "    \"\"\"\n",
    "    Calculate various metrics for a numeric series.\n",
    "\n",
    "    Args:\n",
    "        series (pd.Series): The numeric series to analyze.\n",
    "\n",
    "    Returns:\n",
    "        pd.Series: Various calculated metrics for the series.\n",
    "    \"\"\"\n",
    "    # Calculate basic statistics\n",
    "    mean = series.mean()\n",
    "    median = series.median()\n",
    "    min_val = series.min()\n",
    "    max_val = series.max()\n",
    "    range_val = max_val - min_val\n",
    "    total_sum = series.sum()\n",
    "    \n",
    "    # Calculate dispersion metrics\n",
    "    std_dev = series.std()\n",
    "    variance = series.var()\n",
    "    abs_mean_dev = (series - mean).abs().mean()\n",
    "    cv = std_dev / mean if mean != 0 else float('inf')\n",
    "    \n",
    "    # Calculate quartiles and IQR\n",
    "    q1 = series.quantile(0.25)\n",
    "    q3 = series.quantile(0.75)\n",
    "    iqr = q3 - q1\n",
    "    \n",
    "    # Calculate outlier bounds\n",
    "    lower_bound = q1 - 1.5 * iqr\n",
    "    upper_bound = q3 + 1.5 * iqr\n",
    "    \n",
    "    # Calculate mode(s)\n",
    "    mode_values = series.mode()\n",
    "    mode_value = mode_values[0] if not mode_values.empty else np.nan\n",
    "    \n",
    "    # Calculate additional metrics\n",
    "    skewness = series.skew()\n",
    "    kurtosis = series.kurtosis()\n",
    "    num_outliers = ((series < lower_bound) | (series > upper_bound)).sum()\n",
    "    missing_values = series.isna().sum()\n",
    "    unique_values = series.nunique()\n",
    "\n",
    "    # Format the metrics with thousands separators and up to two decimal places\n",
    "    def format_number(value):\n",
    "        if pd.isna(value):\n",
    "            return 'NaN'\n",
    "        return f\"{value:,.2f}\"\n",
    "    \n",
    "    # Compile all metrics into a pandas Series with logical grouping\n",
    "    return pd.Series({\n",
    "        # Basic Statistics\n",
    "        'Mean': format_number(mean),\n",
    "        'Median': format_number(median),\n",
    "        'Min': format_number(min_val),\n",
    "        'Max': format_number(max_val),\n",
    "        'Range': format_number(range_val),\n",
    "        'Sum': format_number(total_sum),\n",
    "\n",
    "        # Dispersion\n",
    "        'Std Dev': format_number(std_dev),\n",
    "        'Variance': format_number(variance),\n",
    "        'Absolute Mean Deviation': format_number(abs_mean_dev),\n",
    "        'Coefficient of Variation (CV)': format_number(cv),\n",
    "\n",
    "        # Distribution\n",
    "        'Skewness': format_number(skewness),\n",
    "        'Kurtosis': format_number(kurtosis),\n",
    "\n",
    "        # Quartiles and IQR\n",
    "        '1st Quartile (Q1)': format_number(q1),\n",
    "        '3rd Quartile (Q3)': format_number(q3),\n",
    "        'Interquartile Range (IQR)': format_number(iqr),\n",
    "\n",
    "        # Outliers\n",
    "        'Lower Bound for Outliers': format_number(lower_bound),\n",
    "        'Upper Bound for Outliers': format_number(upper_bound),\n",
    "        'Number of Outliers': num_outliers,\n",
    "\n",
    "        # Other\n",
    "        'Mode': mode_value,\n",
    "        'Missing Values': missing_values,\n",
    "        'Unique Values': unique_values\n",
    "    })\n",
    "\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "def datetime_metrics(series: pd.Series) -> pd.Series:\n",
    "    \"\"\"\n",
    "    Calculate various metrics for a datetime series.\n",
    "\n",
    "    Args:\n",
    "        series (pd.Series): The datetime series to analyze.\n",
    "\n",
    "    Returns:\n",
    "        pd.Series: Various calculated metrics for the series.\n",
    "    \"\"\"\n",
    "    # Convert series to datetime format, if not already\n",
    "    series = pd.to_datetime(series, errors='coerce')\n",
    "    \n",
    "    # Calculate basic statistics\n",
    "    min_date = series.min()\n",
    "    max_date = series.max()\n",
    "    mean_date = series.mean()\n",
    "    median_date = series.median()\n",
    "\n",
    "    # Convert datetime to string format for reporting\n",
    "    min_date_str = pd.Timestamp(min_date).strftime('%Y-%m-%d') if pd.notna(min_date) else None\n",
    "    max_date_str = pd.Timestamp(max_date).strftime('%Y-%m-%d') if pd.notna(max_date) else None\n",
    "    mean_date_str = pd.Timestamp(mean_date).strftime('%Y-%m-%d') if pd.notna(mean_date) else None\n",
    "    median_date_str = pd.Timestamp(median_date).strftime('%Y-%m-%d') if pd.notna(median_date) else None\n",
    "    \n",
    "    date_range = (max_date - min_date).days if pd.notna(max_date) and pd.notna(min_date) else None\n",
    "    total_days = date_range  # Same as date_range in this context\n",
    "    missing_values = series.isna().sum()\n",
    "    unique_values = series.nunique()\n",
    "    most_common_date = series.mode().values[0] if not series.mode().empty else None\n",
    "    \n",
    "    # Convert most_common_date to string format\n",
    "    most_common_date_str = pd.Timestamp(most_common_date).strftime('%Y-%m-%d') if pd.notna(most_common_date) else None\n",
    "\n",
    "    # Return all metrics as a pandas Series\n",
    "    return pd.Series({\n",
    "        'Min Date': min_date_str,\n",
    "        'Max Date': max_date_str,\n",
    "        'Mean Date': mean_date_str,\n",
    "        'Median Date': median_date_str,\n",
    "        'Date Range (Days)': date_range,\n",
    "        'Total Days': total_days,\n",
    "        'Missing Values': missing_values,\n",
    "        'Unique Values': unique_values,\n",
    "        'Most Common Date': most_common_date_str\n",
    "    })\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "def string_metrics(series: pd.Series) -> pd.Series:\n",
    "    \"\"\"\n",
    "    Calculate various metrics for a string series.\n",
    "\n",
    "    Args:\n",
    "        series (pd.Series): The string series to analyze.\n",
    "\n",
    "    Returns:\n",
    "        pd.Series: Various calculated metrics for the series.\n",
    "    \"\"\"\n",
    "    # Clean the series\n",
    "    series = series.fillna('')\n",
    "    \n",
    "    # Calculate basic string statistics\n",
    "    most_frequent = series.mode()[0] if not series.mode().empty else np.nan\n",
    "    unique_values = series.nunique()\n",
    "    missing_values = series.isna().sum()\n",
    "    empty_strings = (series == '').sum()\n",
    "    \n",
    "    # String length statistics\n",
    "    lengths = series.apply(len)\n",
    "    longest_string = lengths.max()\n",
    "    shortest_string = lengths.min()\n",
    "    average_length = lengths.mean()\n",
    "    \n",
    "    # Character frequency (top 10 characters)\n",
    "    char_freq = pd.Series(''.join(series).replace(' ', '')).value_counts().head(10)\n",
    "    \n",
    "    # Word frequency (top 10 words, if applicable)\n",
    "    words = series.str.split(expand=True).stack()\n",
    "    word_freq = words.value_counts().head(10)\n",
    "    \n",
    "    # Return all metrics as a pandas Series\n",
    "    return pd.Series({\n",
    "        'Most Frequent': most_frequent,\n",
    "        'Unique Values': unique_values,\n",
    "        'Missing Values': missing_values,\n",
    "        'Empty Strings': empty_strings,\n",
    "        'Longest String Length': longest_string,\n",
    "        'Shortest String Length': shortest_string,\n",
    "        'Average String Length': average_length,\n",
    "        # 'Top 10 Characters': char_freq.to_dict(),\n",
    "        'Top 10 Words': word_freq.to_dict()\n",
    "    })\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "def boolean_metrics(series: pd.Series) -> pd.Series:\n",
    "    \"\"\"\n",
    "    Calculate metrics for a boolean series.\n",
    "\n",
    "    Args:\n",
    "        series (pd.Series): The boolean series to analyze.\n",
    "\n",
    "    Returns:\n",
    "        pd.Series: Various calculated metrics for the series.\n",
    "    \"\"\"\n",
    "    return pd.Series({\n",
    "        'True Count': (series == True).sum(),\n",
    "        'False Count': (series == False).sum(),\n",
    "        'Missing Values': series.isna().sum()\n",
    "    })\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "def data_type_summaries(df: pd.DataFrame) -> dict:\n",
    "    \"\"\"\n",
    "    Get comprehensive metrics for each data type in the DataFrame and return them as a dictionary of DataFrames.\n",
    "\n",
    "    Args:\n",
    "        df (pd.DataFrame): The DataFrame for which to generate the summaries.\n",
    "\n",
    "    Returns:\n",
    "        dict: A dictionary with data type as the key and DataFrame of metrics as the value.\n",
    "    \"\"\"\n",
    "    if df is None:\n",
    "        raise ValueError(\"DataFrame is not set.\")\n",
    "\n",
    "    # Dictionary to hold DataFrames for each data type\n",
    "    summaries = {}\n",
    "\n",
    "    # Metrics calculation functions for each data type\n",
    "    metrics_funcs = {\n",
    "        np.number: numeric_metrics,\n",
    "        'datetime64[ns]': datetime_metrics,\n",
    "        'object': string_metrics,\n",
    "        'bool': boolean_metrics\n",
    "    }\n",
    "\n",
    "    # Calculate and store metrics for each data type\n",
    "    for dtype, metrics_func in metrics_funcs.items():\n",
    "        type_columns = df.select_dtypes(include=[dtype])\n",
    "        summary_list = []\n",
    "\n",
    "        for column in type_columns:\n",
    "            metrics = metrics_func(df[column])\n",
    "            metrics.name = column\n",
    "            summary_list.append(pd.DataFrame(metrics).T)\n",
    "\n",
    "        if summary_list:\n",
    "            summary_df = pd.concat(summary_list)\n",
    "            summary_df.reset_index(inplace=True)\n",
    "            summary_df.rename(columns={'index': 'Column Name'}, inplace=True)\n",
    "            summary_df = summary_df.sort_values(by='Column Name').reset_index(drop=True)\n",
    "            summaries[str(dtype)] = summary_df\n",
    "\n",
    "    # Display DataFrames in a Jupyter Notebook\n",
    "    for dtype, summary_df in summaries.items():\n",
    "        print(f\"\\nData Type: {dtype}\")\n",
    "        if get_ipython() is not None:  # Check if running in Jupyter Notebook\n",
    "            display(summary_df)\n",
    "        else:\n",
    "            print(summary_df.to_string(index=False))  # Print DataFrame as string\n",
    "\n",
    "    return summaries\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Dataframe Correlation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "def correlation_function(df: pd.DataFrame, dummy_variables: bool = False, dummy_threshold: int = 10, report_threshold: float = 0.3) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Calculate the correlation between columns in a DataFrame, optionally creating dummy variables for categorical columns.\n",
    "    \n",
    "    Parameters:\n",
    "    - df (pd.DataFrame): The input DataFrame.\n",
    "    - dummy_variables (bool): If True, create dummy variables for string columns with unique values <= dummy_threshold.\n",
    "    - dummy_threshold (int): Maximum number of unique values in a column to create dummy variables.\n",
    "    - report_threshold (float): Threshold for reporting correlated pairs (both positive and negative).\n",
    "    \n",
    "    Returns:\n",
    "    - pd.DataFrame: DataFrame with columns of the correlated pairs, their correlation values rounded to 2 decimals.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Step 1: Convert string/categorical columns to dummy variables if needed\n",
    "    df_copy = df.copy()\n",
    "    \n",
    "    if dummy_variables:\n",
    "        # Identify string/categorical columns\n",
    "        non_numeric_cols = df_copy.select_dtypes(include=['object', 'category']).columns\n",
    "        \n",
    "        for col in non_numeric_cols:\n",
    "            unique_vals = df_copy[col].nunique()\n",
    "            if unique_vals <= dummy_threshold:\n",
    "                # Convert to dummy variables\n",
    "                df_copy = pd.get_dummies(df_copy, columns=[col], drop_first=True)\n",
    "    \n",
    "    # Step 2: Identify numeric columns (ignoring non-numeric columns)\n",
    "    numeric_df = df_copy.select_dtypes(include=[np.number])\n",
    "    \n",
    "    if numeric_df.empty:\n",
    "        raise ValueError(\"No numeric columns found in the DataFrame for correlation analysis.\")\n",
    "    \n",
    "    # Step 3: Calculate correlation matrix\n",
    "    corr_matrix = numeric_df.corr().round(2)  # Round correlation values to 2 decimals\n",
    "\n",
    "    # Step 4: Extract correlation pairs greater than the report_threshold\n",
    "    corr_pairs = corr_matrix.unstack().reset_index()\n",
    "    corr_pairs.columns = ['Variable1', 'Variable2', 'Correlation']\n",
    "    \n",
    "    # Remove self-correlations\n",
    "    corr_pairs = corr_pairs[corr_pairs['Variable1'] != corr_pairs['Variable2']]\n",
    "    \n",
    "    # Ensure we only report each pair once (e.g., always sort Variable1 and Variable2 alphabetically)\n",
    "    corr_pairs['sorted_pair'] = corr_pairs.apply(lambda x: tuple(sorted([x['Variable1'], x['Variable2']])), axis=1)\n",
    "    corr_pairs = corr_pairs.drop_duplicates(subset='sorted_pair').drop(columns='sorted_pair')\n",
    "\n",
    "    # Step 5: Filter pairs based on the report_threshold\n",
    "    corr_pairs['Abs_Correlation'] = corr_pairs['Correlation'].abs()\n",
    "    filtered_pairs = corr_pairs[corr_pairs['Abs_Correlation'] >= report_threshold]\n",
    "    \n",
    "    # Sort by absolute correlation in descending order\n",
    "    filtered_pairs = filtered_pairs.sort_values(by='Abs_Correlation', ascending=False).drop(columns='Abs_Correlation')\n",
    "\n",
    "    return filtered_pairs.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Qualitative Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Text Analysis Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Text Cleaning Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is a dictionary of common encoding errors\n",
    "\n",
    "common_encoding_errors = {\n",
    "    \"&lt;\": \"<\",        # Less-than sign\n",
    "    \"&gt;\": \">\",        # Greater-than sign\n",
    "    \"&amp;\": \"&\",       # Ampersand\n",
    "    \"&quot;\": \"\\\"\",     # Double quotation mark\n",
    "    \"&apos;\": \"'\",      # Apostrophe\n",
    "    \"&nbsp;\": \" \",      # Non-breaking space\n",
    "    \"&copy;\": \"(c)\",    # Copyright sign\n",
    "    \"&reg;\": \"(r)\",     # Registered sign\n",
    "    \"&euro;\": \"EUR\",    # Euro sign\n",
    "    \"&pound;\": \"GBP\",   # Pound sign\n",
    "    \"&yen;\": \"Yen\",     # Yen sign\n",
    "    \"&cent;\": \"c\",      # Cent sign\n",
    "    \"&sect;\": \"§\",      # Section sign\n",
    "    \"&para;\": \"¶\",      # Pilcrow sign\n",
    "    \"&deg;\": \"°\",       # Degree sign\n",
    "    \"&plusmn;\": \"±\",    # Plus-minus sign\n",
    "    \"&micro;\": \"µ\",     # Micro sign\n",
    "    \"&sup2;\": \"^2\",     # Superscript two\n",
    "    \"&sup3;\": \"^3\",     # Superscript three\n",
    "    \"&frac14;\": \"1/4\",  # Fraction one quarter\n",
    "    \"&frac12;\": \"1/2\",  # Fraction one half\n",
    "    \"&frac34;\": \"3/4\",  # Fraction three quarters\n",
    "    \"&times;\": \"x\",     # Multiplication sign\n",
    "    \"&divide;\": \"/\",    # Division sign\n",
    "    \"&bull;\": \"•\",      # Bullet\n",
    "    \"&ndash;\": \"-\",     # En dash\n",
    "    \"&mdash;\": \"-\",     # Em dash\n",
    "    \"&lsquo;\": \"'\",     # Left single quotation mark\n",
    "    \"&rsquo;\": \"'\",     # Right single quotation mark\n",
    "    \"&sbquo;\": \",\",     # Single low-9 quotation mark\n",
    "    \"&ldquo;\": \"\\\"\",    # Left double quotation mark\n",
    "    \"&rdquo;\": \"\\\"\",    # Right double quotation mark\n",
    "    \"&bdquo;\": \"\\\"\",    # Double low-9 quotation mark\n",
    "    \"&hellip;\": \"...\",  # Horizontal ellipsis\n",
    "    \"%20\": \" \",         # Space\n",
    "    \"%21\": \"!\",         # Exclamation mark\n",
    "    \"%22\": \"\\\"\",        # Double quotation mark\n",
    "    \"%23\": \"#\",         # Number sign\n",
    "    \"%24\": \"$\",         # Dollar sign\n",
    "    \"%25\": \"%\",         # Percent sign\n",
    "    \"%26\": \"&\",         # Ampersand\n",
    "    \"%27\": \"'\",         # Apostrophe\n",
    "    \"%28\": \"(\",         # Left parenthesis\n",
    "    \"%29\": \")\",         # Right parenthesis\n",
    "    \"%2A\": \"*\",         # Asterisk\n",
    "    \"%2B\": \"+\",         # Plus sign\n",
    "    \"%2C\": \",\",         # Comma\n",
    "    \"%2D\": \"-\",         # Hyphen\n",
    "    \"%2E\": \".\",         # Period\n",
    "    \"%2F\": \"/\",         # Slash\n",
    "    \"%3A\": \":\",         # Colon\n",
    "    \"%3B\": \";\",         # Semicolon\n",
    "    \"%3C\": \"<\",         # Less-than sign\n",
    "    \"%3D\": \"=\",         # Equal sign\n",
    "    \"%3E\": \">\",         # Greater-than sign\n",
    "    \"%3F\": \"?\",         # Question mark\n",
    "    \"%40\": \"@\",         # At sign\n",
    "    \"%5B\": \"[\",         # Left square bracket\n",
    "    \"%5C\": \"\\\\\",        # Backslash\n",
    "    \"%5D\": \"]\",         # Right square bracket\n",
    "    \"%5E\": \"^\",         # Caret\n",
    "    \"%5F\": \"_\",         # Underscore\n",
    "    \"%60\": \"`\",         # Grave accent\n",
    "    \"%7B\": \"{\",         # Left curly bracket\n",
    "    \"%7C\": \"|\",         # Vertical bar\n",
    "    \"%7D\": \"}\",         # Right curly bracket\n",
    "    \"%7E\": \"~\",         # Tilde\n",
    "    \"â€”\": \"-\",         # Em dash\n",
    "    \"â€“\": \"–\",         # En dash\n",
    "    \"â€˜\": \"'\",         # Left single quotation mark\n",
    "    \"â€™\": \"'\",         # Right single quotation mark\n",
    "    \"â€œ\": \"\\\"\",        # Left double quotation mark\n",
    "    \"â€\": \"\\\"\",         # Right double quotation mark\n",
    "    \"â€¦\": \"...\",       # Ellipsis\n",
    "    \"â€\": \"\\\"\",         # Right double quotation mark (alternate)\n",
    "    \"â€™\": \"'\",         # Right single quotation mark (alternate)\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_standardize_highway_references(text):\n",
    "    \"\"\"Standardize highway references in the provided text.\"\"\"\n",
    "    # Updated pattern to capture all variations with a number\n",
    "    regex_highway_search = re.compile(\n",
    "        r\"(?:#*\\b(?:bc\\s*)?(?:h(?:ighway|wy|iway|way|w|y)|hw|hwy|hway|hiway|h)\\s*[\\s#\\.\\-]*)(\\d+[abcd]*)\\b\",\n",
    "        re.IGNORECASE,\n",
    "    )\n",
    "    \n",
    "    def replace_match(match):\n",
    "        number = match.group(1)  # Capture highway number\n",
    "        return f\"BCHwy{number}\"\n",
    "    \n",
    "    text = re.sub(regex_highway_search, replace_match, text)\n",
    "\n",
    "    return text\n",
    "\n",
    "# Main Text cleaning function\n",
    "def clean_text(text):\n",
    "    \"\"\"\n",
    "    Clean and preprocess the input text based on specified cleaning options.\n",
    "\n",
    "    This function applies a series of text cleaning operations to the input `text` based on the \n",
    "    global `text_cleaning_options` dictionary. The cleaning operations include:\n",
    "    - Converting text to lowercase\n",
    "    - Expanding or contracting contractions\n",
    "    - Replacing common encoding errors\n",
    "    - Standardizing highway and DriveBC references\n",
    "    - Using a custom search-and-replace dictionary\n",
    "    - Removing stopwords\n",
    "    - Stripping or replacing punctuation\n",
    "    - Performing spell check\n",
    "    - Stemming or lemmatizing words\n",
    "    - Cleaning up extra whitespace\n",
    "\n",
    "    Args:\n",
    "        text (str): The text to be cleaned and preprocessed.\n",
    "\n",
    "    Returns:\n",
    "        str: The cleaned text after applying the specified text cleaning operations.\n",
    "\n",
    "    Raises:\n",
    "        Exception: If any error occurs during text processing, an error message is printed, \n",
    "                   and an empty string is returned.\n",
    "\n",
    "    Notes:\n",
    "        - The function uses global dictionaries for contractions, common encoding errors, \n",
    "          custom search-and-replace terms, and stopwords.\n",
    "        - The `text_cleaning_options` dictionary determines which cleaning operations are applied.\n",
    "        - The `ensure_string` function is assumed to ensure that `text` is a string. It is not defined here.\n",
    "        - The `clean_whitespace` function is assumed to handle extra whitespace. It is not defined here.\n",
    "    \"\"\"\n",
    "\n",
    "\n",
    "    global text_cleaning_options\n",
    "\n",
    "    # Set Cleaning Options\n",
    "    transform_contractions = text_cleaning_options[\"Transform contractions\"]        \n",
    "    convert_to_lowercase = text_cleaning_options[\"Convert to lowercase\"]\n",
    "    covert_common_encoding_errors = text_cleaning_options[\"Convert Common Encoding Errors\"]\n",
    "    \n",
    "    strip_punctuation = text_cleaning_options[\"Strip punctuation\"]\n",
    "    remove_stopwords = text_cleaning_options[\"Remove stopwords\"]\n",
    "\n",
    "    use_custom_sar_dict = text_cleaning_options[\"Use custom Search-and-Replace dictionary\"]\n",
    "    standardize_highway_references = text_cleaning_options[\"Standardize highway references\"]\n",
    "    standardize_drivebc_references  = text_cleaning_options[\"Standardize drivebc references\"]\n",
    "    spell_check = text_cleaning_options[\"Spell check\"]\n",
    "    stem_words  = text_cleaning_options[\"Stem words\"]\n",
    "    lemmatize_words = text_cleaning_options[\"Lemmatize words\"]\n",
    "\n",
    "    try:\n",
    "        # Make sure the data is actually text\n",
    "        text = ensure_string(text,3)\n",
    "\n",
    "        # Convert all text to lower case\n",
    "        if convert_to_lowercase is True:\n",
    "            text = text.lower()\n",
    "\n",
    "        # Replace contractions\n",
    "        if transform_contractions == \"Expand\":\n",
    "            # Expand contractions\n",
    "            for key, value in contractions_dict.items():\n",
    "                text = text.replace(value, key)\n",
    "            # Contract expansions\n",
    "        elif transform_contractions == \"Contract\":\n",
    "                for key, value in contractions_dict.items():\n",
    "                    text = text.replace(key, value)\n",
    "\n",
    "        if covert_common_encoding_errors is True:\n",
    "            # Perform character replacement using the dictionary\n",
    "            for improper, proper in common_encoding_errors.items():\n",
    "                text = text.replace(improper, proper)\n",
    "\n",
    "        # Standardize Highway References\n",
    "        if standardize_highway_references is True:\n",
    "            text = clean_standardize_highway_references(text)\n",
    "\n",
    "        # Standardize DriveBC references\n",
    "        if standardize_drivebc_references is True:\n",
    "            regex_drivebc_search = re.compile(\n",
    "                r\"\\bdrive.*bc\\b\",\n",
    "                re.IGNORECASE,\n",
    "            )\n",
    "            regex_drivebc_replace = r\"drivebc\"\n",
    "            text = re.sub(regex_drivebc_search, regex_drivebc_replace, text)\n",
    "\n",
    "        # Use a custom search and replace dictionary\n",
    "        if use_custom_sar_dict is True:\n",
    "            for key, value in custom_search_and_replace_dict.items():\n",
    "                text = text.replace(key, value)\n",
    "\n",
    "        # Remove stop words\n",
    "        if remove_stopwords is True:\n",
    "            for key, value in external_stopword_list_dict.items():\n",
    "                text = text.replace(key, value)\n",
    "\n",
    "        # strip punctuation\n",
    "        if strip_punctuation is True:\n",
    "            # # This removed punctuation marks\n",
    "            # text = \" \".join([char for char in text if char not in string.punctuation])\n",
    "            \n",
    "            # This replaces punctuation marks with a space\n",
    "            translation_table = str.maketrans(string.punctuation, ' ' * len(string.punctuation))\n",
    "            # Replace punctuation with spaces\n",
    "            text = text.translate(translation_table)\n",
    "\n",
    "        if spell_check is True:\n",
    "            text = text.split()\n",
    "            text = [spell(word) for word in text]\n",
    "            text = \" \".join(text)\n",
    "\n",
    "        # Stem words\n",
    "        if stem_words is True:\n",
    "            text = text.split()\n",
    "            text = [ps.stem(word) for word in text]\n",
    "            text = \" \".join(text)\n",
    "\n",
    "        # Lemmatize words\n",
    "        if lemmatize_words is True:\n",
    "            text = text.split()\n",
    "            text = [wn.lemmatize(word) for word in text]\n",
    "            text = \" \".join(text)\n",
    "\n",
    "        # Clean Whitespace\n",
    "        text = clean_whitespace(text)\n",
    "\n",
    "        return text\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing text: {e}\")\n",
    "        return \"\"\n",
    "\n",
    "# Add clean text to dataframe\n",
    "def add_clean_text_column(df,comments):\n",
    "    \"\"\"\n",
    "    Adds a cleaned text column to a DataFrame by applying text cleaning operations.\n",
    "\n",
    "    This function adds a new column to the input DataFrame, `df`, which contains cleaned text\n",
    "    derived from the `comments` Series. The cleaning is performed using the `clean_text` function.\n",
    "\n",
    "    Args:\n",
    "        df (pd.DataFrame): The DataFrame to which the cleaned text column will be added.\n",
    "        comments (pd.Series): A Series containing the text data to be cleaned and added to the DataFrame.\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: The original DataFrame with an additional column named 'clean_text' that contains the cleaned text.\n",
    "\n",
    "    Raises:\n",
    "        ValueError: If `comments` is not a Series or if its length does not match the number of rows in `df`.\n",
    "        Exception: If any error occurs during the application of the `clean_text` function.\n",
    "\n",
    "    Notes:\n",
    "        - The `comments` Series should have the same length as the number of rows in the DataFrame `df`.\n",
    "        - The `clean_text` function is expected to be defined elsewhere and handle the text cleaning process.\n",
    "    \"\"\"\n",
    "    if not isinstance(comments, pd.Series):\n",
    "        raise ValueError(\"The 'comments' argument must be a pandas Series.\")\n",
    "    if len(comments) != len(df):\n",
    "        raise ValueError(\"The length of 'comments' must match the number of rows in the DataFrame.\")\n",
    "\n",
    "    try:\n",
    "        df[\"clean_text\"] = comments.apply(lambda x: clean_text(x))\n",
    "        return df\n",
    "    except Exception as e:\n",
    "        print(f\"Error adding clean text column: {e}\")\n",
    "        raise"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### N-Gram Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ngram_counter(series):\n",
    "    \"\"\"\n",
    "    Counts and returns the most common n-grams in a pandas Series of text data.\n",
    "\n",
    "    The function prompts the user to input the starting and ending n-gram lengths, \n",
    "    as well as the number of top n-grams to find for each length. It then processes \n",
    "    the text data, tokenizes it, generates n-grams for the specified lengths, and \n",
    "    counts their occurrences. Finally, it returns a DataFrame with the n-grams, \n",
    "    their types, and counts.\n",
    "\n",
    "    Parameters:\n",
    "    series (pandas.Series): A pandas Series containing text data.\n",
    "\n",
    "    Returns:\n",
    "    pandas.DataFrame: A DataFrame with columns 'N-Gram', 'N-Gram Type', and 'Count',\n",
    "                      containing the most common n-grams and their counts for each\n",
    "                      specified n-gram length.\n",
    "\n",
    "    User Inputs:\n",
    "    - Starting n-gram length: An integer specifying the smallest n-gram length (e.g., \n",
    "                              1 for unigram, 2 for bigram, etc.).\n",
    "    - Ending n-gram length: An integer specifying the largest n-gram length.\n",
    "    - Number of n-grams: An integer specifying the number of top n-grams to find \n",
    "                         for each n-gram length.\n",
    "\n",
    "    Example:\n",
    "    >>> import pandas as pd\n",
    "    >>> series = pd.Series([\"This is a sample text.\", \"This text is for testing purposes.\"])\n",
    "    >>> result = ngram_counter(series)\n",
    "    >>> print(result)\n",
    "               N-Gram N-Gram Type  Count\n",
    "    0            this       1-gram      2\n",
    "    1              is       1-gram      2\n",
    "    2              a       1-gram      1\n",
    "    3          sample       1-gram      1\n",
    "    4            text       1-gram      2\n",
    "    5           this is     2-gram      2\n",
    "    6              is a     2-gram      1\n",
    "    7          sample text  2-gram      1\n",
    "    8            text is    2-gram      1\n",
    "    9              is for   2-gram      1\n",
    "    10           for testing 2-gram     1\n",
    "    11 testing purposes 2-gram  1\n",
    "    \"\"\"\n",
    "    # Prompt user for the starting and ending n-gram lengths\n",
    "    os.system(\"cls\")\n",
    "    while True:\n",
    "        try:\n",
    "            start_length = int(input(\"Enter the starting n-gram length (1 for unigram, 2 for bigram, etc.): \"))\n",
    "            break\n",
    "        except ValueError:\n",
    "            print(\"Invalid input. Please enter a valid integer.\")\n",
    "    \n",
    "    while True:\n",
    "        try:\n",
    "            end_length = int(input(\"Enter the ending n-gram length: \"))\n",
    "            break\n",
    "        except ValueError:\n",
    "            print(\"Invalid input. Please enter a valid integer.\")\n",
    "\n",
    "    # Prompt user for the number of n-grams to find\n",
    "    while True:\n",
    "        try:\n",
    "            num_ngrams = int(input(\"Enter the number of n-grams to find for each type: \"))\n",
    "            break\n",
    "        except ValueError:\n",
    "            print(\"Invalid input. Please enter a valid integer.\")\n",
    "\n",
    "    ngrams_list = []\n",
    "\n",
    "    # Process each n-gram length\n",
    "    for n in range(start_length, end_length + 1):\n",
    "        ngram_counts = Counter()\n",
    "        \n",
    "        # Process each text in the series\n",
    "        for text in series.dropna().str.lower():\n",
    "            tokens = word_tokenize(text)\n",
    "            if len(tokens) >= n:\n",
    "                # Generate and count n-grams directly\n",
    "                generated_ngrams = ngrams(tokens, n)\n",
    "                ngram_counts.update(generated_ngrams)\n",
    "\n",
    "        # Get the top n-grams of each type\n",
    "        top_ngrams = ngram_counts.most_common(num_ngrams)\n",
    "        for ngram, count in top_ngrams:\n",
    "            ngrams_list.append([' '.join(ngram), f\"{n}-gram\", count])\n",
    "\n",
    "    # Create a DataFrame with the n-grams, their types, and counts\n",
    "    result_df = pd.DataFrame(ngrams_list, columns=['N-Gram', 'N-Gram Type', 'Count'])\n",
    "\n",
    "    return result_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Isolate Parts of Speech Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "def isolate_pos(series, selected_parts_of_speech, string_length_threshold):\n",
    "    \"\"\"\n",
    "    Isolates and counts words from a text series based on selected parts of speech and a string length threshold.\n",
    "\n",
    "    This function processes the input `series` by tokenizing it, tagging the tokens with their parts of speech,\n",
    "    and filtering the words based on the selected parts of speech. It then removes stop words, applies a string \n",
    "    length threshold, and returns a DataFrame with word counts for the remaining words.\n",
    "\n",
    "    Args:\n",
    "        series (pd.Series or str): The input text data, either as a pandas Series or a single string. \n",
    "        selected_parts_of_speech (list of str): A list of parts of speech to filter the tokens. \n",
    "        string_length_threshold (int): The minimum length of words to include in the final word count.\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: A DataFrame containing two columns:\n",
    "            - 'word': The words filtered by the selected parts of speech and length threshold.\n",
    "            - 'count': The frequency of each word.\n",
    "\n",
    "    Raises:\n",
    "        ValueError: If `selected_parts_of_speech` is empty or `string_length_threshold` is not a positive integer.\n",
    "        TypeError: If `series` is neither a pandas Series nor a string.\n",
    "\n",
    "    Notes:\n",
    "        - The `series` is converted to a string if it is a pandas Series.\n",
    "        - NLTK's `word_tokenize` and `pos_tag` functions are used for tokenization and part-of-speech tagging.\n",
    "        - The function uses a predefined list of stop words to filter out common, non-informative words.\n",
    "        - Words are filtered based on the specified length threshold before counting their occurrences.\n",
    "\n",
    "    Example:\n",
    "        >>> df = pd.Series([\"The quick brown fox jumps over the lazy dog.\"])\n",
    "        >>> pos_list = ['NN', 'JJ']  # Example parts of speech: Noun and Adjective\n",
    "        >>> threshold = 3\n",
    "        >>> isolate_pos(df, pos_list, threshold)\n",
    "           word  count\n",
    "        0   quick      1\n",
    "        1   brown      1\n",
    "        2   jumps      1\n",
    "        3   over       1\n",
    "    \"\"\"\n",
    "\n",
    "\n",
    "    if not selected_parts_of_speech:\n",
    "        print(\"No Parts of Speech have been selected. Please select them and try again.\")\n",
    "        input(f\"Press 'Enter' to continue.\")\n",
    "        return\n",
    "\n",
    "    if isinstance(series, pd.Series):\n",
    "        series = series.to_string()\n",
    "\n",
    "    # Tokenize the sentence into individual words\n",
    "    tokens = nltk.word_tokenize(series)\n",
    "\n",
    "    # Use NLTK's POS tagger to tag each word with its part of speech\n",
    "    pos_tags = nltk.pos_tag(tokens)\n",
    "\n",
    "    # Create a list of words filtered by chosen Part-of-Speech (PoS) type\n",
    "    word_list = []\n",
    "    print(\"Tagging parts of speech...\")\n",
    "    for word, pos in pos_tags:\n",
    "        if pos in selected_parts_of_speech:\n",
    "            word_list.append(word)\n",
    "\n",
    "    # Remove any stop words from the list\n",
    "    stop_words = [\n",
    "        \"..\", \"'m\", \"'s\", \"#name?\", \"are\", \"be\", \"been\", \"being\", \"did\", \"do\", \"does\", \"done\",\n",
    "        \"get\", \"go\", \"had\", \"has\", \"have\", \"i\", \"ì\", \"is\", \"make\", \"n/a\", \"needs\", \"put\", \"Q\",\n",
    "        \"seems\", \"take\", \"use\", \"vs\", \"was\", \"way\", \"were\"\n",
    "    ]\n",
    "    \n",
    "    cleaned_word_list = [element for element in word_list if element not in stop_words]\n",
    "\n",
    "    # Apply string length threshold\n",
    "    filtered_word_list = [word for word in cleaned_word_list if len(word) >= string_length_threshold]\n",
    "\n",
    "    # Count the frequency of each word\n",
    "    word_counts = pd.Series(filtered_word_list).value_counts().reset_index()\n",
    "\n",
    "    # Rename the columns of the DataFrame\n",
    "    word_counts.columns = [\"word\", \"count\"]\n",
    "\n",
    "    # Sort the DataFrame by count in descending order\n",
    "    word_counts = word_counts.sort_values(by=\"count\", ascending=False).reset_index(drop=True)\n",
    "\n",
    "    return word_counts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Sentiment Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_sentiment_column(df, text_column):\n",
    "    \"\"\"\n",
    "    Add a sentiment score column to the DataFrame based on the specified text column using VADER.\n",
    "\n",
    "    Parameters:\n",
    "    df (pd.DataFrame): The original DataFrame.\n",
    "    text_column (str): The name of the column containing the text for sentiment analysis.\n",
    "\n",
    "    Returns:\n",
    "    pd.DataFrame: The DataFrame with an added \"sentiment\" score column.\n",
    "    \"\"\"\n",
    "    # Ensure the text_column exists in the DataFrame\n",
    "    if text_column not in df.columns:\n",
    "        raise ValueError(f\"Column '{text_column}' not found in DataFrame columns.\")\n",
    "\n",
    "    # Initialize the VADER sentiment intensity analyzer\n",
    "    analyzer = SentimentIntensityAnalyzer()\n",
    "\n",
    "    # Define a function to calculate the sentiment score\n",
    "    def get_sentiment(text):\n",
    "        if pd.isnull(text) or not isinstance(text, str) or text.strip() == \"\":\n",
    "            return None  # Return None for rows without valid text\n",
    "        sentiment = analyzer.polarity_scores(text)\n",
    "        return sentiment['compound']  # Compound sentiment score\n",
    "\n",
    "    # Apply the sentiment function to the text column and add the results to a new column\n",
    "    df['sentiment'] = df[text_column].apply(get_sentiment)\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Read Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Enter the file (or folder) with the data to load into a dataframe\n",
    "# read_file_path = r'C:\\Users\\Windows\\Downloads\\archive (7)\\netflix1.csv'\n",
    "# read_file_path = r'C:\\Users\\Windows\\Downloads\\happyscore_income.csv'\n",
    "\n",
    "read_file_path = r'D:\\Data Analysis\\Qualitative Analysis\\comments_for_demo.csv'\n",
    "\n",
    "df = load_to_dataframe(read_file_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Loading Notes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read a Single File\n",
    "\n",
    "Force a file to load as text with force_plain_text=True </br>\n",
    "When loading an Excel file, use sheet_name=\"Sheet1\" to specify the sheet name to load </br>\n",
    "\n",
    " - df = load_to_dataframe(\"path/to/file.csv\", file_type=\"csv\")\n",
    " - df = load_to_dataframe(\"path/to/file.txt\", file_type=\"txt\", force_plain_text=True)\n",
    " - df = load_to_dataframe(\"path/to/file.xlsx\", file_type=\"xlsx\", sheet_name=\"Sheet1\")\n",
    " - df = load_to_dataframe(\"path/to/file.json\", file_type=\"json\")\n",
    " - df = load_to_dataframe(\"path/to/file.parquet\", file_type=\"parquet\")\n",
    " - df = load_to_dataframe(\"path/to/file.hdf\", file_type=\"hdf\")\n",
    " - df = load_to_dataframe(\"path/to/file.feather\", file_type=\"feather\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read Multiple Files\n",
    " - df = load_to_dataframe(\"path/to/folder\", file_type=\"csv\")\n",
    " - df = load_to_dataframe(\"path/to/folder\", file_type=\"log\", force_plain_text=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# General Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataframe Diagnostics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "column_list = df.columns.tolist()\n",
    "print(\"Columns:\")\n",
    "print(\"========\")\n",
    "for column_name in column_list:\n",
    "    print(column_name)\n",
    "\n",
    "diags = dataframe_diagnostics(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Summaries"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataframe Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "summary_df = overall_summary(df)\n",
    "display(summary_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(type(df.columns))\n",
    "\n",
    "index = df.columns\n",
    "# Convert to list and join with a separator, like a newline or comma\n",
    "# Join the index into a user-friendly string\n",
    "user_friendly_output = \"\\n\".join(index.tolist())\n",
    "\n",
    "print(\"Index Columns:\\n\", user_friendly_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(summary_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Column Summaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the summary\n",
    "summary_df = data_types_summary(df)\n",
    "display(summary_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Column Details"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run the function\n",
    "summaries = data_type_summaries(df)\n",
    "display(summaries)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Correlation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = correlation_function(df, dummy_variables=True, dummy_threshold=5, report_threshold=.5)\n",
    "display(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Qualitiative Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set Data Series from Dataframe for Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_for_analysis = df[\"Comment\"]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load External Dictionaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Custom Dictionaries\n",
    "external_stopword_list_dict = load_csv_to_dict(external_stopword_list, \"stopword\", \"replace\")\n",
    "contractions_dict = load_csv_to_dict(external_contractions_dictionary, \"expansion\", \"contraction\")\n",
    "custom_search_and_replace_dict = load_csv_to_dict(custom_search_and_replace_dictionary, \"search_term\", \"replace_term\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Text Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Text Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_cleaning_options = {\n",
    "    \"Convert to lowercase\": True,\n",
    "    \"Convert Common Encoding Errors\": True,\n",
    "    \"Strip punctuation\": True,  \n",
    "    \"Remove stopwords\": False,  # Examples could include: a, about, at, be, by\n",
    "    \"Transform contractions\": \"Expand\", \n",
    "    \"Use custom Search-and-Replace dictionary\": True,\n",
    "    \"Standardize highway references\": True, # \n",
    "    \"Standardize drivebc references\": True,\n",
    "    \"Spell check\": False,\n",
    "    \"Stem words\": False,\n",
    "    \"Lemmatize words\": False,\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cleaned_text = add_clean_text_column(df, text_for_analysis)\n",
    "cleaned_comments = cleaned_text[\"clean_text\"]\n",
    "\n",
    "base_cleaned_text_report = r\"D:\\Data Analysis\\Qualitative Analysis\\reports\\cleaned_text_report.csv\"\n",
    "cleaned_text_report = save_dataframe_with_incremented_filename(base_cleaned_text_report)\n",
    "\n",
    "# remove_columns = [\"ResponseID\", \"Question Text\", \"Question Number\"]\n",
    "\n",
    "# # Check which columns in the list are present in the DataFrame\n",
    "# columns_present = [col for col in remove_columns if col in cleaned_text.columns]\n",
    "\n",
    "# # Drop the columns that are present\n",
    "# df.drop(columns=columns_present, inplace=True)\n",
    "\n",
    "# print(cleaned_text)\n",
    "cleaned_text.to_csv(cleaned_text_report,index=False)\n",
    "open_in_excel(cleaned_text_report)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### N-Grams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NGrams - Cleaned\n",
    "base_ngram_report_cleaned = r\"D:\\Data Analysis\\Qualitative Analysis\\reports\\ngram_report_cleaned.csv\"\n",
    "return_ngrams = ngram_counter(cleaned_comments)\n",
    "ngram_report = save_dataframe_with_incremented_filename(base_ngram_report_cleaned)\n",
    "return_ngrams.to_csv(ngram_report,index=False)\n",
    "open_in_excel(ngram_report)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parts of Speech"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_verbs = [\"VB\",\"VBD\",\"VBG\",\"VBN\",\"VBP\",\"VBZ\"]\n",
    "all_nouns = [\"NN\",\"NNP\",\"NNS\"]\n",
    "all_nouns_and_verbs = [\"NN\",\"NNP\",\"NNS\",\"VB\",\"VBD\",\"VBG\",\"VBN\",\"VBP\",\"VBZ\"]\n",
    "\n",
    "parts_of_speech = [\n",
    "    (\"CC\", \"conjunction, coordinating\"),\n",
    "    (\"CD\", \"numeral, cardinal\"),\n",
    "    (\"DT\", \"determiner\"),\n",
    "    (\"EX\", \"existential there\"),\n",
    "    (\"IN\", \"preposition or conjunction, subordinating\"),\n",
    "    (\"JJ\", \"adjective or numeral, ordinal\"),\n",
    "    (\"JJR\", \"adjective, comparative\"),\n",
    "    (\"JJS\", \"adjective, superlative\"),\n",
    "    (\"LS\", \"list item marker\"),\n",
    "    (\"MD\", \"modal auxiliary\"),\n",
    "    (\"NN\", \"noun, common, singular or mass\"),\n",
    "    (\"NNP\", \"noun, proper, singular\"),\n",
    "    (\"NNS\", \"noun, common, plural\"),\n",
    "    (\"PDT\", \"pre-determiner\"),\n",
    "    (\"POS\", \"genitive marker\"),\n",
    "    (\"PRP\", \"pronoun, personal\"),\n",
    "    (\"RB\", \"adverb\"),\n",
    "    (\"RBR\", \"adverb, comparative\"),\n",
    "    (\"RBS\", \"adverb, superlative\"),\n",
    "    (\"RP\", \"particle\"),\n",
    "    (\"TO\", \"to as preposition or infinitive marker\"),\n",
    "    (\"UH\", \"interjection\"),\n",
    "    (\"VB\", \"verb, base form\"),\n",
    "    (\"VBD\", \"verb, past tense\"),\n",
    "    (\"VBG\", \"verb, present participle or gerund\"),\n",
    "    (\"VBN\", \"verb, past participle\"),\n",
    "    (\"VBP\", \"verb, present tense, not 3rd person singular\"),\n",
    "    (\"VBZ\", \"verb, present tense, 3rd person singular\"),\n",
    "    (\"WDT\", \"WH-determiner\"),\n",
    "    (\"WP\", \"WH-pronoun\"),\n",
    "    (\"WRB\", \"Wh-adverb\")\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pos_df = isolate_pos(cleaned_comments, all_nouns_and_verbs, 3)\n",
    "\n",
    "# Specify the length threshold\n",
    "string_length_threshold = 3\n",
    "\n",
    "# Filter out rows where the length of the string in the \"word\" column is less than the threshold\n",
    "filtered_pos_df = pos_df[pos_df['word'].apply(lambda x: len(x) >= string_length_threshold)]\n",
    "\n",
    "base_pos_text_report = r\"D:\\Data Analysis\\Qualitative Analysis\\reports\\parts_of_speech_text_report.csv\"\n",
    "pos_text_report = save_dataframe_with_incremented_filename(base_pos_text_report)\n",
    "\n",
    "filtered_pos_df.to_csv(pos_text_report,index=False)\n",
    "open_in_excel(pos_text_report)   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sentiment Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add sentiment column to the DataFrame Use the text name for the header column, not a pandas series\n",
    "df_with_sentiment = add_sentiment_column(df, \"Comment\")\n",
    "\n",
    "base_sentiment_report = r\"D:\\Data Analysis\\Qualitative Analysis\\reports\\sentiment_report.csv\"\n",
    "sentiment_report = save_dataframe_with_incremented_filename(base_sentiment_report)\n",
    "\n",
    "df_with_sentiment.to_csv(sentiment_report,index=False)\n",
    "open_in_excel(sentiment_report)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
